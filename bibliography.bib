@misc{Linux,
author = {Linux},
title = {{gettimeofday(2) - Linux manual page}},
url = {http://man7.org/linux/man-pages/man2/gettimeofday.2.html},
urldate = {2019-04-19}
}
@misc{ChristineHall2018,
author = {{Christine Hall}},
booktitle = {datacenterknowledge.com},
title = {{Microsoft is Live Streaming from Underwater Data Center in Scotland}},
url = {https://www.datacenterknowledge.com/microsoft/microsoft-live-streaming-its-underwater-data-center-scotland},
urldate = {2019-04-18},
year = {2018}
}
@inproceedings{SergiGirona2018,
author = {{Sergi Girona}},
booktitle = {ISC18},
organization = {ISC18},
title = {{EuroHPC, Exascale for Europe}},
url = {https://2018.isc-program.com/presentation/?id=inv{\_}sp108{\&}sess=sess120},
year = {2018}
}
@inproceedings{lo2014roofline,
abstract = {We present preliminary results of the Roofline Toolkit for multicore, manycore, and accelerated architectures. This paper focuses on the processor architecture characterization engine, a collection of portable instrumented micro benchmarks implemented with Message Passing Interface (MPI), and OpenMP used to express thread-level parallelism. These benchmarks are specialized to quantify the behavior of different architectural features. Compared to previous work on performance characterization, these microbenchmarks focus on capturing the performance of each level of the memory hierarchy, along with thread-level parallelism, instruction-level parallelism and explicit SIMD parallelism, measured in the context of the compilers and run-time environments. We also measure sustained PCIe throughput with four GPU memory managed mechanisms. By combining results from the architecture characterization with the Roofline model based solely on architectural specifications, this work offers insights for performance prediction of current and future architectures and their software systems. To that end, we instrument three applications and plot their resultant performance on the corresponding Roofline model when run on a Blue Gene/Q architecture.},
author = {Lo, Yu Jung and Williams, Samuel and {Van Straalen}, Brian and Ligocki, Terry J. and Cordery, Matthew J. and Wright, Nicholas J. and Hall, Mary W. and Oliker, Leonid},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-17248-4_7},
isbn = {9783319172477},
issn = {16113349},
keywords = {CUDA unified memory,Memory bandwidth,Roofline},
organization = {Springer},
pages = {129--148},
title = {{Roofline model toolkit: A practical tool for architectural and program analysis}},
volume = {8966},
year = {2015}
}
@techreport{Williams2018,
author = {Williams, Samuel and Deslippe, Jack and Gov, Jrdeslippe@lbl and Basu, Protonu and Yang, Charlene},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Williams et al. - Unknown - Performance Tuning of Scientific Codes with the Roofline Model.pdf:pdf},
title = {{Performance Tuning of Scientific Codes with the Roofline Model}},
url = {https://crd.lbl.gov/assets/Uploads/ECP18-Roofline-1-intro.pdf},
year = {2018}
}
@misc{Wikichipa,
author = {Wikichip},
title = {{Xeon Gold 6148 - Intel - WikiChip}},
url = {https://en.wikichip.org/wiki/intel/xeon{\_}gold/6148{\#}Frequencies},
urldate = {2019-04-11}
}
@misc{Aspsys,
author = {Aspsys},
title = {{Intel Xeon Scalable Family of Processors - High Performance Computers | Aspen Systems}},
url = {https://www.aspsys.com/solutions/hpc-processors/intel/},
urldate = {2019-04-11}
}
@incollection{brunst2013custom,
author = {Brunst, Holger and Weber, Matthias},
booktitle = {Tools for High Performance Computing 2012},
doi = {10.1007/978-3-642-37349-7_7},
pages = {95--114},
publisher = {Springer},
title = {{Custom Hot Spot Analysis of HPC Software with the Vampir Performance Tool Suite}},
year = {2013}
}
@techreport{Intel2017a,
author = {Intel},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Unknown - 2019 - Intel {\textregistered} Xeon {\textregistered} Processor Scalable Family Specification Update.pdf:pdf},
keywords = {336062,datasheet,electrical,xeon bronze processors,xeon gold processors,xeon platinum processors,xeon processor scalable family,xeon silver processors},
number = {July},
pages = {1--134},
title = {{Intel {\textregistered} Xeon Processor Scalable Family}},
url = {https://www.intel.com/content/dam/www/public/us/en/documents/specification-updates/xeon-scalable-spec-update.pdf},
year = {2017}
}
@incollection{little2008little,
author = {Little, John D C and Graves, Stephen C},
booktitle = {Building intuition},
pages = {81--100},
publisher = {Springer},
title = {{Little's law}},
year = {2008}
}
@misc{JohnMcCalpin2010,
author = {{John McCalpin}},
title = {{John McCalpin's blog » Blog Archive » Optimizing AMD Opteron Memory Bandwidth, Part 1: single-thread, read-only}},
url = {http://sites.utexas.edu/jdm4372/2010/11/03/optimizing-amd-opteron-memory-bandwidth-part-1-single-thread-read-only/},
urldate = {2019-04-08},
year = {2010}
}
@article{dolbeau2015theoretical,
abstract = {This is a companion paper to "Theoreti-cal Peak FLOPS per instruction set on modern Intel CPUs" [1]. In it, we survey some alternative hardware for which the peak FLOPS can be of interest. As in the main paper, we take into account and explain the peculiarities of the surveyed hardware.},
author = {Dolbeau, Romain},
keywords = {Index,Terms-FLOPS},
title = {{Theoretical Peak FLOPS per instruction set on less conventional hardware}},
url = {http://www.dolbeau.name/dolbeau/publications/peak-alt.pdf},
year = {2015}
}
@online{boson,
author = {{Daniel Bloch}},
title = {{Computing at the Large Hadron Collider in the CMS experiment}},
url = {http://www.iphc.cnrs.fr/IMG/pdf/bloch{\_}calculcms{\_}12mai15.pdf}
}
@article{camp2010trends,
abstract = {Among all scientific domains, geophysics is certainly one of the most computationally demanding, with probably the broadest requirements for per-formance and scalability.},
author = {Camp, William J. and Thierry, Philippe},
doi = {10.1190/1.3284052},
issn = {1070-485X},
journal = {The Leading Edge},
number = {1},
pages = {44--47},
publisher = {Society of Exploration Geophysicists},
title = {{Trends for high-performance scientific computing}},
volume = {29},
year = {2010}
}
@incollection{imbert2011tips,
abstract = {Thanks to the computer capability grow in the last decade, the FD method reappeared in 3D seismic to simulate the full two-way wave equation, together with 3D reverse time migration (RTM) and full waveform inversion (FWI). In addition to the optimized implementation of 3D stencils, the reverse time migration heavily relies on interconnect and i/o (input/output) subsystem to support domain decomposition communication as well as the necessary snapshots for cross correlation of the forward and backward wave-fields. In this paper, we summarize the different levels of optimization available to speedup stencil computations. We also define a model to estimate the actual efficiency of such algorithm. To optimize the FWI algorithm we revisit check-pointing algorithms based on Griewank (1992) that were already used in Computational fluid dynamics and in seismic (Symes, 2007) to avoid or limit the i/o. Such techniques are implemented using main memory and/or disk as a checkpoint device at the cost of some recomputation of the forward propagation. The key point is to use a fast i/o device. We also show that a dedicated hybrid parallel implementation using MPI and OpenMP can help to extend this model to larger workload and to keep almost all data in memory, still achieving good scalability: Instead of using large parallel filesystem, local fast i/o devices based on SSD technology can ensure the extra i/o needs. {\textcopyright} 2011 Society of Exploration Geophysicists.},
author = {Imadoueddine, Khadija and Borges, Leonardo and Imbert, David and Thierry, Philippe and Chauris, Herv{\'{e}}},
booktitle = {SEG Technical Program Expanded Abstracts 2011},
doi = {10.1190/1.3627855},
pages = {3174--3178},
publisher = {Society of Exploration Geophysicists},
title = {{Tips and tricks for finite difference and i/o‐less FWI}},
year = {2011}
}
@book{book,
author = {Hoisie, Adolfy and Kerbyson, Darren and Lucas, Robert and Rodrigues, Arun and Shalf, John and Vetter, Jeffrey and Harrod, William and Sachs, Sonia and Barker, Kevin and Belak, Jim and Bronevetsky, Greg and Carothers, Chris and Norris, Boyana and Yalamanchili, Sudhakar},
title = {{Report on the ASCR Workshop on Modeling and Simulation of Exascale Systems and Applications}},
url = {http://medcontent.metapress.com/index/A65RM03P4874243N.pdf},
year = {2012}
}
@phdthesis{khabou2013dense,
author = {Khabou, Amal},
title = {{Dense matrix computations : communication cost and numerical stability}},
year = {2013}
}
@inproceedings{barrett2012navigating,
abstract = {The computing community is in the midst of a disruptive architectural change. The advent of manycore and heterogeneous computing nodes forces us to reconsider every aspect of the system software and application stack. To address this challenge there is a broad spectrum of approaches, which we roughly classify as either revolutionary or evolutionary. With the former, the entire code base is re-written, perhaps using a new programming language or execution model. The latter, which is the focus of this work, seeks a piecewise path of effective incremental change. The end effect of our approach will be revolutionary in that the control structure of the application will be markedly different in order to utilize single-instruction multiple-data/thread (SIMD/SIMT), manycore and heterogeneous nodes, but the physics code fragments will be remarkably similar. Our approach is guided by a set of mission driven applications and their proxies, focused on balancing performance potential with the realities of existing application code bases. Although the specifics of this process have not yet converged, we find that there are several important steps that developers of scientific and engineering application programs can take to prepare for making effective use of these challenging platforms. Aiding an evolutionary approach is the recognition that the performance potential of the architectures is, in a meaningful sense, an extension of existing capabilities: vectorization, threading, and a re-visiting of node interconnect capabilities. Therefore, as architectures, programming models, and programming mechanisms continue to evolve, the preparations described herein will provide significant performance benefits on existing and emerging architectures. {\textcopyright} 2012 IEEE.},
author = {Barrett, R. F. and Hammond, S. D. and Vaughan, C. T. and Doerfler, D. W. and Heroux, M. A. and Luitjens, J. P. and Roweth, D.},
booktitle = {Proceedings - 2012 SC Companion: High Performance Computing, Networking Storage and Analysis, SCC 2012},
doi = {10.1109/SC.Companion.2012.55},
isbn = {9780769549569},
keywords = {scientific applications; high performance computin},
organization = {IEEE},
pages = {355--365},
title = {{Navigating an evolutionary fast path to exascale}},
year = {2012}
}
@phdthesis{farjallah2014preparing,
author = {Farjallah, Asma},
school = {Universit{\'{e}} de Versailles-Saint Quentin en Yvelines},
title = {{Preparing depth imaging applications for Exascale challenges and impacts}},
year = {2014}
}
@article{bergman2008exascale,
abstract = {This document reflects the thoughts of a group of highly talented individuals from universities, industry, and research labs on what might be the challenges in advancing computing by a thousand- fold by 2015. The work was sponsored by DARPA IPTO with Dr. William Harrod as Program Manager, under AFRL contract FA8650-07-C-7724. The report itself was drawn from the results of a series of meetings over the second half of 2007, and as such reflects a snapshot in time. The goal of the study was to assay the state of the art, and not to either propose a potential system or prepare and propose a detailed roadmap for its development. Further, the report itself was assembled in just a few months at the beginning of 2008 from input by the participants. As such, all inconsistencies reflect either areas where there really are significant open research questions, or misunderstandings by the editor. There was, however, virtually complete agreement about the key challenges that surfaced from the study, and the potential value that solving them may have towards advancing the field of high performance computing. I am honored to have been part of this study, and wish to thank the study members for their passion for the subject, and for contributing far more of their precious time than they expected.},
author = {Kogge, Peter and Bergman, K and Borkar, S and Campbell, Dan and Carson, W and Dally, W and Denneau, M and Franzon, P and Harrod, W and Hill, K and Hiller, Jon and Richards, Mark and Snavely, Allan},
doi = {10.1.1.165.6676},
isbn = {FA8650-07-C-7724},
journal = {(P. Kogge, Editor and Study Lead)},
keywords = {exascale,parallel architecture,parallel benchmarks},
pages = {1--278},
title = {{ExaScale Computing Study : Technology Challenges in Achieving Exascale Systems}},
url = {http://scholar.google.com/scholar?hl=en{\&}btnG=Search{\&}q=intitle:ExaScale+Computing+Study+:+Technology+Challenges+in+Achieving+Exascale+Systems{\#}0},
volume = {TR-2008-13},
year = {2008}
}
@misc{hall2011ascr,
abstract = {This paper reports on the results of a workshop on programming models, languages, compilers and runtime systems for exascale machines. The goal was to identify some of the challenges in each of these areas, the promising approaches that should be pursued, and measures to assess progress. The challenges derived from more complex systems with additional levels of memory, orders of magnitude more concurrency, and with less memory, memory bandwidth, and network bandwidth per core than today's petascale machines. Additional problems arise from heterogeneous processor architectures or accelerators, and the possible need for power and resilience management. The workshop participants considered a range of approaches, from evolutionary ones based on existing programming techniques to revolutionary ones that changed the basic concepts for creating and managing parallelism and locality. They also considered models that separated the intra and inter-node parallelization strategies and agreed that while there were challenges in both, the more daunting ones were within the node where most of the hardware disruptions would occur. As described in this paper the participants in the workshop articulated the research challenges in programming support for anticipated exascale systems, including specifying what is known and what remains uncertain.},
author = {Amarasinghe, Saman and Hall, Mary and Lethin, Richard and Pingali, Keshav and Quinlan, Dan and Sarkar, Vivek and Shalf, John and Lucas, Robert and Yelick, Katherine},
booktitle = {Exascale Programming Challenges, Marina del Rey, CA, USA, July},
publisher = {see},
title = {{ASCR programming challenges for exascale computing}},
year = {2011}
}
@phdthesis{batten2010simplified,
author = {Batten, Christopher Francis},
number = {May 1999},
school = {Massachusetts Institute of Technology},
title = {{Simplified Vector-Thread Architectures for Flexible and Efficient Data-Parallel Accelerators}},
year = {2010}
}
@inproceedings{mazouz2011performance,
abstract = {With the introduction of multi-core processors, thread affinity has quickly appeared to be one of the most important factors to accelerate program execution times. The current article presents a complete experimental study on the per- formance of various thread pinning strategies. We investi- gate four application independent thread pinning strategies and five application sensitive ones based on cache sharing. We made extensive performance evaluation on three different multi-core machines reflecting three usual utilisa- tion: workstation machine, server machine and high per- formance machine. In overall, we show that fixing thread affinities (whatever the tested strategy) is a better choice for improving program performance on HPC ccNUMA machines compared to OS-based thread placement. This means that the current Linux OS scheduling strategy is not necessarily the best choice in terms of performance on ccNUMA machines, even if it is a good choice in terms of cores usage ratio and work balancing. On smaller Core2 and Nehalem machines, we show that the benefit of thread pinning is not satisfactory in terms of speedups versus OS- based scheduling, but the performance stability is much better.},
author = {Mazouz, Abdelhafid and Touati, Sid Ahmed Ali and Barthou, Denis},
booktitle = {Proceedings of the 2011 International Conference on High Performance Computing and Simulation, HPCS 2011},
doi = {10.1109/HPCSim.2011.5999834},
isbn = {9781612843810},
keywords = {Multi-Cores,OpenMP,Operating Systems,Thread Affinity,Thread Level Parallelism},
organization = {IEEE},
pages = {273--279},
title = {{Performance evaluation and analysis of thread pinning strategies on multi-core platforms: Case study of SPEC OMP applications on intel architectures}},
year = {2011}
}
@inproceedings{de2010new,
author = {{De Melo}, Arnaldo Carvalho},
booktitle = {Slides from Linux Kongress},
title = {{The new linux'perf'tools}},
volume = {18},
year = {2010}
}
@inproceedings{charif2014cqa,
author = {Charif-Rubial, Andres S and Oseret, Emmanuel and Lartigue, Ghislain and Jalby, William},
booktitle = {21th Annual International Conference on High Performance Computing - HiPC'14},
organization = {IEEE},
pages = {1--10},
title = {{CQA: A Code Quality Analyzer tool at binary level using performance modeling and evaluation}},
url = {papers2://publication/uuid/8D4D0E54-EC8F-4AE5-BD4A-EC334C5D8F04},
year = {2014}
}
@inproceedings{ozisikyilmaz2008machine,
author = {Ozisikyilmaz, Berkin and Memik, Gokhan and Choudhary, Alok},
booktitle = {2008 37th International Conference on Parallel Processing},
organization = {IEEE},
pages = {495--502},
title = {{Machine learning models to predict performance of computer system design alternatives}},
year = {2008}
}
@article{reinders2005vtune,
author = {Reinders, James},
journal = {Intel Press},
title = {{VTune performance analyzer essentials}},
year = {2005}
}
@article{knuth1971empirical,
author = {Knuth, Donald E},
journal = {Software: Practice and experience},
number = {2},
pages = {105--133},
publisher = {Wiley Online Library},
title = {{An empirical study of FORTRAN programs}},
volume = {1},
year = {1971}
}
@inproceedings{wong2015vp3,
author = {Wong, David C and Kuck, David J and Palomares, David and Bendifallah, Zakaria and Tribalat, Mathieu and Oseret, Emmanuel and Jalby, William},
booktitle = {Workshop on Programming Models for SIMD/Vector Processing},
title = {{Vp3: A vectorization potential performance prototype}},
year = {2015}
}
@phdthesis{palomares2015combining,
author = {Palomares, Vincent},
school = {Universit{\'{e}} de Versailles-Saint Quentin en Yvelines},
title = {{Combining static and dynamic approaches to model loop performance in HPC}},
year = {2015}
}
@inproceedings{marin2004cross,
author = {Marin, Gabriel and Mellor-Crummey, John},
booktitle = {ACM SIGMETRICS Performance Evaluation Review},
number = {1},
organization = {ACM},
pages = {2--13},
title = {{Cross-architecture performance predictions for scientific applications using parameterized models}},
volume = {32},
year = {2004}
}
@book{bienia2011benchmarking,
author = {Bienia, Christian and Li, Kai},
publisher = {Princeton University Princeton},
title = {{Benchmarking modern multiprocessors}},
year = {2011}
}
@misc{Lattner2016,
abstract = {The Clang Static Analyzer is a source code analysis tool that finds bugs in C, C++, and Objective-C programs. Currently it can be run either as a standalone tool or within Xcode. The standalone tool is invoked from the command line, and is intended to be run in tandem with a build of a codebase. The analyzer is 100{\%} open source and is part of the Clang project. Like the rest of Clang, the analyzer is implemented as a C++ library that can be used by other tools and applications.},
author = {Lattner, Chris},
title = {{Clang Static Analyzer}},
url = {http://clang-analyzer.llvm.org/},
year = {2016}
}
@inproceedings{Djoudi2005,
author = {Djoudi, Lamia and Barthou, Denis},
booktitle = {Workshop on EPIC architectures and compiler technology},
title = {{Maqao: Modular assembler quality analyzer and optimizer for itanium 2}},
url = {http://www.researchgate.net/publication/228767331{\_}MAQAO{\_}Modular{\_}Assembler{\_}Quality{\_}Analyzer{\_}and{\_}Optimizer{\_}for{\_}Itanium{\_}2/file/50463519f7dbc47c4e.pdf},
year = {2005}
}
@article{sprunt2002basics,
author = {Sprunt, Brinkley},
doi = {10.1109/MM.2002.1028477},
issn = {0272-1732},
journal = {IEEE Micro},
number = {4},
pages = {64--71},
publisher = {IEEE},
title = {{The basics of performance-monitoring hardware}},
url = {http://ieeexplore.ieee.org/document/1028477/},
volume = {22},
year = {2002}
}
@inproceedings{popov2015pcere,
author = {Popov, Mihail and Akel, Chadi and Conti, Florent and Jalby, William and Castro, Pablo De Oliveira},
booktitle = {Proceedings - 2015 IEEE 29th International Parallel and Distributed Processing Symposium, IPDPS 2015},
doi = {10.1109/IPDPS.2015.19},
isbn = {9781479986484},
keywords = {Cross-architecture performance prediction,OpenMP applications,checkpoint restart,parallel code isolation,program replay,scalability prediction},
organization = {IEEE},
pages = {1151--1160},
title = {{PCERE: Fine-Grained Parallel Benchmark Decomposition for Scalability Prediction}},
year = {2015}
}
@article{ladd2004acovea,
author = {Ladd, Scott R},
journal = {Describing the EvolutionaryAlgorithm”, http://stderr. org/doc/acovea/html/acoveaga. html},
title = {{Acovea: Analysis of compiler options via evolutionary algorithm}},
year = {2004}
}
@inproceedings{dubach2008exploring,
abstract = {Embedded processor performance is dependent on both the underlying architecture and the compiler optimisations ap- plied. However, designing both simultaneously is extremely difficult to achieve due to the time constraints designers must work under. Therefore, current methodology involves designing compiler and architecture in isolation, leading to sub-optimal performance of the final product. This paper develops a novel approach to this co-design space problem. For any microarchitectural configuration we automatically predict the performance that an optimising compiler would achieve without actually building it. Once trained, a single run of -O1 on the new architecture is enough to make a prediction with just a 1.6{\%} error rate. This allows the designer to accurately choose an architectural configu- ration with knowledge of how an optimising compiler will perform on it. We use this to find the best optimising com- piler/architectural configuration in our co-design space and demonstrate that it achieves an average 13{\%} performance improvement and energy savings of 23{\%} compared to the baseline, leading to an energy-delay (ED) value of 0.67},
author = {Dubach, Christophe and Jones, Timothy M. and O'Boyle, Michael F.P.},
booktitle = {Proceedings of the 2008 international conference on Compilers, architectures and synthesis for embedded systems},
doi = {10.1145/1450095.1450103},
organization = {ACM},
pages = {31},
title = {{Exploring and predicting the architecture/optimising compiler co-design space}},
year = {2008}
}
@article{de1997hardware,
author = {Micheli, Giovanni De and Sami, Mariagiovanna},
isbn = {9780792338833},
journal = {Proceedings of the IEEE},
number = {3},
pages = {349--365},
publisher = {IEEE},
title = {{Hardware/Software Co-Design}},
volume = {85},
year = {1996}
}
@inproceedings{hoste2008cole,
author = {Hoste, Kenneth and Eeckhout, Lieven},
booktitle = {ACM conference on code generation and optimization},
doi = {10.1145/1356058.1356080},
isbn = {9781595939784},
keywords = {all or part of,compiler optimization,is granted without fee,multi-objective search,or hard copies of,permission to make digital,personal or classroom use,provided that copies are,this work for},
organization = {ACM},
pages = {165--174},
title = {{COLE : Compiler Optimization Level Exploration Categories and Subject Descriptors}},
year = {2008}
}
@article{castro2015cere,
abstract = {This article presents Codelet Extractor and REplayer (CERE), an open-source framework for code isolation. CERE finds and extracts the hotspots of an application as isolated fragments of code, called codelets. Codelets can be modified, compiled, run, and measured independently from the original application. Code isolation reduces benchmarking cost and allows piecewise optimization of an application. Unlike previous approaches, CERE isolates codes at the compiler Intermediate Representation (IR) level. Therefore CERE is language agnostic and supports many input languages such as C, C++, Fortran, and D. CERE automatically detects codelets invocations that have the same performance behavior. Then, it selects a reduced set of representative codelets and invocations, much faster to replay, which still captures accurately the original application. In addition, CERE supports recompiling and retargeting the extracted codelets. Therefore, CERE can be used for cross-architecture performance prediction or piecewise code optimization. On the SPEC 2006 FP benchmarks, CERE codelets cover 90.9{\%} and accurately replay 66.3{\%} of the execution time. We use CERE codelets in a realistic study to evaluate three different architectures on the NAS benchmarks. CERE accurately estimates each architecture performance and is 7.3 × to 46.6 × cheaper than running the full benchmark.},
author = {Castro, Pablo De Oliveira and Akel, Chadi and Petit, Eric and Popov, Mihail and Jalby, William},
doi = {10.1145/2724717},
issn = {15443566},
journal = {ACM Transactions on Architecture and Code Optimization},
keywords = {Program replay,checkpoint restart,iterative optimization,performance prediction},
number = {1},
pages = {1--24},
publisher = {ACM},
title = {{CERE: LLVM-Based Codelet Extractor and REplayer for Piecewise Benchmarking and Optimization}},
url = {http://dl.acm.org/citation.cfm?id=2744295.2724717},
volume = {12},
year = {2015}
}
@inproceedings{datta2008stencil,
abstract = {Understanding the most efficient design and utilization of emerging multicore systems is one of the most challenging questions faced by the mainstream and scientific computing industries in several decades. Our work explores multicore stencil (nearest-neighbor) computations -- a class of algorithms at the heart of many structured grid codes, including PDE solvers. We develop a number of effective optimization strategies, and build an auto-tuning environment that searches over our optimizations and their parameters to minimize runtime, while maximizing performance portability. To evaluate the effectiveness of these strategies we explore the broadest set of multicore architectures in the current HPC literature, including the Intel Clovertown, AMD Barcelona, Sun Victoria Falls, IBM QS22 PowerXCell 8i, and NVIDIA GTX280. Overall, our auto- tuning optimization methodology results in the fastest multicore stencil performance to date. Finally, we present several key insights into the architectural trade-offs of emerging multicore designs and their implications on scientific algorithm development.},
author = {Datta, Kaushik and Murphy, Mark and Volkov, Vasily and Williams, Samuel and Carter, Jonathan and Oliker, Leonid and Patterson, David and Shalf, John and Yelick, Katherine},
booktitle = {2008 SC - International Conference for High Performance Computing, Networking, Storage and Analysis, SC 2008},
doi = {10.1109/SC.2008.5222004},
isbn = {9781424428359},
issn = {1424428351},
organization = {IEEE Press},
pages = {4},
pmid = {3115053},
title = {{Stencil computation optimization and auto-tuning on state-of-the-art multicore architectures}},
year = {2008}
}
@phdthesis{popov:tel-01412638,
author = {Popov, Mihail},
keywords = {Optimization ; Performance prediction ; Parallelis},
month = {oct},
number = {2016SACLV087},
school = {Universit{\'{e}} Paris-Saclay},
title = {{Automatic decomposition of parallel programs for optimization and performance prediction.}},
type = {Theses},
url = {https://tel.archives-ouvertes.fr/tel-01412638},
year = {2016}
}
@inproceedings{yi2006exigency,
abstract = {Due to the amount of time required to design a new processor, one set of benchmark programs may be used during the design phase while another may be the standard when the design is finally delivered. Using one benchmark suite to design a processor while using a different, presumably more current, suite to evaluate its ultimate performance may lead to sub-optimal design decisions if there are large differences between the characteristics of the two suites and their respective compilers. We call this changes across time "drift". To evaluate the impact of using yesterday's benchmark and compiler technology to design tomorrow's processors, we compare common benchmarks from the SPEC 95 and SPEC 2000 benchmark suites. Our results yield three key conclusions. First, we show that the amount of drift, for common programs in successive SPEC benchmark suites, is significant. In SPEC 2000, the main memory access time is a far more significant performance bottleneck than in SPEC 95, while less significant SPEC 2000 performance bottlenecks include the L2 cache latency, the L1 I-cache size, and the number of reorder buffer entries. Second, using two different statistical techniques, we show that compiler drift is not as significant as benchmark drift. Third, we show that benchmark and compiler drift can have a significant impact on the final design decisions. Specifically, we use a one-parameter-at-a-time optimization algorithm to design two different year-2000 processors, one optimized for SPEC 95 and the other optimized for SPEC 2000, using the energy-delay product (EDP) as the optimization criterion. The results show that using SPEC 95 to design a year-2000 processor results in an 18.5{\%} larger EDP and a 20.8{\%} higher CPI than using the SPEC 2000 benchmarks to design the corresponding processor. Finally, we make a few recommendations to help computer architects minimize the effects of benchmark and compiler drift. Copyright ? 2006 ACM.},
author = {Yi, J.J. and Vandierendonck, H. and Eeckhout, L. and Lilja, D.J.},
booktitle = {Proceedings of the International Conference on Supercomputing},
doi = {10.1145/1183401.1183414},
isbn = {1595932828 | 9781595932822},
keywords = {[Benchmark drift, Compiler drift, Microprocessor d},
organization = {ACM},
pages = {75--86},
title = {{The exigency of benchmark and compiler drift: Designing tomorrow's processors with yesterday's tools}},
year = {2006}
}
@inproceedings{Villa2014,
abstract = {Modern scientific discovery is driven by an insatiable demand for computing performance. The HPC community is targeting development of supercomputers able to sustain 1 ExaFlops by the year 2020 and power consumption is the primary obstacle to achieving this goal. A combination of architectural improvements, circuit design, and manufacturing technologies must provide over a 20× improvement in energy efficiency. In this paper, we present some of the progress NVIDIA Research is making toward the design of Exascale systems by tailoring features to address the scaling challenges of performance and energy efficiency. We evaluate several architectural concepts for a set of HPC applications demonstrating expected energy efficiency improvements resulting from circuit and packaging innovations such as low-voltage SRAM, low-energy signalling, and on-package memory. Finally, we discuss the scaling of these features with respect to future process technologies and provide power and performance projections for our Exascale research architecture.},
author = {Villa, Oreste and Johnson, Daniel R. and O'Connor, Mike and Bolotin, Evgeny and Nellans, David and Luitjens, Justin and Sakharnykh, Nikolai and Wang, Peng and Micikevicius, Paulius and Scudiero, Anthony and Keckler, Stephen W. and Dally, William J.},
booktitle = {International Conference for High Performance Computing, Networking, Storage and Analysis, SC},
doi = {10.1109/SC.2014.73},
isbn = {1479955000},
issn = {21674337},
number = {January},
pages = {830--841},
publisher = {IEEE Press},
title = {{Scaling the Power Wall: A Path to Exascale}},
volume = {2015-Janua},
year = {2014}
}
@article{Messina2017,
abstract = {The US Department of Energy's (DOE) Exascale Computing Project is a partnership between the DOE Office of Science and the National Nuclear Security Administration. Its mission is to transform today's high-performance computing (HPC) ecosystem by executing a multifaceted plan: developing mission-critical applications of unprecedented complexity; supporting US national security initiatives; partnering with the US HPC industry to develop exascale computer architectures; collaborating with US software vendors to develop a software stack that is both exascale-capable and usable on US industrial- and academic-scale systems; and training the next-generation workforce of computer and computational scientists, engineers, mathematicians, and data scientists},
author = {Messina, Paul},
doi = {10.1109/MCSE.2017.57},
issn = {15219615},
journal = {Computing in Science and Engineering},
keywords = {Exascale,HPC,co-design,scientific computing},
number = {3},
pages = {63--67},
title = {{The Exascale Computing Project}},
volume = {19},
year = {2017}
}
@incollection{Treibig2012a,
author = {Treibig, Jan and Hager, Georg and Wellein, Gerhard},
booktitle = {Proceedings of the 5th International Workshop on Parallel Tools for High Performance Computing 2011},
doi = {10.1007/978-3-642-31476-6_3},
isbn = {9783642314759},
pages = {27--36},
publisher = {Springer},
title = {{Likwid-bench: An extensible microbenchmarking platform for x86 multicore compute nodes}},
year = {2012}
}
@article{Wilkes2001,
author = {Wilkes, Maurice V},
issn = {0163-5964},
journal = {ACM SIGARCH Computer Architecture News},
number = {1},
pages = {2--7},
title = {{The memory gap and the future of high performance memories}},
volume = {29},
year = {2001}
}
@article{Rojas1997,
author = {Rojas, Raul},
doi = {10.1016/S0172-2190(97)90117-4},
issn = {01722190},
journal = {IEEE Annals of the History of Computing},
number = {2},
pages = {5},
publisher = {IEEE},
title = {{The Architecture of the Z1 and Z3}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0172219097901174},
volume = {19},
year = {1997}
}
@inproceedings{Gurel2018,
author = {Gurel, Levent},
booktitle = {2018 International Workshop on Computing, Electromagnetics, and Machine Intelligence (CEMi)},
doi = {10.1109/CEMI.2018.8610529},
isbn = {978-1-5386-7845-9},
pages = {17--18},
publisher = {IEEE},
title = {{Towards Exascale Computing for Autonomous Driving}},
url = {https://ieeexplore.ieee.org/document/8610529/},
year = {2018}
}
@article{Reed2015,
abstract = {The article examines some of the technical challenges of exascale computing and big data. Topics discussed include the history of advanced computing, the interdependence of computational modeling and data analytics, and the global ecosystem and competition for leadership in advanced computing. Also mentioned are the tools and cultures of high-performance computing and big data analystics, the international nature of science that demands further development of advanced computer architectures, and the need for global standards for data processing.},
archivePrefix = {arXiv},
arxivId = {1309.5821},
author = {Reed, Daniel A. and Dongarra, Jack},
doi = {10.1145/2699414},
eprint = {1309.5821},
isbn = {0001-0782},
issn = {00010782},
journal = {Communications of the ACM},
number = {7},
pages = {56--68},
publisher = {ACM},
title = {{Exascale computing and big data}},
url = {http://dl.acm.org/citation.cfm?doid=2797100.2699414},
volume = {58},
year = {2015}
}
@article{LeCun1989,
author = {LeCun, Yann},
journal = {Connectionism in perspective},
pages = {143--155},
publisher = {Citeseer},
title = {{Generalization and network design strategies}},
year = {1989}
}
@inproceedings{Becker1988,
author = {Becker, Sue and {Le Cun}, Yann},
booktitle = {Proceedings of the 1988 connectionist models summer school},
pages = {29--37},
publisher = {San Matteo, CA: Morgan Kaufmann},
title = {{Improving the convergence of back-propagation learning with second order methods}},
year = {1988}
}
@misc{Top5002018,
author = {Top500},
title = {{Top500 - projected Performance Development}},
url = {https://www.top500.org/statistics/perfdevel/},
year = {2018}
}
@inproceedings{Khaleel2008,
author = {Khaleel, M. A. and Johnson, G. M. and Washington, W. M.},
booktitle = {Extreme Scale 2009},
pages = {98},
title = {{Scientific Grand Challenges: Challenges in Climate Change Science and the Role of Computing At the Extreme Scale}},
url = {http://esg-pcmdi.llnl.gov/publications{\_}and{\_}documents/Extreme{\_}Scale{\_}Data{\_}Mgmt{\_}Panel Report.pdf},
year = {2008}
}
@inproceedings{Kettan,
abstract = {In the past thirty years, advances in high performance computing have increased the performance by million times, and decreased the volume of the machine by similar order. 1 Accordingly, the fastest computer in the world increased its performance from one Gigaflop/s in mid-1980s to a projected one Exaflop/s by 2020. In addition, current hand-held devices such as smartphones have performance that rivals those machines of the 1980s. Due to hardware limitations, parallel computing became an integral part of our lives that it is hard to imagine a device that is not using multiprocessor power, including smartphones. What started as a hardware solution to physical limitation, prompted software engineers to adopt to parallelism, which also motivates the theoretical solution to algorithms design and analysis to provide a solution that is parallel oriented rather than a serial oriented one.},
author = {Kettani, Houssain},
booktitle = {Proceedings of the 2017 International Conference on Smart Digital Environment - ICSDE '17},
doi = {10.1145/3128128.3128163},
isbn = {9781450352819},
pages = {229--231},
title = {{Advances in high performance computing and their impact on smart cities}},
url = {http://dl.acm.org/citation.cfm?doid=3128128.3128163},
year = {2017}
}
@misc{Top500,
author = {Top500},
title = {{Efficiency, Power, Cores... | TOP500 Supercomputer Sites}},
url = {https://www.top500.org/statistics/efficiency-power-cores/},
urldate = {2018-12-14}
}
@inproceedings{Luszczek2006,
author = {Luszczek, Piotr R and Bailey, David H and Dongarra, Jack J and Kepner, Jeremy and Lucas, Robert F and Rabenseifner, Rolf and Takahashi, Daisuke},
booktitle = {Proceedings of the 2006 ACM/IEEE conference on Supercomputing},
publisher = {Citeseer},
title = {{The HPC Challenge (HPCC) benchmark suite}},
volume = {213},
year = {2006}
}
@article{Strohmaier2015,
abstract = {For more than two decades, the TOP500 list has enjoyed incredible success as a metric for supercomputing performance and as a source of data for identifying technological trends. The project's editors reflect on its usefulness and limitations for guiding large-scale scientific computing into the exascale era.},
author = {Strohmaier, Erich and Meuer, Hans W. and Dongarra, Jack and Simon, Horst D.},
doi = {10.1109/MC.2015.338},
issn = {00189162},
journal = {Computer},
keywords = {Application performance,Linpack,TOP500,benchmarks,high-performance computing,parallel computing,scientific computing,supercomputers},
number = {11},
pages = {42--49},
publisher = {IEEE},
title = {{The TOP500 List and Progress in High-Performance Computing}},
volume = {48},
year = {2015}
}
@techreport{Ang2016,
author = {Ang, James A},
publisher = {Sandia National Lab.(SNL-NM), Albuquerque, NM (United States)},
title = {{Exascale System and Node Architectures}},
year = {2016}
}
@phdthesis{Molka2017b,
abstract = {Systems for high performance computing are getting increasingly complex. On the one hand, the number of processors is increasing. On the other hand, the individual processors are getting more and more powerful. In recent years, the latter is to a large extent achieved by increasing the number of cores per processor. Unfortunately, scientific applications often fail to fully utilize the available computational performance. Therefore, performance analysis tools that help to localize and fix performance problems are indispensable. Large scale systems for high performance computing typically consist of multiple compute nodes that are connected via network. Performance analysis tools that analyze performance problems that arise from using multiple nodes are readily available. However, the increasing number of cores per processor that can be observed within the last decade represents a major change in the node architecture. Therefore, this work concentrates on the analysis of the node performance. The goal of this thesis is to improve the understanding of the achieved application performance on existing hardware. It can be observed that the scaling of parallel applications on multi-core processors differs significantly from the scaling on multiple processors. Therefore, the properties of shared resources in contemporary multi-core processors as well as remote accesses in multi-processor systems are investigated and their respective impact on the application performance is analyzed. As a first step, a comprehensive suite of highly optimized micro-benchmarks is developed. These benchmarks are able to determine the performance of memory accesses depending on the location and coherence state of the data. They are used to perform an in-depth analysis of the characteristics of memory accesses in contemporary multi-processor systems, which identifies potential bottlenecks. However, in order to localize performance problems, it also has to be determined to which extend the application performance is limited by certain resources. Therefore, a methodology to derive metrics for the utilization of individual components in the memory hierarchy as well as waiting times caused by memory accesses is developed in the second step. The approach is based on hardware performance counters, which record the number of certain hardware events. The developed micro-benchmarks are used to selectively stress individual components, which can be used to identify the events that provide a reasonable assessment for the utilization of the respective component and the amount of time that is spent waiting for memory accesses to complete. Finally, the knowledge gained from this process is used to implement a visualization of memory related performance issues in existing performance analysis tools. The results of the micro-benchmarks reveal that the increasing number of cores per processor and the usage of multiple processors per node leads to complex systems with vastly different performance characteristics of memory accesses depending on the location of the accessed data. Furthermore, it can be observed that the aggregated throughput of shared resources in multi-core processors does not necessarily scale linearly with the number of cores that access them concurrently, which limits the scalability of parallel applications. It is shown that the proposed methodology for the identification of meaningful hardware performance counters yields useful metrics for the localization of memory related performance limitations. 

(PDF) Performance Analysis of Complex Shared Memory Systems. Available from: https://www.researchgate.net/publication/315703632{\_}Performance{\_}Analysis{\_}of{\_}Complex{\_}Shared{\_}Memory{\_}Systems [accessed Jun 28 2018].},
author = {{Daniel Molka}},
month = {mar},
title = {{Performance Analysis of Complex Shared Memory Systems}},
url = {https://www.researchgate.net/publication/315703632{\_}Performance{\_}Analysis{\_}of{\_}Complex{\_}Shared{\_}Memory{\_}Systems},
year = {2017}
}
@techreport{Bailey1997,
abstract = {This note discuses Little's law and relates the form cited in queuing theory with a form often cited in the field of high performance computing. A rigorous mathematical proof of Little's law is included.},
author = {Bailey, David H},
booktitle = {Performance Computing},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Bailey - 1997 - Little's Law and High Performance Computing.pdf:pdf},
pages = {1--3},
title = {{Little ' s Law and High Performance Computing}},
url = {http://www.tera.com/arpa95/architecture.html.},
year = {1997}
}
@article{Mei2017,
abstract = {Memory access efficiency is a key factor in fully utilizing the computational power of graphics processing units (GPUs). However, many details of the GPU memory hierarchy are not released by GPU vendors. In this paper, we propose a novel fine-grained microbenchmarking approach and apply it to three generations of NVIDIA GPUs, namely Fermi, Kepler and Maxwell, to expose the previously unknown characteristics of their memory hierarchies. Specifically, we investigate the structures of different GPU cache systems, such as the data cache, the texture cache and the translation look-aside buffer (TLB). We also investigate the throughput and access latency of GPU global memory and shared memory. Our microbenchmark results offer a better understanding of the mysterious GPU memory hierarchy, which will facilitate the software optimization and modelling of GPU architectures. To the best of our knowledge, this is the first study to reveal the cache properties of Kepler and Maxwell GPUs, and the superiority of Maxwell in shared memory performance under bank conflict.},
author = {Mei, Xinxin and Chu, Xiaowen},
doi = {10.1109/TPDS.2016.2549523},
issn = {10459219},
journal = {IEEE Transactions on Parallel and Distributed Systems},
keywords = {CUDA,GPU,cache structure,memory hierarchy,throughput},
number = {1},
pages = {72--86},
publisher = {IEEE},
title = {{Dissecting GPU Memory Hierarchy Through Microbenchmarking}},
volume = {28},
year = {2017}
}
@techreport{Honarmand2016,
author = {Honarmand, Nima},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Honarmand - 2016 - Memory Prefetching.pdf:pdf},
title = {{Memory Prefetching}},
url = {https://pdfs.semanticscholar.org/presentation/b419/8bbc5b4d5436e516a4e3668bc54498724d5e.pdf},
year = {2016}
}
@inproceedings{Ainsworth2017,
author = {Ainsworth, Sam and Jones, Timothy M},
booktitle = {Proceedings of the 2017 International Symposium on Code Generation and Optimization},
isbn = {1509049312},
pages = {305--317},
publisher = {IEEE Press},
title = {{Software prefetching for indirect memory accesses}},
year = {2017}
}
@misc{Diffen,
author = {Diffen},
title = {{SRAM vs. DRAM}},
url = {https://www.diffen.com/difference/Dynamic{\_}random-access{\_}memory{\_}vs{\_}Static{\_}random-access{\_}memory}
}
@article{Mandelman2002,
abstract = {Significant challenges face DRAM scaling toward and beyond the 0.10-µm generation. Scaling techniques used in earlier generations for the array-access transistor and the storage capacitor are encountering limitations which necessitate major innovation in electrical operating mode, structure, and processing. Although a variety of options exist for advancing the technology, such as low-voltage operation, vertical MOSFETs, and novel capacitor structures, uncertainties exist about which way to proceed. This paper discusses the interrelationships among the DRAM scaling requirements and their possible solutions. The emphasis is on trench-capacitor DRAM technology.},
author = {Mandelman, J. A. and Dennard, R. H. and Bronner, G. B. and DeBrosse, J. K. and Divakaruni, R. and Li, Y. and Radens, C. J.},
doi = {10.1147/rd.462.0187},
isbn = {0018-8646},
issn = {0018-8646},
journal = {IBM Journal of Research and Development},
month = {mar},
number = {2.3},
pages = {187--212},
title = {{Challenges and future directions for the scaling of dynamic random-access memory (DRAM)}},
url = {http://ieeexplore.ieee.org/document/5388990/},
volume = {46},
year = {2002}
}
@misc{Lavenier2016,
author = {Lavenier, Dominique and Deltel, Charles and Furodet, David and Roy, Jean-Fran{\c{c}}ois},
publisher = {INRIA Rennes-Bretagne Atlantique},
title = {{BLAST on UPMEM}},
year = {2016}
}
@article{Hameed2014,
abstract = {Memory speed has become a major performance bottleneck as more and more cores are integrated on a multi-core chip. The widening latency gap between high speed cores and memory has led to the evolution of multi-level SRAM/DRAM cache hierarchies that exploit the latency benefits of smaller caches (e.g. private L1 and L2 SRAM caches) and the capacity benefits of larger caches (e.g. shared L3 SRAM and shared L4 DRAM cache). The main problem of employing large L3/L4 caches is their high tag lookup latency. To solve this problem, we introduce the novel concept of small and low latency SRAM/DRAM Tag-Cache structures that can quickly determine whether an access to the large L3/L4 caches will be a hit or a miss. The performance of the proposed Tag-Cache architecture depends upon the Tag-Cache hit rate and to improve it we propose a novel Tag-Cache insertion policy and a DRAM row buffer mapping policy that reduce the latency of memory requests. For a 16-core system, this improves the average harmonic mean instruction per cycle throughput of latency sensitive applications by 13.3{\%} compared to state-of-the-art.},
address = {New York, New York, USA},
author = {Hameed, Fazal and Bauer, Lars and Henkel, J{\"{o}}rg},
doi = {10.1145/2593069.2593197},
isbn = {9781450327305},
issn = {0738100X},
journal = {Proceedings of the The 51st Annual Design Automation Conference on Design Automation Conference - DAC '14},
pages = {1--6},
publisher = {ACM Press},
title = {{Reducing Latency in an SRAM/DRAM Cache Hierarchy via a Novel Tag-Cache Architecture}},
url = {http://dl.acm.org/citation.cfm?doid=2593069.2593197},
year = {2014}
}
@article{Mellor2018,
author = {Mellor, Chris},
doi = {10.1007/s11432-018-9404-2},
journal = {Science China Information Sciences},
month = {aug},
number = {8},
pages = {081302},
title = {{Chinese boffins on 3D XPoint: If it works like phase-change memory, it's probably phase-change memory}},
url = {http://link.springer.com/10.1007/s11432-018-9404-2},
volume = {61},
year = {2018}
}
@misc{Lapedus,
author = {Lapedus, Mark},
title = {{Next-Gen Memory Ramping Up}},
url = {https://semiengineering.com/next-gen-memory-ramping-up/},
urldate = {2018-09-13},
year = {2018}
}
@misc{Handly2018,
author = {Handly, Jim},
title = {{Why Are NVDIMMs Suddenly Hot?}},
url = {https://www.electronicdesign.com/industrial-automation/why-are-nvdimms-suddenly-hot},
urldate = {2018-09-13},
year = {2018}
}
@misc{Handy2017,
author = {Handy, Jim},
title = {{An NVDIMM Primer (Part 1 of 2) | The SSD Guy}},
url = {http://8ea.590.myftpupload.com/an-nvdimm-primer-part-1-of-2/},
urldate = {2018-09-13},
year = {2017}
}
@misc{PMEM,
author = {PMEM},
title = {{PMEM Persistent Memory Programming library}},
url = {http://pmem.io/},
urldate = {2018-09-12}
}
@techreport{Handy2016,
author = {Handy, Jim},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Handy - 2016 - WHY WAIT FOR STORAGE CLASS MEMORY.pdf:pdf},
title = {{WHY WAIT FOR STORAGE CLASS MEMORY?}},
url = {www.OBJECTIVE-ANALYSIS.com},
year = {2016}
}
@misc{Sivaram2016,
author = {Sivaram, Siva},
title = {{Storage Class Memory: Lessons Learned from 3D NAND}},
url = {https://blog.westerndigital.com/storage-class-memory-3d-nand-lessons/},
urldate = {2018-09-12},
year = {2016}
}
@article{Boroumand2017,
author = {Boroumand, Amirali and Ghose, Saugata and Patel, Minesh and Hassan, Hasan and Lucia, Brandon and Hsieh, Kevin and Malladi, Krishna T. and Zheng, Hongzhong and Mutlu, Onur},
doi = {10.1109/LCA.2016.2577557},
issn = {1556-6056},
journal = {IEEE Computer Architecture Letters},
month = {jan},
number = {1},
pages = {46--50},
title = {{LazyPIM: An Efficient Cache Coherence Mechanism for Processing-in-Memory}},
url = {http://ieeexplore.ieee.org/document/7485993/},
volume = {16},
year = {2017}
}
@article{Gokhale1995,
abstract = {SRC researchers have designed and fabricated a processor-in-memory$\backslash$n(PIM) chip, a standard 4-bit memory augmented with a single-bit ALU$\backslash$ncontrolling each column of memory. In principle, PIM chips can replace$\backslash$nthe memory of any processor, including a supercomputer. To validate the$\backslash$nnotion of integrating SIMD computing into conventional processors on a$\backslash$nmore modest scale, we have built a half dozen Terasys workstations,$\backslash$nwhich are Sun Microsystems Sparcstation-2 workstations in which 8$\backslash$nmegabytes of address space consist of PIM memory holding 32K single-bit$\backslash$nALUs. We have designed and implemented a high-level parallel language,$\backslash$ncalled data parallel bit C (dbC), for Terasys and demonstrated that dbC$\backslash$napplications using the PIM memory as a SIMD array run at the speed of$\backslash$nmultiple Cray-YMP processors. Thus, we can deliver supercomputer$\backslash$nperformance for a small fraction of supercomputer cost. Since the$\backslash$nsuccessful creation of the Terasys research prototype, we have begun$\backslash$nwork on processing in memory in a supercomputer setting. In a$\backslash$ncollaborative research project, we are working with Cray Computer to$\backslash$nincorporate a new Cray-designed implementation of the PIM chips into two$\backslash$noctants of Cray-3 memory},
author = {Gokhale, Maya and Holmes, Bill and Iobst, Ken},
doi = {10.1109/2.375174},
issn = {00189162},
journal = {Computer},
month = {apr},
number = {4},
pages = {23--31},
title = {{Processing in Memory: The Terasys Massively Parallel PIM Array}},
url = {http://ieeexplore.ieee.org/document/375174/},
volume = {28},
year = {1995}
}
@misc{Wikichip,
author = {Wikichip},
title = {{ThruChip Interface (TCI) - WikiChip}},
url = {https://en.wikichip.org/wiki/thruchip{\_}interface},
urldate = {2018-09-11}
}
@misc{Thruchip2018,
author = {Thruchip},
title = {{Thruchip Communications Main Site}},
url = {http://www.thruchip.com/tsv-compare.htm},
urldate = {2018-09-11},
year = {2018}
}
@misc{JDEC2016,
author = {JDEC},
title = {{JEDEC Updates Groundbreaking High Bandwidth Memory (HBM) Standard}},
url = {https://www.jedec.org/news/pressreleases/jedec-updates-groundbreaking-high-bandwidth-memory-hbm-standard},
urldate = {2018-09-10},
year = {2016}
}
@book{Kirk2016,
author = {Kirk, David B and Wen-Mei, W Hwu},
isbn = {012811987X},
publisher = {Morgan kaufmann},
title = {{Programming massively parallel processors: a hands-on approach}},
year = {2016}
}
@article{AMD,
author = {AMD},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/IBM - Unknown - HBM REINVENTING MEMORY TECHNOLOGY.pdf:pdf},
title = {{HBM: REINVENTING MEMORY TECHNOLOGY}},
url = {https://www.amd.com/Documents/High-Bandwidth-Memory-HBM.pdf}
}
@misc{Hill,
abstract = {The 3D XPoint technology that forms the basis of Optane was co-developed by Intel and Micron, and the pair have announced that a second-generation of the non-volatile, low-latency memory is currently in development.},
author = {Hill, Brandon},
publisher = {HotHardware},
title = {{Intel And Micron Announce Partnership For Second Gen 3D XPoint Memory To Further Advance Optane Storage}},
url = {https://hothardware.com/news/intel-micron-3d-xpoint-optane-2nd-gen},
urldate = {2018-09-10}
}
@misc{Cutress2018,
author = {Cutress, IAN},
title = {{Intel Launches Optane DIMMs Up To 512GB: Apache Pass Is Here!}},
url = {https://www.anandtech.com/show/12828/intel-launches-optane-dimms-up-to-512gb-apache-pass-is-here},
urldate = {2018-09-10},
year = {2018}
}
@misc{McCalpin,
author = {McCalpin, John},
title = {{Calculate the Max Flops on Skylake}},
url = {https://software.intel.com/en-us/forums/software-tuning-performance-optimization-platform-monitoring/topic/761046},
urldate = {2018-09-04},
year = {2018}
}
@techreport{Gervasi2018,
author = {Gervasi, Bill},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Gervasi - 2018 - Carbon Nanotube Memory.pdf:pdf},
title = {{Carbon Nanotube Memory}},
url = {https://www.snia.org/sites/default/files/PM-Summit/2018/presentations/12{\_}A{\_}PMSummit{\_}18{\_}Gervasi{\_}Final{\_}Post.pdf},
year = {2018}
}
@inproceedings{GervasiBill2018,
author = {{Gervasi Bill}},
title = {{Architecture for Carbon Nanotube Based Memory (NRAM)}},
url = {https://www.anandtech.com/show/13252/hot-chips-2018-nanotubes-as-dram-by-nantero-live-blog},
year = {2018}
}
@inproceedings{Bette2003,
author = {Bette, A and DeBrosse, J and Gogl, D and Hoenigschmid, H and Robertazzi, R and Arndt, C and Braun, D and Casarotto, D and Havreluk, R and Lammers, S},
booktitle = {VLSI Circuits, 2003. Digest of Technical Papers. 2003 Symposium on},
isbn = {4891140348},
pages = {217--220},
publisher = {IEEE},
title = {{A high-speed 128 Kbit MRAM core for future universal memory applications}},
year = {2003}
}
@misc{Mertens2004,
author = {Mertens, Ron},
title = {{Micron drops MRAM, phase-change, mulls other memories | MRAM-Info}},
url = {https://www.mram-info.com/technical{\_}research/micron{\_}drops{\_}mram{\_}phase{\_}change{\_}mulls{\_}other{\_}memories},
year = {2004}
}
@book{Redaelli,
abstract = {This book describes the physics of phase change memory devices, starting from basic operation to reliability issues. The book gives a comprehensive overlook of PCM with particular attention to the electrical transport and the phase transition physics between the two states. The book also contains design engineering details on PCM cell architecture, PCM cell arrays (including electrical circuit management), as well as the full spectrum of possible future applications. Chapter 1. Memory overview and PCM introduction -- Chapter 2.Electrical transport in crystalline and amorphous chalcogenides -- Chapter 3.Thermal model and remarkable temperature effects on calcogenide alloys -- Chapter 4.Self-consistent numerical model -- Chapter 5.PCM main reliability features -- Chapter 6.Structure and properties of chalcogenide materials for PCM -- Chapter 7.Material Engineering for PCM Device Optimization -- Chapter 8.PCM scaling -- Chapter 9.PCM device design -- Chapter 10.PCM array architecture and management -- Chapter 11. PCM applications and an outlook to the future.},
author = {Redaelli, Andrea},
isbn = {9783319690537},
pages = {342},
title = {{Phase change memory : device physics, reliability and applications}}
}
@article{Nowak2016,
author = {Nowak, Janusz J and Robertazzi, Ray P and Sun, Jonathan Z and Hu, Guohan and Park, Jeong-Heon and Lee, JungHyuk and Annunziata, Anthony J and Lauer, Gen P and Kothandaraman, Raman and O'Sullivan, Eugene J},
issn = {1949-307X},
journal = {IEEE Magnetics Letters},
pages = {1--4},
publisher = {IEEE},
title = {{Dependence of voltage and size on write error rates in spin-transfer torque magnetic random-access memory}},
volume = {7},
year = {2016}
}
@misc{Alvarez-Herault2010,
author = {Alvarez-H{\'{e}}rault, J{\'{e}}r{\'{e}}my},
publisher = {Universit{\'{e}} de Grenoble},
title = {{M{\'{e}}moire magn{\'{e}}tique {\`{a}} {\'{e}}criture par courant polaris{\'{e}} en spin assist{\'{e}}e thermiquement}},
year = {2010}
}
@inproceedings{Versari2001,
author = {Versari, Roberto and Esseni, David and Falavigna, Gianluca and Lanzoni, Massimo and Ricc{\`{o}}, Bruno},
booktitle = {Electronics, Circuits and Systems, 2001. ICECS 2001. The 8th IEEE International Conference on},
isbn = {0780370570},
pages = {945--948},
publisher = {IEEE},
title = {{Optimized programming of multilevel flash EEPROMs}},
volume = {2},
year = {2001}
}
@misc{Ricart2008,
author = {Ricart, Thibault},
publisher = {Universit{\'{e}} Paul Sabatier-Toulouse III},
title = {{Etude de nano-syst{\`{e}}mes {\'{e}}lectro-m{\'{e}}caniques (NEMS) {\`{a}} base de nanotubes de carbone pour applications hyperfr{\'{e}}quences}},
year = {2008}
}
@inproceedings{Webb2018,
author = {Webb, Mark},
booktitle = {Flash Memory Summit},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Webb - 2018 - Advances in Persistent Memories Markets and Applications.pdf:pdf},
title = {{Advances in Persistent Memories: Markets and Applications}},
url = {https://www.flashmemorysummit.com/English/Collaterals/Proceedings/2018/20180807{\_}PMEM-101-1{\_}Webb.pdf},
year = {2018}
}
@techreport{SivaSivaram2016,
abstract = {EVP of Memory Technology,
Western Digital Corporation},
author = {{Siva Sivaram}},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Siva Sivaram - 2016 - Storage Class Memory Learning from 3D NAND.pdf:pdf},
title = {{Storage Class Memory: Learning from 3D NAND}},
url = {https://www.flashmemorysummit.com/English/Collaterals/Proceedings/2016/20160809{\_}Keynote4{\_}WD{\_}Sivaram.pdf},
year = {2016}
}
@incollection{Torelli1999,
address = {Boston, MA},
author = {Torelli, Guido and Lanzoni, Massimo and Manstretta, Alessandro and Ricc{\`{o}}, Bruno},
booktitle = {Flash Memories},
doi = {10.1007/978-1-4615-5015-0_6},
pages = {361--397},
publisher = {Springer US},
title = {{Multilevel Flash Memories}},
url = {http://link.springer.com/10.1007/978-1-4615-5015-0{\_}6},
year = {1999}
}
@inproceedings{Burr2010a,
author = {Burr, Geoffrey W},
booktitle = {Non-volatile Memories Workshop},
pages = {1--25},
title = {{Storage Class Memory}},
year = {2010}
}
@book{Przybylski1990,
author = {Przybylski, Steven A},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Storage Class Memory Towards a disruptively low-cost solid-state non-volatile memory.pdf:pdf},
isbn = {1558601368},
publisher = {Morgan Kaufmann},
title = {{Cache and memory hierarchy design: a performance-directed approach}},
year = {1990}
}
@article{Mahapatra1999,
author = {Mahapatra, Nihar R and Venkatrao, Balakrishna},
issn = {1528-4972},
journal = {Crossroads},
number = {3es},
pages = {2},
publisher = {ACM},
title = {{The processor-memory bottleneck: problems and solutions}},
volume = {5},
year = {1999}
}
@inproceedings{Lam2010,
author = {Lam, Chung H},
booktitle = {Solid-State and Integrated Circuit Technology (ICSICT), 2010 10th IEEE International Conference on},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Storage Class Memory.pdf:pdf},
isbn = {142445798X},
pages = {1080--1083},
publisher = {IEEE},
title = {{Storage class memory}},
year = {2010}
}
@article{Freitas2008,
author = {Freitas, R. F. and Wilcke, W. W.},
doi = {10.1147/rd.524.0439},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Storage-class memory- The next storage system technology.pdf:pdf},
issn = {0018-8646},
journal = {IBM Journal of Research and Development},
month = {jul},
number = {4.5},
pages = {439--447},
title = {{Storage-class memory: The next storage system technology}},
url = {http://ieeexplore.ieee.org/document/5388608/},
volume = {52},
year = {2008}
}
@techreport{JoshFryman2016,
author = {{Josh Fryman}},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Josh Fryman - 2016 - HPC Memory Subsystem Beyond Myths {\&}amp Hype.pdf:pdf},
title = {{HPC Memory Subsystem Beyond Myths and Hype}},
url = {http://scc.acad.bg/ncsa/articles/library/Library2016{\_}Supercomputers-at-Work/ISC2016{\_}Frankfurt/HPC{\_}Memory{\_}Subsystem.pdf},
year = {2016}
}
@inproceedings{Burr2010,
author = {Burr, G W},
booktitle = {2010 IEEE International Interconnect Technology Conference},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Storage Class Memory Towards a disruptively low-cost solid-state non-volatile memory.pdf:pdf},
title = {{Storage Class Memory-Towards a disruptively low-cost solid-state nonvolatile memory}},
year = {2010}
}
@inproceedings{Molka2017a,
annote = {Framework pour valider les hardware counter

Est ce que le compteurs de stalls peut {\^{e}}tre utilis{\'{e}} pour connaitre les stall m{\'{e}}moire: il semble que oui},
author = {Molka, Daniel and Sch{\"{o}}ne, Robert and Hackenberg, Daniel and Nagel, Wolfgang E},
booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Molka et al. - 2017 - Detecting memory-boundedness with hardware performance counters.pdf:pdf;:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Detecting Memory-Boundedness with Hardware Performance Counters.pdf:pdf},
isbn = {1450344046},
pages = {27--38},
publisher = {ACM},
title = {{Detecting memory-boundedness with hardware performance counters}},
year = {2017}
}
@article{Kothe2016,
author = {Kothe, Douglas B},
title = {{Exascale Applications: Opportunities and Challenges}},
year = {2016}
}
@book{Deane2006,
annote = {1947 
GE (banded) 
Von Neumann {\&} Goldstine 
n5 
n7 
1950 
Optimal SOR 
Young 
n3 
n4 log n 
1971 
CG 
Reid 
n3 
n3.5 log n 
1984 
Full MG 
Brandt 
n3 
n3},
author = {Deane, Anil and Brenner, Gunther and Emerson, David R and McDonough, James and Tromeur-Dervout, Damien and Satofuka, N and Ecer, A and Periaux, Jacques},
isbn = {0080467938},
publisher = {Elsevier},
title = {{Parallel Computational Fluid Dynamics 2005: Theory and Applications}},
year = {2006}
}
@article{Molka,
author = {Molka, Daniel},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Molka - Unknown - Performance Analysis of Complex Shared Memory Systems.pdf:pdf;:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Molka - Unknown - Performance Analysis of Complex Shared Memory Systems(2).pdf:pdf;:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Performance Analysis of Complex Shared Memory Systems.pdf:pdf},
title = {{Performance Analysis of Complex Shared Memory Systems}}
}
@inproceedings{Shalf2010,
author = {Shalf, John and Dosanjh, Sudip and Morrison, John},
booktitle = {International Conference on High Performance Computing for Computational Science},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Exascale Computing Technology Challenges.pdf:pdf},
pages = {1--25},
publisher = {Springer},
title = {{Exascale computing technology challenges}},
year = {2010}
}
@inproceedings{Lam1991,
author = {Lam, Monica D and Rothberg, Edward E and Wolf, Michael E},
booktitle = {ACM SIGARCH Computer Architecture News},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/The Cache Performance and Optimization of Blocked Algorithms.pdf:pdf},
isbn = {0897913809},
number = {2},
pages = {63--74},
publisher = {ACM},
title = {{The cache performance and optimizations of blocked algorithms}},
volume = {19},
year = {1991}
}
@inproceedings{Yang2004,
abstract = {Power consumption is an important design issue of current embedded systems. It has been shown that the instruction cache accounts for a significant portion of the power dissipation of the whole chip. Several studies propose to add a cache (L0 cache) that is very small relative to the conventional L1 cache on chip for power optimization since a smaller cache has lower load capacitance. However, energy savings often come at the cost of performance degradation. In this paper, we propose a novel instruction cache architecture, the HotSpot cache, that achieves energy savings without sacrificing performance. The HotSpot cache identifies frequently accessed instructions dynamically and stores them in the L0 cache. Other instructions are placed only in the L1 cache. A steering mechanism is employed to direct an instruction to its allocated cache in the instruction fetch stage. The simulation results show that the HotSpot cache can achieve 52{\%} instruction cache energy reduction on the average for a set of multimedia applications without performance degradation.},
author = {Yang, Chia Lin and Lee, Chien Hao},
booktitle = {Proceedings of the International Symposium on Low Power Electronics and Design},
doi = {10.1109/LPE.2004.240812},
isbn = {1581139292},
issn = {15334678},
keywords = {Embedded Systems,Instruction Cache,Low Power Design},
number = {January},
pages = {114--119},
publisher = {ACM},
title = {{HotSpot Cache: Joint Temporal and Spatial Locality Exploitation for I-Cache Energy Reduction}},
volume = {2004-Janua},
year = {2004}
}
@article{Wulf1995,
author = {Wulf, Wm A and McKee, Sally A},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Hitting the Memory Wall Implications of the Obvious .pdf:pdf},
issn = {0163-5964},
journal = {ACM SIGARCH computer architecture news},
number = {1},
pages = {20--24},
publisher = {ACM},
title = {{Hitting the memory wall: implications of the obvious}},
volume = {23},
year = {1995}
}
@article{McCalpin1995,
author = {McCalpin, John D},
journal = {Link: www. cs. virginia. edu/stream/ref. html{\#} what},
title = {{STREAM benchmark}},
volume = {22},
year = {1995}
}
@inproceedings{Lobet2018,
author = {Lobet, Mathieu and Haefele, Matthieu and Soni, Vineet and Tamain, Patrick and Derouillat, Julien and Beck, Arnaud},
booktitle = {15{\`{e}}me congr{\'{e}}s de la Soci{\'{e}}t{\'{e}} Fran{\c{c}}aise de Physique division Plasma},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/High-Performance Computing at Exascale- challenges and benefits.pdf:pdf},
title = {{High-Performance Computing at Exascale: challenges and benefits}},
year = {2018}
}
@inproceedings{Murphy2005,
author = {Murphy, Richard and Rodrigues, Arun and Kogge, Peter and Underwood, Keith},
booktitle = {Proceedings of the 19th annual international conference on Supercomputing},
isbn = {1595931678},
pages = {332--340},
publisher = {ACM},
title = {{The implications of working set analysis on supercomputing memory hierarchy design}},
year = {2005}
}
@inproceedings{Bergman2011,
author = {Bergman, Keren and Hendry, Gilbert and Hargrove, Paul and Shalf, John and Jacob, Bruce and Hemmert, K Scott and Rodrigues, Arun and Resnick, David},
booktitle = {Proceedings of the 2011 ACM SIGPLAN Workshop on Memory Systems Performance and Correctness},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Let There Be Light!.pdf:pdf},
isbn = {1450307949},
pages = {43--48},
publisher = {ACM},
title = {{Let there be light!: the future of memory systems is photonics and 3D stacking}},
year = {2011}
}
@article{Daly2006,
author = {Daly, John T},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/A higher order estimate of the optimum checkpoint interval for restart dumps.pdf:pdf},
issn = {0167-739X},
journal = {Future generation computer systems},
number = {3},
pages = {303--312},
publisher = {Elsevier},
title = {{A higher order estimate of the optimum checkpoint interval for restart dumps}},
volume = {22},
year = {2006}
}
@misc{ChrisEvans2017,
abstract = {According to a recent article on The Register, Diablo Technologies appears to have closed down.  Diablo was or is a manufacturer of NVDIMM and DRAM extension technology, including Memory1.  As previously reported, many of the original founders and senior execs had already left the company over the past 12 months.  Is this just another company failure, or does it signal issues with the adoption of NVDIMM and SCM technology?},
author = {{Chris Evans}},
title = {{Has NVMe Killed off NVDIMM? | Architecting IT}},
url = {https://blog.architecting.it/has-nvme-killed-off-nvdimm/},
urldate = {2018-08-22},
year = {2017}
}
@incollection{Shalf2011,
author = {Shalf, John and Dosanjh, Sudip and Morrison, John},
doi = {10.1007/978-3-642-19328-6_1},
pages = {1--25},
publisher = {Springer, Berlin, Heidelberg},
title = {{Exascale Computing Technology Challenges}},
url = {http://link.springer.com/10.1007/978-3-642-19328-6{\_}1},
year = {2011}
}
@article{Wulf1995a,
author = {Wulf, Wm. A. and McKee, Sally A.},
doi = {10.1145/216585.216588},
issn = {01635964},
journal = {ACM SIGARCH Computer Architecture News},
month = {mar},
number = {1},
pages = {20--24},
publisher = {ACM},
title = {{Hitting the memory wall}},
url = {http://portal.acm.org/citation.cfm?doid=216585.216588},
volume = {23},
year = {1995}
}
@article{Oliker2005,
abstract = {The last decade has witnessed a rapid proliferation of superscalar cache-based microprocessors to build high-end capability and capacity computers primarily because of their generality, scalability, and cost effectiveness. However, the recent development of massively parallel vector systems is having a significant effect on the supercomputing landscape. In this paper, we compare the performance of the recently-released Cray XI vector system with that of the cacheless NEC SX-6 vector machine, and the superscalar cache-based IBM Power3 and Power4 architectures for scientific applications. Overall results demonstrate that the X1 is quite promising, but performance improvements are expected as the hardware, systems software, and numerical libraries mature. Code reengineering to effectively utilize the complex architecture may also lead to significant efficiency enhancements. {\textcopyright} Springer-Verlag Berlin Heidelberg 2005.},
author = {Canning, Andrew and Djomehri, M. Jahed and Carter, Jonathan and Biswas, Rupak and Oliker, Leonid and Skinner, David and Borrill, Julian and Shan, Hongzhang},
doi = {10.1007/11403937_5},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/A performance evaluation of the Cray X1 for scientific applications.pdf:pdf},
issn = {03029743},
journal = {High Performance {\ldots}},
pages = {51--65},
publisher = {Springer, Berlin, Heidelberg},
title = {{A Performance Evaluation of the Cray X1 for Scientific Applications}},
url = {http://link.springer.com/10.1007/11403937{\_}5 http://link.springer.com/chapter/10.1007/11403937{\_}5},
year = {2010}
}
@article{Dongarra2004a,
annote = {- plsu precis que lmbench et calibrator
- assure de bien mesurer un level de cache},
author = {Dongarra, Jack and Moore, Shirley and Mucci, Philip and Seymour, Keith},
doi = {http://springerlink.metapress.com/openurl.asp?genre=article&amp;issn=0302-9743&amp;volume=3038&amp;spage=432},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Accurate Cache and TLB Characterization Using Hardware Counters.pdf:pdf},
isbn = {3-540-22116-6},
issn = {03029743},
journal = {Science-ICCS 2004},
pages = {432--439},
publisher = {Springer, Berlin, Heidelberg},
title = {{Accurate cache and TLB characterization using hardware counters}},
url = {http://link.springer.com/10.1007/978-3-540-24688-6{\_}57 http://www.springerlink.com/index/Q9MHWHDE3HQU2K3B.pdf},
year = {2004}
}
@article{Saavedra1995,
abstract = {In previous research, we have developed and presented a model for$\backslash$nmeasuring machines and analyzing programs, and for accurately predicting$\backslash$nthe running time of any analyzed program on any measured machine. That$\backslash$nwork is extended here by: (1) developing a high level program to measure$\backslash$nthe design and performance of the cache and TLB units; (2) using those$\backslash$nmeasurements, along with published miss ratio data, to improve the$\backslash$naccuracy of our runtime predictions; (3) using our analysis tools and$\backslash$nmeasurements to study and compare the design of several machines, with$\backslash$nparticular reference to their cache and TLB performance. As part of this$\backslash$nwork, we describe the design and performance of the cache and TLB for$\backslash$nten machines. The work presented, in this paper extends a powerful$\backslash$ntechnique for the evaluation and analysis of both computer systems and$\backslash$ntheir workloads; this methodology is valuable both to computer users and$\backslash$ncomputer system designers},
author = {Saavedra, Rafael H. and Smith, Alan Jay},
doi = {10.1109/12.467697},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Measuring Cache and TLB Performance and Their Effect on Benchmark Run Times.pdf:pdf},
issn = {00189340},
journal = {IEEE Transactions on Computers},
keywords = {Performance evaluation,execution time prediction,memory hierarchy,processor caches,table lookaside buffers},
number = {10},
pages = {1223--1235},
title = {{Measuring Cache and TLB Performance and Their Effect on Benchmark Runtimes}},
url = {http://ieeexplore.ieee.org/document/467697/},
volume = {44},
year = {1995}
}
@inproceedings{Cabezas2014,
abstract = {Software, even if carefully optimized, rarely reaches the peak performance of a processor. Understanding which hardware resource is the bottleneck is difficult but important as it can help with both further optimizing the code or deciding which hardware component to upgrade for higher performance. If the bottleneck is the memory bandwidth, the roofline model provides a simple but instructive analysis and visualization. In this paper, we take the roofline analysis further by including additional performance-relevant hardware features such as latency, throughput, capacity information for a multilevel cache hierarchy and out-of-order execution buffers. Two key ideas underlie our analysis. First, we estimate performance based on a scheduling of the computation DAG on a high-level model of a microarchitecture and extract data including utilization of resources and overlaps from a cycle-by-cycle analysis of the schedule. Second, we show how to use this data to create only one plot with multiple rooflines that visualize performance bottlenecks. We validate our model against performance data obtained from a real system, and then apply our bottleneck analysis to a number of floating-point kernels to identify and interpret bottlenecks.},
author = {Cabezas, Victoria Caparros and Puschel, Markus},
booktitle = {IISWC 2014 - IEEE International Symposium on Workload Characterization},
doi = {10.1109/IISWC.2014.6983061},
isbn = {9781479964536},
issn = {1349-2543},
pages = {222--231},
title = {{Extending the roofline model: Bottleneck analysis with microarchitectural constraints}},
year = {2014}
}
@article{Muddukrishna2016,
abstract = {Average programmers struggle to solve performance problems in OpenMP programs with tasks and parallel for-loops. Existing performance analysis tools visualize OpenMP task performance from the runtime system's perspective where task execution is interleaved with other tasks in an unpredictable order. Problems with OpenMP parallel for-loops are similarly difficult to resolve since tools only visualize aggregate thread-level statistics such as load imbalance without zooming into a per-chunk granularity. The runtime system/threads oriented visualization provides poor support for understanding problems with task and chunk execution time, parallelism, and memory hierarchy utilization, forcing average programmers to rely on experts or use tedious trial-and-error tuning methods for performance. We present grain graphs, a new OpenMP performance analysis method that visualizes grains -- computation performed by a task or a parallel for-loop chunk instance -- and highlights problems such as low parallelism, work inflation and poor parallelization benefit at the grain level. We demonstrate that grain graphs can quickly reveal performance problems that are difficult to detect and characterize in fine detail using existing visualizations in standard OpenMP programs, simplifying OpenMP performance analysis. This enables average programmers to make portable optimizations for poor performing OpenMP programs, reducing pressure on experts and removing the need for tedious trial-and-error tuning.},
author = {Muddukrishna, Ananya and Jonsson, Peter a. and Podobas, Artur and Brorsson, Mats},
doi = {10.1145/2851141.2851156},
isbn = {9781450340922},
issn = {0362-1340},
journal = {PPoPP: Symposium on Principles and Practice of Parallel Programming},
title = {{Grain Graphs: OpenMP Performance Analysis Made Easy}},
year = {2016}
}
@inproceedings{Boehme2017,
abstract = {Many performance engineering tasks, from long- term performance monitoring to post-mortem analysis and on- line tuning, require efficient runtime methods for introspection and performance data collection. To understand interactions between components in increasingly modular HPC software, performance introspection hooks must be integrated into run- time systems, libraries, and application codes across the soft- ware stack. This requires an interoperable, cross-stack, general- purpose approach to performance data collection, which neither application-specific performance measurement nor traditional profile or trace analysis tools provide. With Caliper, we have developed a general abstraction layer to provide performance data collection as a service to applications, runtime systems, libraries, and tools. Individual software components connect to Caliper in independent data producer, data consumer, and measurement control roles, which allows them to share perfor- mance data across software stack boundaries. We demonstrate Caliper's performance analysis capbilities with two case studies of production scenarios},
author = {Boehme, David and Gamblin, Todd and Beckingsale, David and Bremer, Peer Timo and Gimenez, Alfredo and Legendre, Matthew and Pearce, Olga and Schulz, Martin},
booktitle = {International Conference for High Performance Computing, Networking, Storage and Analysis, SC},
doi = {10.1109/SC.2016.46},
isbn = {9781467388153},
issn = {21674337},
keywords = {Computer performance,High performance computing,Parallel processing,Performance analysis,Software performance,Software reusability,Software tools},
pages = {550--560},
title = {{Caliper: Performance Introspection for HPC Software Stacks}},
year = {2017}
}
@inproceedings{Liu2014,
abstract = {Almost all of today's microprocessors contain memory con- trollers and directly attach to memory. Modern multiproces- sor systems support non-uniform memory access (NUMA): it is faster for a microprocessor to access memory that is directly attached than it is to access memory attached to an- other processor.Without careful distribution of computation and data, a multithreaded program running on such a sys- tem may have high average memory access latency. To use multiprocessor systems efficiently, programmers need per- formance tools to guide the design of NUMA-aware codes. To address this need, we enhanced the HPCToolkit perfor- mance tools to support measurement and analysis of per- formance problems on multiprocessor systems with mul- tiple NUMA domains. With these extensions, HPCToolkit helps pinpoint, quantify, and analyze NUMA bottlenecks in executions of multithreaded programs. It computes derived metrics to assess the severity of bottlenecks, analyzes mem- ory accesses, and provides a wealth of information to guide NUMA optimization, including information about how to distribute data to reduce access latency and minimize con- tention. This paper describes the design and implementation of our extensions to HPCToolkit.We demonstrate their util- ity by describing case studies in which we use these capabil- ities to diagnose NUMA bottlenecks in four multithreaded applications.},
author = {Liu, Xu and Mellor-Crummey, John},
booktitle = {Proceedings of the 19th ACM SIGPLAN symposium on Principles and practice of parallel programming - PPoPP '14},
doi = {10.1145/2555243.2555271},
isbn = {9781450326568},
issn = {15232867},
pages = {259--272},
title = {{A tool to analyze the performance of multithreaded programs on NUMA architectures}},
url = {http://dl.acm.org/citation.cfm?doid=2555243.2555271},
year = {2014}
}
@article{Licht2018,
abstract = {Specialized hardware architectures promise a major step in performance and energy efficiency over the traditional load/store devices currently employed in large scale computing systems. The adoption of high-level synthesis (HLS) from languages such as C/C++ and OpenCL has greatly increased programmer productivity when designing for such platforms. While this has enabled a wider audience to target specialized hardware, the optimization principles known from software design are no longer sufficient to implement high-performance codes, due to fundamental differences between software and hardware architectures. In this work, we propose a set of optimizing transformations for HLS, targeting scalable and efficient architectures for high-performance computing (HPC) applications. We show how these can be used to efficiently exploit pipelining, on-chip distributed fast memory, and on-chip streaming dataflow, allowing for massively parallel architectures with little off-chip data movement. To quantify the effect of our transformations, we use them to optimize a set of high-throughput FPGA kernels, demonstrating that they are sufficient to scale up parallelism within the hardware constraints of the target device. With the transformations covered, we hope to establish a common framework for performance engineers, compiler developers, and hardware developers, to tap into the performance potential offered by specialized hardware architectures using HLS.},
archivePrefix = {arXiv},
arxivId = {1805.08288},
author = {Licht, Johannes de Fine and Meierhans, Simon and Hoefler, Torsten},
eprint = {1805.08288},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Licht, Meierhans, Hoefler - 2018 - Transformations of High-Level Synthesis Codes for High-Performance Computing.pdf:pdf},
month = {may},
title = {{Transformations of High-Level Synthesis Codes for High-Performance Computing}},
url = {http://arxiv.org/abs/1805.08288},
year = {2018}
}
@article{Unat2015,
abstract = {One of the emerging challenges to designing HPC systems is understanding and projecting the requirements of exascale applications. In order to determine the performance consequences of different hardware designs, analytic models are essential because they can provide fast feedback to the co-design centers and chip designers without costly simulations. However, current attempts to analytically model program performance typically rely on the user manually specifying a performance model. We introduce the ExaSAT framework that automates the extraction of parameterized performance models directly from source code using compiler analysis. The parameterized analytic model enables quantitative evaluation of a broad range of hardware design trade-offs and software optimizations on a variety of different performance metrics, with a primary focus on data movement as a metric. We demonstrate the ExaSAT framework's ability to perform deep code analysis of a proxy application from the Department of Energy Combustion Co-design Center to illustrate its value to the exascale co-design process. ExaSAT analysis provides insights into the hardware and software trade-offs and lays the groundwork for exploring a more targeted set of design points using cycle-accurate architectural simulators.},
author = {Unat, Didem and Chan, Cy and Zhang, Weiqun and Williams, Samuel and Bachan, John and Bell, John and Shalf, John},
doi = {10.1177/1094342014568690},
issn = {17412846},
journal = {International Journal of High Performance Computing Applications},
keywords = {Performance modeling,abstract machine model,cache modeling,combustion codes,compiler analysis,design trade-offs,exascale co-design,exascale systems,performance analysis,stencil applications},
number = {2},
pages = {209--232},
title = {{ExaSAT: An exascale co-design tool for performance modeling}},
volume = {29},
year = {2015}
}
@article{Hofmann2015,
abstract = {This report serves two purposes: To introduce and validate the Execution-Cache-Memory (ECM) performance model and to provide a thorough analysis of current Intel processor architectures with a special emphasis on Intel Xeon Haswell-EP. The ECM model is a simple analytical performance model which focuses on basic architectural resources. The architectural analysis and model predictions are showcased and validated using a set of elementary microbenchmarks.},
archivePrefix = {arXiv},
arxivId = {1509.03118},
author = {Hofmann, Johannes and Eitzinger, Jan and Fey, Dietmar},
eprint = {1509.03118},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Hofmann, Eitzinger, Fey - 2015 - Execution-Cache-Memory Performance Model Introduction and Validation.pdf:pdf},
month = {sep},
title = {{Execution-Cache-Memory Performance Model: Introduction and Validation}},
url = {http://arxiv.org/abs/1509.03118},
year = {2015}
}
@article{Stengel2014,
abstract = {Stencil algorithms on regular lattices appear in many fields of computational science, and much effort has been put into optimized implementations. Such activities are usually not guided by performance models that provide estimates of expected speedup. Understanding the performance properties and bottlenecks by performance modeling enables a clear view on promising optimization opportunities. In this work we refine the recently developed Execution-Cache-Memory (ECM) model and use it to quantify the performance bottlenecks of stencil algorithms on a contemporary Intel processor. This includes applying the model to arrive at single-core performance and scalability predictions for typical corner case stencil loop kernels. Guided by the ECM model we accurately quantify the significance of "layer conditions," which are required to estimate the data traffic through the memory hierarchy, and study the impact of typical optimization approaches such as spatial blocking, strength reduction, and temporal blocking for their expected benefits. We also compare the ECM model to the widely known Roofline model.},
archivePrefix = {arXiv},
arxivId = {1410.5010},
author = {Stengel, Holger and Treibig, Jan and Hager, Georg and Wellein, Gerhard},
doi = {10.1145/2751205.2751240},
eprint = {1410.5010},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Stengel et al. - 2014 - Quantifying performance bottlenecks of stencil computations using the Execution-Cache-Memory model.pdf:pdf},
isbn = {9781450335591},
month = {oct},
title = {{Quantifying performance bottlenecks of stencil computations using the Execution-Cache-Memory model}},
url = {http://arxiv.org/abs/1410.5010 http://dx.doi.org/10.1145/2751205.2751240 http://arxiv.org/abs/1410.5010{\%}0Ahttp://dx.doi.org/10.1145/2751205.2751240},
year = {2014}
}
@article{Guntz2017,
author = {Guntz, Thomas and Balzarini, Raffaella and Vaufreydaz, Dominique and Crowley, James L},
journal = {arXiv preprint arXiv:1710.04486},
title = {{Multimodal observation and interpretation of subjects engaged in problem solving}},
year = {2017}
}
@inproceedings{Walcott-Justice2012,
author = {Walcott-Justice, Kristen and Mars, Jason and Soffa, Mary Lou},
booktitle = {Proceedings of the 2012 International Symposium on Software Testing and Analysis},
isbn = {1450314546},
pages = {12--22},
publisher = {ACM},
title = {{THeME: a system for testing by hardware monitoring events}},
year = {2012}
}
@article{Kanev2015,
address = {New York, New York, USA},
author = {Kanev, Svilen and Darago, Juan Pablo and Hazelwood, Kim and Wei, Gu-Yeon and Brooks, David},
doi = {10.1145/2749469.2750392},
isbn = {9781450334020},
journal = {Proceedings of the 42nd Annual International Symposium on Computer Architecture - ISCA '15},
number = {Section 3},
pages = {158--169},
publisher = {ACM Press},
title = {{Profiling a warehouse-scale computer}},
url = {http://dl.acm.org/citation.cfm?doid=2749469.2750392},
year = {2015}
}
@inproceedings{Moseley2007a,
abstract = {In profiling, a tradeoff exists between information and overhead. For example, hardware-sampling profilers incur negligible overhead, but the information they collect is consequently very coarse. Other profilers use instrumentation tools to gather temporal traces such as path profiles and hot memory streams, but they have high overhead. Runtime and feedback-directed compilation systems need detailed information to aggressively optimize, but the cost of gathering profiles can outweigh the benefits. Shadow profiling is a novel method for sampling long traces of instrumented code in parallel with normal execution, taking advantage of the trend of increasing numbers of cores. Each instrumented sample can be many millions of instructions in length. The primary goal is to incur negligible overhead, yet attain profile information that is nearly as accurate as a perfect profile. The profiler requires no modifications to the operating system or hardware, and is tunable to allow for greater coverage or lower overhead. We evaluate the performance and accuracy of this new profiling technique for two common types of instrumentation-based profiles: interprocedural path profiling and value profiling. Overall, profiles collected using the shadow profiling framework are 94{\%} accurate versus perfect value profiles, while incurring less than 1{\%} overhead. Consequently, this technique increases the viability of dynamic and continuous optimization systems by hiding the high overhead of instrumentation and enabling the online collection of many types of profiles that were previously too costly},
author = {Moseley, Tipp and Shye, Alex and Reddi, Vijay Janapa and Grunwald, Dirk and Peri, Ramesh},
booktitle = {International Symposium on Code Generation and Optimization, CGO 2007},
doi = {10.1109/CGO.2007.35},
isbn = {0769527647},
month = {mar},
pages = {198--208},
publisher = {IEEE},
title = {{Shadow profiling: Hiding instrumentation costs with parallelism}},
url = {http://ieeexplore.ieee.org/document/4145115/},
year = {2007}
}
@article{Stengel2014a,
abstract = {Stencil algorithms on regular lattices appear in many fields of computational science, and much effort has been put into optimized implementations. Such activities are usually not guided by performance models that provide estimates of expected speedup. Understanding the performance properties and bottlenecks by performance modeling enables a clear view on promising optimization opportunities. In this work we refine the recently developed Execution-Cache-Memory (ECM) model and use it to quantify the performance bottlenecks of stencil algorithms on a contemporary Intel processor. This includes applying the model to arrive at single-core performance and scalability predictions for typical corner case stencil loop kernels. Guided by the ECM model we accurately quantify the significance of "layer conditions," which are required to estimate the data traffic through the memory hierarchy, and study the impact of typical optimization approaches such as spatial blocking, strength reduction, and temporal blocking for their expected benefits. We also compare the ECM model to the widely known Roofline model.},
address = {New York, New York, USA},
archivePrefix = {arXiv},
arxivId = {1410.5010},
author = {Stengel, Holger and Treibig, Jan and Hager, Georg and Wellein, Gerhard},
doi = {10.1145/2751205.2751240},
eprint = {1410.5010},
isbn = {9781450335591},
journal = {Proceedings of the 29th ACM on International Conference on Supercomputing - ICS '15},
pages = {207--216},
publisher = {ACM Press},
title = {{Quantifying performance bottlenecks of stencil computations using the Execution-Cache-Memory model}},
url = {http://dl.acm.org/citation.cfm?doid=2751205.2751240 http://arxiv.org/abs/1410.5010{\%}0Ahttp://dx.doi.org/10.1145/2751205.2751240},
year = {2014}
}
@inproceedings{Dimakopoulou2017,
abstract = {—Processor hardware performance counters have re-cently improved in quality and features, while performance mon-itoring support in Linux has been significantly revamped with the development of the perf events subsystem, which contributed in making performance analysis an increasingly common practice among developers. However, no performance analysis is possible without an efficient monitoring interface and reliable hardware counter data. In this paper, we first address a reliability issue in the Performance Monitoring Unit of recent Intel processors with Hyper-Threading enabled. A published erratum causes cross hyper-thread hardware counter corruption and may produce unreliable results. We propose a cache-coherence style protocol which we implement in the Linux kernel to address the issue by introducing cross hyper-thread dynamic event scheduling. Second, we improve event scheduling efficiency by introducing an algorithm which optimally schedules events onto hardware counters consistently. The proposed optimizations do not require any user level changes. They leverage the internal design of the perf events subsystem and have broader applicability in processors. The improvements have been contributed to the upstream Linux kernel 4.1.},
author = {Dimakopoulou, Maria and Eranian, Stephane and Koziris, Nectarios and Bambos, Nicholas},
booktitle = {International Conference for High Performance Computing, Networking, Storage and Analysis, SC},
doi = {10.1109/SC.2016.33},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Reliable and Efficient Performance Monitoring in Linux.pdf:pdf},
isbn = {9781467388153},
issn = {21674337},
month = {nov},
pages = {396--408},
publisher = {IEEE},
title = {{Reliable and Efficient Performance Monitoring in Linux}},
url = {http://ieeexplore.ieee.org/document/7877112/},
year = {2017}
}
@inproceedings{Neill2017,
abstract = {Analyzing the behavior of OpenMP programs and their interaction with the hardware is essential for locating performance bottlenecks and identifying performance optimization opportunities. However, current architectures only provide a small number of dedicated registers to quantify hardware events, which strongly limits the scope of performance analyses. Hardware event multiplexing can help cover more events, but incurs a significant loss of accuracy and introduces overheads that change the behavior of program execution significantly. In this paper, we present an implementation of our technique for building a unique, coherent profile that contains all available hardware events from multiple executions of the same OpenMP program, each monitoring only a subset of the available hardware events. Reconciliation of the execution profiles relies on a new labeling scheme for OpenMP that uniquely identifies each dynamic unit of work across executions under dynamic scheduling across processing units. We show that our approach yields significantly better accuracy and lower monitoring overhead per execution than hardware event multiplexing.},
author = {Neill, Richard and Drebes, Andi and Pop, Antoniu},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-65578-9_18},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Accurate and Complete Hardware Profiling for OpenMP.pdf:pdf},
isbn = {9783319655772},
issn = {16113349},
keywords = {Hardware events,OpenMP profiling,Performance analysis,Performance monitoring counters},
month = {sep},
pages = {266--280},
publisher = {Springer, Cham},
title = {{Accurate and complete hardware profiling for OpenMP: multiplexing hardware events across executions}},
url = {http://link.springer.com/10.1007/978-3-319-65578-9{\_}18},
volume = {10468 LNCS},
year = {2017}
}
@inproceedings{Hirzel2001,
author = {Hirzel, Martin and Chilimbi, Trishul},
booktitle = {4th ACM Workshop on Feedback-Directed and Dynamic Optimization (FDDO-4)},
pages = {117--126},
title = {{Bursty tracing: A framework for low-overhead temporal profiling}},
year = {2001}
}
@article{Arnold2001,
author = {Arnold, Matthew and Ryder, Barbara G},
issn = {1581134142},
journal = {Acm Sigplan Notices},
number = {5},
pages = {168--179},
publisher = {ACM},
title = {{A framework for reducing the cost of instrumented code}},
volume = {36},
year = {2001}
}
@inproceedings{Moseley2007,
abstract = {In profiling, a tradeoff exists between information and overhead. For example, hardware-sampling profilers incur negligible overhead, but the information they collect is consequently very coarse. Other profilers use instrumentation tools to gather temporal traces such as path profiles and hot memory streams, but they have high overhead. Runtime and feedback-directed compilation systems need detailed information to aggressively optimize, but the cost of gathering profiles can outweigh the benefits. Shadow profiling is a novel method for sampling long traces of instrumented code in parallel with normal execution, taking advantage of the trend of increasing numbers of cores. Each instrumented sample can be many millions of instructions in length. The primary goal is to incur negligible overhead, yet attain profile information that is nearly as accurate as a perfect profile. The profiler requires no modifications to the operating system or hardware, and is tunable to allow for greater coverage or lower overhead. We evaluate the performance and accuracy of this new profiling technique for two common types of instrumentation-based profiles: interprocedural path profiling and value profiling. Overall, profiles collected using the shadow profiling framework are 94{\%} accurate versus perfect value profiles, while incurring less than 1{\%} overhead. Consequently, this technique increases the viability of dynamic and continuous optimization systems by hiding the high overhead of instrumentation and enabling the online collection of many types of profiles that were previously too costly},
author = {Moseley, Tipp and Shye, Alex and Reddi, Vijay Janapa and Grunwald, Dirk and Peri, Ramesh},
booktitle = {International Symposium on Code Generation and Optimization, CGO 2007},
doi = {10.1109/CGO.2007.35},
isbn = {0769527647},
month = {mar},
pages = {198--208},
publisher = {IEEE},
title = {{Shadow profiling: Hiding instrumentation costs with parallelism}},
url = {http://ieeexplore.ieee.org/document/4145115/},
year = {2007}
}
@inproceedings{Williams2004,
author = {Williams, Chadd C and Hollingsworth, Jeffrey K},
booktitle = {Second International Workshop on Remote Analysis and Measurement of Software Systems (RAMSS)},
title = {{Interactive binary instrumentation}},
year = {2004}
}
@inproceedings{Luk2005,
author = {Luk, Chi-Keung and Cohn, Robert and Muth, Robert and Patil, Harish and Klauser, Artur and Lowney, Geoff and Wallace, Steven and Reddi, Vijay Janapa and Hazelwood, Kim},
booktitle = {Acm sigplan notices},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Pin- Building Customized Program Analysis Tools with Dynamic Instrumentation.pdf:pdf},
isbn = {1595930566},
number = {6},
pages = {190--200},
publisher = {ACM},
title = {{Pin: building customized program analysis tools with dynamic instrumentation}},
volume = {40},
year = {2005}
}
@inproceedings{Laurenzano2012,
abstract = {In order to achieve a high level of performance, data intensive applications such as the real-time processing of surveillance feeds from unmanned aerial vehicles will require the strategic application of multi/many-core processors and coprocessors using a hybrid of inter-process message passing (e.g. MPI and SHMEM) and intra-process threading (e.g. pthreads and OpenMP). To facilitate program design decisions, memory traces gathered through binary instrumentation can be used to understand the low-level interactions between a data intensive code and the memory subsystem of a multi-core processor or many-core co-processor. Toward this end, this paper introduces the addition of threading support for PMaCs Efficient Binary Instrumentation Toolkit for Linux/x86 (PEBIL) and compares PEBILs threading model to the threading models of two other popular Linux/x86 binary instrumentation platforms - Pin and Dyninst - on both theoretical and empirical grounds. The empirical comparisons are based on experiments which collect memory address traces for the OpenMP-threaded implementations of the NASA Advanced Supercomputing Parallel Benchmarks (NPBs). This work shows that the overhead of collecting full memory address traces for multithreaded programs is higher in PEBIL (7.7x) than in Pin (4.7x), both of which are significantly lower than Dyninst (897x). This work also shows that PEBIL, uniquely, is able to take advantage of interval-based sampling of a memory address trace by rapidly disabling and re-enabling instrumentation at the transitions into and out of sampling periods in order to achieve significant decreases in the overhead of memory address trace collection. For collecting the memory address streams of each of the NPBs at a 10{\%} sampling rate, PEBIL incurs an average slowdown of 2.9x compared to 4.4x with Pin and 897x with Dyninst.},
author = {Laurenzano, Michael A. and Peraza, Joshua and Carrington, Laura and Tiwari, Ananta and Ward, William A. and Campbell, Roy},
booktitle = {Proceedings - 2012 SC Companion: High Performance Computing, Networking Storage and Analysis, SCC 2012},
doi = {10.1109/SC.Companion.2012.101},
isbn = {9780769549569},
month = {nov},
pages = {741--745},
publisher = {IEEE},
title = {{A static binary instrumentation threading model for fast memory trace collection}},
url = {http://ieeexplore.ieee.org/document/6495883/},
year = {2012}
}
@article{Patil2004,
abstract = {Detailed modeling of the performance of commercial applications is difficult. The applications can take a very long time to run on real hardware and it is impractical to simulate them to completion on performance models. Furthermore, these applications have complex execution environments that cannot easily be reproduced on a simulator, making porting the applications to simulators difficult. We attack these problems using the well-known SimPoint methodology to find representative portions of an application to simulate, and a dynamic instrumentation framework called Pin to avoid porting altogether. Our system uses dynamic instrumentation instead of simulation to find representative portions - called Pin-Points - for simulation. We have developed a toolkit that automatically detects PinPoints, validates whether they are representative using hardware performance counters, and generates traces for large Itanium{\&}174; programs. We compared SimPoint-based selection to random selection of simulation points. We found for 95{\%} of the SPEC2000 programs we tested, the PinPoints prediction was within 8{\%} of the actual whole-program CPI, as opposed to 18{\%} for random selection. We measure the end-to-end error, comparing real hardware to a performance model, and have a simple and efficient methodology to determine the step that introduced the error. Finally, we evaluate the system in the context of multiple configurations of real hardware, commercial applications, and industrial-strength performance models to understand the behavior of a complete and practical workload collection system. We have successfully used our system with many commercial Itanium{\&}174; programs, some running for trillions of instructions, and have used the resulting traces for predicting performance of those applications on future Itanium processors.},
author = {Patil, H. and Cohn, R. and Charney, M. and Kapoor, R. and Sun, A. and Karunanidhi, A.},
doi = {10.1109/MICRO.2004.28},
isbn = {0-7695-2126-6},
issn = {1072-4451},
journal = {37th International Symposium on Microarchitecture MICRO3704},
pages = {81--92},
publisher = {IEEE},
title = {{Pinpointing Representative Portions of Large Intelamp;{\#}174; Itaniumamp;{\#}174; Programs with Dynamic Instrumentation}},
url = {http://ieeexplore.ieee.org/document/1550984/ http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1550984},
year = {2004}
}
@inproceedings{Markomanolis2014,
abstract = {{\textcopyright} 2014 IEEE.The performance analysis of a parallel application can be a difficult task. Specially in the case that this application is an operational atmospheric-chemistry model there can be multiple performance bottlenecks caused from different fields. Although the exascale era is coming, the applications are not ready to take advantage of all the new technologies and programming models. It is needed to improve our model in order to simulate higher resolutions and scale more efficient. In this article we describe the approaches that we follow for the performance analysis of an atmospheric-chemistry global model called NMMB/BSC chemical transport model and the identification of various bottlenecks by using the Paraver tool. We present the differences between some model configurations depending on the usage of extra modules and we study eight different topics that limit the scalability of the model. These topics include categories that there is no need for code modification such as mapping, processor affinity and more in depth analysis with hardware counters and load imbalance issues. The final results show the directions that we should follow in order to improve our model.},
author = {Markomanolis, G. S. and Jorba, O. and Baldasano, J. M.},
booktitle = {Proceedings of the 2014 International Conference on High Performance Computing and Simulation, HPCS 2014},
doi = {10.1109/HPCSim.2014.6903763},
isbn = {9781479953127},
keywords = {Atmospheric Models,Performance Analysis,Scaling},
month = {jul},
pages = {738--745},
publisher = {IEEE},
title = {{Performance analysis of an online atmospheric-chemistry global model with Paraver: Identification of scaling limitations}},
url = {http://ieeexplore.ieee.org/document/6903763/},
year = {2014}
}
@article{Intel2017,
author = {Intel},
title = {{Intel 64 and IA-32 architectures software developer's manual combined volumes: 1, 2A, 2B, 2C, 2D, 3A, 3B, 3C, 3D, and 4}},
year = {2017}
}
@article{Shende2006,
abstract = {The ability of performance technology to keep pace with the growing complexity of parallel and distributed systems depends on robust performance frameworks that can at once provide system-specific performance capabilities and support high-level performance problem solving. Flexibility and portability in empirical methods and processes are influenced primarily by the strategies available for instrmentation and measurement, and how effectively they are integrated and composed. This paper presents the TAU (Tuning and Analysis Utilities) parallel performance sytem and describe how it addresses diverse requirements for performance observation and analysis.},
author = {Shende, Sameer S. and Malony, Allen D.},
doi = {10.1177/1094342006064482},
isbn = {1078-3482},
issn = {10943420},
journal = {International Journal of High Performance Computing Applications},
keywords = {Analysis,Instrumentation,Measurement,Performance evaluation,TAU},
month = {may},
number = {2},
pages = {287--311},
publisher = {Sage Publications, Inc.},
title = {{The TAU parallel performance system}},
url = {http://journals.sagepub.com/doi/10.1177/1094342006064482},
volume = {20},
year = {2006}
}
@inproceedings{Molka2009,
author = {Molka, Daniel and Hackenberg, Daniel and Schone, Robert and Muller, Matthias S},
booktitle = {Parallel Architectures and Compilation Techniques, 2009. PACT'09. 18th International Conference on},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Memory Performance and Cache Coherency Effects on an Intel Nehalem Multiprocessor System.pdf:pdf},
isbn = {0769537715},
pages = {261--270},
publisher = {IEEE},
title = {{Memory performance and cache coherency effects on an intel nehalem multiprocessor system}},
year = {2009}
}
@inproceedings{Ould-Ahmed-Vall2007,
author = {Ould-Ahmed-Vall, ElMoustapha and Woodlee, James and Yount, Charles and Doshi, Kshitij A and Abraham, Seth},
booktitle = {Performance Analysis of Systems {\&} Software, 2007. ISPASS 2007. IEEE International Symposium on},
isbn = {1424410819},
pages = {116--125},
publisher = {IEEE},
title = {{Using model trees for computer architecture performance analysis of software applications}},
year = {2007}
}
@inproceedings{Cavazos2006,
author = {Cavazos, John and Dubach, Christophe and Agakov, Felix and Bonilla, Edwin and O'Boyle, Michael F P and Fursin, Grigori and Temam, Olivier},
booktitle = {Proceedings of the 2006 international conference on Compilers, architecture and synthesis for embedded systems},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Automatic performance model construction for the fast software exploration of new hardware designs.pdf:pdf},
isbn = {1595935436},
pages = {24--34},
publisher = {ACM},
title = {{Automatic performance model construction for the fast software exploration of new hardware designs}},
year = {2006}
}
@inproceedings{Moseley2005,
author = {Moseley, Tipp and Kihm, Joshua L and Connors, Daniel A and Grunwald, Dirk},
booktitle = {Computer Design: VLSI in Computers and Processors, 2005. ICCD 2005. Proceedings. 2005 IEEE International Conference on},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Methods for Modeling Resource Contention on Simultaneous Multithreading Processors.pdf:pdf},
isbn = {0769524516},
pages = {373--380},
publisher = {IEEE},
title = {{Methods for modeling resource contention on simultaneous multithreading processors}},
year = {2005}
}
@misc{RobertD.2011,
abstract = {The Intel{\textregistered} Platform Modeling tool is an Intel research project which simultaneously collects all available hardware metrics on the platform – host processor, chipset, video, 3D, even NVIDIA. Some of the hardware metrics are not yet publicly available but will be made available as soon as authorization is received. Software metrics such as Perfmon and metrics from application code can be added to the mix of simultaneous hardware metrics. By collecting metrics at the same time, relationships between metrics can be uncovered using any available method including Machine Learning (ML). With an increasing number of metrics available, a list of the best predictors of performance from ML can orient the developer to the right bottleneck in a hardware or software design. The Intel{\textregistered} Platform Modeling tool is available for Windows XP and Vista 32-bit platforms.},
author = {{Robert D.}},
title = {{Intel PMT}},
url = {https://software.intel.com/en-us/articles/intel-platform-modeling-with-machine-learning/},
urldate = {2018-05-29},
year = {2011}
}
@inproceedings{Moseley2011a,
author = {Moseley, Tipp and Vachharajani, Neil and Jalby, William},
booktitle = {IFIP International Conference on Network and Parallel Computing},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Hardware Performance Monitoring for the Rest of Us A Position and Survey.pdf:pdf},
pages = {293--312},
publisher = {Springer},
title = {{Hardware performance monitoring for the rest of us: a position and survey}},
year = {2011}
}
@article{Dean1997,
abstract = {Profile data is valuable for identifying performance bottlenecks and guiding optimizations. Periodic sampling of a processor's performance monitoring hardware is an effective, unobtrusive way to obtain detailed profiles. Unfortunately, existing hardware simply counts events, such as cache misses and branch mispredictions, and cannot accurately attribute these events to instructions, especially on out-of-order machines. We propose an alternative approach, called ProfileMe, that samples instructions. As a sampled instruction moves through the processor pipeline, a detailed record of all interesting events and pipeline stage latencies is collected. ProfileMe also supports paired sampling, which captures information about the interactions between concurrent instructions, revealing information about useful concurrency and the utilization of various pipeline stages while an instruction is in flight. We describe an inexpensive hardware implementation of ProfileMe, outline a variety of software techniques to extract useful profile information from the hardware, and explain several ways in which this information can provide valuable feedback for programmers and optimizers},
author = {Dean, J. and Hicks, J.E. and Waldspurger, C.A.},
doi = {10.1109/MICRO.1997.645821},
isbn = {0-8186-7977-8},
issn = {1072-4451},
journal = {Proceedings of the 30th {\ldots}},
pages = {292--302},
publisher = {IEEE Comput. Soc},
title = {{ProfileMe: Hardware support for instruction-level profiling on out-of-order processors}},
url = {http://ieeexplore.ieee.org/document/645821/ http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=645821{\%}5Cnhttp://dl.acm.org/citation.cfm?id=266828},
year = {1997}
}
@inproceedings{Sprunt2005,
author = {Sprunt, B},
booktitle = {Workshop on Hardware Performance Monitors in conjunction with HPCA-11},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Performance Monitoring Hardware Will Always Be A Low-Priority, Second-Class Feature Until.pdf:pdf},
title = {{Performance monitoring hardware will always be a low priority, second class feature in processor design until}},
year = {2005}
}
@book{Straatsma2017,
author = {Straatsma, Tjerk P. and Antypas, K B and Williams, T J},
isbn = {9781351999236},
pages = {567},
title = {{Exascale Scientific Applications: Scalability and Performance Portability}},
url = {https://books.google.fr/books?id=smlQDwAAQBAJ{\&}hl=fr{\&}lr= https://books.google.com/books?id=rGQ-DwAAQBAJ},
year = {2017}
}
@phdthesis{Palomares2015,
author = {Palomares, Vincent},
booktitle = {http://www.theses.fr},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Palomares - 2015 - Combiner approches statique et dynamique pour mod{\'{e}}liser la performance de boucles HPC.pdf:pdf},
month = {sep},
publisher = {Versailles-St Quentin en Yvelines},
title = {{Combiner approches statique et dynamique pour mod{\'{e}}liser la performance de boucles HPC}},
url = {http://theses.fr/2015VERS040V},
year = {2015}
}
@phdthesis{Rubial2012,
author = {Rubial, Andr{\'{e}}s Salim Charif},
booktitle = {http://www.theses.fr},
keywords = {MAQAO},
mendeley-tags = {MAQAO},
month = {jan},
publisher = {Versailles-St Quentin en Yvelines},
title = {{Analyse et optimisation de performances sur architectures multicoeurs}},
url = {http://theses.fr/2012VERS0026},
year = {2012}
}
@phdthesis{Noudohouenou2013,
abstract = {Comprendre l'interaction existante entre applications, compilateurs, et architecture est fondamentale pour fabriquer de meilleurs produits: applications, compilateurs, et processeurs. L'un des moyens traditionnels permettant d'aborder ce probl{\`{e}}me reste l'exp{\'{e}}rimentation. Cependant cette m{\'{e}}thode utilisant des applications compl{\`{e}}tes pr{\'{e}}sente plusieurs inconv{\'{e}}nients majeurs: complexit{\'{e}} des applications pour faire, par exemple, de la simulation, ou une analyse d{\'{e}}taill{\'{e}}e de la performance de l'application; impossibilit{\'{e}} d'avoir une vue pr{\'{e}}cise de l'interaction; et enfin difficult{\'{e}} {\`{a}} pr{\'{e}}dire la performance d'une autre application n'existant pas dans la collection initiale. Cette th{\`{e}}se effectue une caract{\'{e}}risation syst{\'{e}}matique des applications en quatre {\'{e}}tapes: extraction de code, analyse de performance, caract{\'{e}}risation m{\'{e}}moire, et pr{\'{e}}diction de performance au niveau mat{\'{e}}riel. Afin de pr{\'{e}}dire la performance d'un code {\`{a}} une autre fr{\'{e}}quence, cette th{\`{e}}se combine analyses statique et dynamique, ainsi que de la caract{\'{e}}risation m{\'{e}}moire pour proposer l'outil Capacity, destin{\'{e}} {\`{a}} la pr{\'{e}}diction de performance au niveau mat{\'{e}}riel et {\`{a}} la d{\'{e}}couverte de connaissance. L'outil propos{\'{e}} est {\`{a}} la fois plus pr{\'{e}}cis et plus rapide que les simulations, plus informatif que de simples exp{\'{e}}riences ou micro-exp{\'{e}}riences. Il est {\'{e}}galement utile et instructif pour diagnostiquer les probl{\`{e}}mes de performance de code. Aujourd'hui, cet outil est utilis{\'{e}} par la technologie Cape-sim d'Intel pour simuler la performance d'un code {\`{a}} une autre fr{\'{e}}quence.},
author = {Noudohouenou, Jos{\'{e}}},
booktitle = {http://www.theses.fr},
month = {jan},
publisher = {Versailles-St Quentin en Yvelines},
title = {{Pr{\'{e}}diction de performance utilisant une caract{\'{e}}risation des applications orient{\'{e}}e codelet}},
url = {http://theses.fr/2013VERS0030},
year = {2013}
}
@misc{LinuxManual2012,
author = {{Linux Manual}},
title = {{cpuid (4) - Linux manual page}},
url = {http://man7.org/linux/man-pages/man4/cpuid.4.html},
urldate = {2018-05-25},
year = {2012}
}
@inproceedings{Trahay2011,
abstract = {Modern supercomputers with multi-core nodes en- hanced by accelerators, as well as hybrid programming models, introduce more complexity in modern applications. Exploiting efficiently all the resources requires a complex analysis of the performance of applications in order to detect time-consuming or idle sections. This paper presents EZTRACE, a generic trace generation framework that aims at providing a simple way to analyze applications. EZTRACE is based on plugins that allow it to trace different programming models such as MPI, pthread or OpenMP as well as user-defined libraries or application. This framework uses two steps: one to collect the basic information during execution and one post-mortem analysis. This permits tracing the execution of applications with low overhead while allowing to refine the analysis after the execution of the program. We also present a simple script language for EZTRACE that gives the user the capability to easily define the functions to instrument without modifying the source code of the application. The evaluation of EZTRACE shows that the framework offers a convenient way to analyze applications without impacting the execution.},
annote = {http://eztrace.gforge.inria.fr/


EZTrace is a tool that aims at generating automatically execution trace from HPC (High Performance Computing) programs.
EZtrace support major HPC programming models: it can record calls to MPI functions, OpenMP directives, pthread synchronisation primitives.
It generates execution trace files that can be interpreted by visualization tools such as ViTE or vampire.


￼





Installtion:
./bootstrap
./configure  --prefix=/nfs/pourroy/TOOLS/EZtrace/build  --with-mpi=/opt/openmpi/gnu/bin/ --with-papi=/nfs/pourroy/TOOLS/papi/papi{\_}5.4.3
make
make install 




Utilisation


First, you should select the functions you want to instrument in your application.
export EZTRACE{\_}TRACE="pthread mpi"},
author = {Trahay, Fran{\c{c}}ois and Rue, Fran{\c{c}}ois and Faverge, Mathieu and Ishikawa, Yutaka and Namyst, Raymond and Dongarra, Jack},
booktitle = {International Symposium on Cluster, Cloud and Grid Computing (CCGrid)},
doi = {10.1109/CCGrid.2011.83},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/EZTrace - A Generic Framework for Performance Analysis.pdf:pdf},
isbn = {978-1-4577-0129-0},
month = {may},
pages = {618--619},
publisher = {IEEE},
title = {{EZTrace: a generic framework for performance analysis}},
url = {http://ieeexplore.ieee.org/document/5948661/},
year = {2011}
}
@misc{Gregg2015,
author = {Gregg, Brendan},
title = {{CPU Flame Graphs}},
url = {http://www.brendangregg.com/FlameGraphs/cpuflamegraphs.html},
urldate = {2018-05-25},
year = {2015}
}
@misc{FAgner,
author = {{F Agner}},
title = {{Software optimization resources}},
url = {http://www.agner.org/optimize/},
urldate = {2018-05-25}
}
@phdthesis{Selva2015,
author = {Selva, Manuel},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Performance monitoring of throughput constrained dataflow programs executed on shared-memory multi-core architectures.pdf:pdf},
publisher = {INSA de Lyon},
title = {{Performance monitoring of throughput constrained dataflow programs executed on shared-memory multi-core architectures}},
year = {2015}
}
@article{Bitzes2014,
author = {Bitzes, Georgios and Nowak, Andrzej},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/The overhead of profiling using PMU hardware counters.pdf:pdf},
title = {{The overhead of profiling using PMU hardware counters}},
year = {2014}
}
@incollection{Barthou2010,
author = {Barthou, Denis and Rubial, Andres Charif and Jalby, William and Koliai, Souad and Valensi, Cedric},
booktitle = {Tools for High Performance Computing 2009},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Performance Tuning of x86 OpenMP Codes with MAQAO.pdf:pdf},
pages = {95--113},
publisher = {Springer},
title = {{Performance tuning of x86 openmp codes with maqao}},
year = {2010}
}
@book{Barthou2009,
author = {Barthou, Denis and Charif-Rubial, Andr{\'{e}}s S and Jalby, William and Koliai, Souad and Valensi, C{\'{e}}dric},
doi = {10.1007/978-3-642-11261-4_7},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Performance Tuning of x86 OpenMP Codes with MAQAO.pdf:pdf},
month = {jan},
pages = {95--113},
title = {{Performance tuning of x86 OpenMP codes with MAQAO}},
year = {2009}
}
@book{Bendifallah2014,
author = {Bendifallah, Zakaria and Jalby, William and Noudohouenou, Jose and Oseret, Emmanuel and Palomares, Vincent and Charif-Rubial, Andr{\'{e}}s S},
doi = {10.1007/978-3-319-08144-1_9},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/PAMDA- Performance Assessment Using MAQAO Toolset and Differential Analysis.pdf:pdf},
isbn = {978-3-319-08143-4},
month = {jan},
pages = {107--127},
title = {{PAMDA: Performance Assessment Using MAQAO Toolset and Differential Analysis}},
year = {2014}
}
@article{Weaver2016,
annote = {When conducting performance analysis the easiest type of report to get is for the total, aggregate results. The total number of cycles a program ran, the total number of cache misses, the total wall clock time the program ran. While this is of interest, often more detail is wanted. Where did the cache misses happen, what function took the most cycles, etc. The most straightforward way to get this type of information is to periodically interrupt the program's execution and gather performance information. A detailed breakdown of program behavior can be extrapolated based on these representative samples. There is a tradeoff between overhead and accuracy; the more often you sample the more accurate the results, but at the same time if you sample too frequently you will start to add overhead and affect the program being measured.},
author = {Weaver, Vincent M},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Advanced Hardware Profiling and Sampling (PEBS, IBS, etc.)- Creating a New PAPI Sampling Interface.pdf:pdf},
title = {{Advanced Hardware Profiling and Sampling (PEBS, IBS, etc.): Creating a New PAPI Sampling Interface}},
year = {2016}
}
@inproceedings{Williams2016,
abstract = {{\textcopyright} 2008 IEEE. This article consists of a collection of slides from the authors' conference presentation. The Roofline model is a visually intuitive figure for kernel analysis and optimization. The authors believe undergraduates will find it useful in assessing performance and scalability limitations. It is easily extended to other architectural paradigms. It is easily extendable to other metrics: □ performance (sort, graphics, crypto..) □ bandwidth (L2, PCIe, ..). A performance counters could be used to generate a runtime-specific roofline that would greatly aide the optimization.},
author = {Williams, Samuel and Patterson, David and Oliker, Leonid and Shalf, John and Yelick, Katherine},
booktitle = {2008 IEEE Hot Chips 20 Symposium, HCS 2008},
doi = {10.1109/HOTCHIPS.2008.7476531},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/The Roofline Model- A pedagogical tool for program analysis and optimization.pdf:pdf},
isbn = {9781467388719},
keywords = {roofline},
mendeley-tags = {roofline},
month = {aug},
pages = {1--71},
publisher = {IEEE},
title = {{The roofline model: A pedagogical tool for program analysis and optimization}},
url = {http://ieeexplore.ieee.org/document/7476531/ https://crd.lbl.gov/assets/pubs{\_}presos/parlab08-roofline-talk.pdf},
year = {2016}
}
@misc{JohnMcCalpin2015,
author = {{John McCalpin}},
title = {{How to read performance counters by rdpmc instruction?}},
url = {https://software.intel.com/en-us/forums/software-tuning-performance-optimization-platform-monitoring/topic/595214},
urldate = {2018-05-25},
year = {2015}
}
@misc{Kleen2015,
abstract = {Tracing is a technique that is used for both performance analysis and debugging. A tracer generates its data into a log; these are tracing events that can be later analyzed to understand the program's execution. Linux has ftrace that can log function calls and tracepoint data for the kernel. CPUs also support tracing mechanisms for different events. Processor Trace (PT) is a new tracing mechanism for Intel CPUs that traces branches executing on the CPU, which allows the reconstruction of the control flow of all executed code},
annote = {1{\`{e}}re lecture

{\_}{\_} Synth{\`{e}}se {\_}{\_} 


Objectif du papier, probl{\`{e}}me trait{\'{e}}




Contribution et solution propos{\'{e}}es




R{\'{e}}sultats obtenus




2e lecture 


{\_}{\_} Critique {\_}{\_}


Points forts




Points faibles, limites




Mon avis, remarques
},
author = {Kleen, Andi},
keywords = {trace},
mendeley-tags = {trace},
title = {{Adding Processor Trace support to Linux}},
url = {https://lwn.net/Articles/648154/},
urldate = {2018-05-25},
year = {2015}
}
@techreport{StephaneEranian2015,
author = {{Stephane Eranian}},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Stephane Eranian - 2015 - Linux perf{\_}events status update.pdf:pdf},
keywords = {outils},
mendeley-tags = {outils},
title = {{Linux perf{\_}events status update}},
url = {http://www.paradyn.org/petascale2015/slides/CSCADS2015{\_}perf{\_}events{\_}status{\_}update.pdf},
year = {2015}
}
@article{Hamilton2009,
abstract = {The Cooperative Expendable Micro-Slice Servers (CEMS) project evaluates low cost, low power servers for high-scale internetservices using commodity, client-side components. It is a followon project to the 2007 CIDR paper Architecture for Modular Data Centers 11. The goals of the CEMS project are to establish that low-cost, low-power servers produce better price/performance and better power/performance than current purpose-built servers. In addition, we aim to establish the viability and efficiency of a failin-place model. We use work done per dollar and work done per joule as measures of server efficiency and show that more, lowerpower servers produce the same aggregate throughput much more cost effectively and we use measured performance results from a large, consumer internet service to argue this point. Categories and Subject Descriptors B.8.8.m Hardware PERFORMANCE AND RELIABILITY Miscellaneous: Server design; low-power, low-cost client and embedded parts in servers.},
author = {Hamilton, James},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Hamilton - 2009 - Cooperative Expendable Micro-Slice Servers (CEMS) Low Cost, Low Power Servers for Internet-Scale Services.pdf:pdf},
journal = {Power},
pages = {1--8},
title = {{Cooperative Expendable Micro-Slice Servers ( CEMS ): Low Cost , Low Power Servers for Internet-Scale Services}},
url = {http://mvdirona.com/jrh/work http://www.mvdirona.com/jrh/TalksAndPapers/JamesHamilton{\_}CEMS.pdf},
year = {2009}
}
@article{DatacenterasaComputer:uneplongeedanslesdatacentersdesacteursducloud2011,
author = {{Datacenter as a Computer : une plong{\'{e}}e dans les datacenters des acteurs du cloud}},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Datacenter as a Computer une plong{\'{e}}e dans les datacenters des acteurs du cloud - 2011 - The Datacenter as a Computer An Introduction t.pdf:pdf},
title = {{The Datacenter as a Computer An Introduction to the Design of Warehouse-Scale Machines}},
url = {https://blog.octo.com/datacenter-as-a-computer-une-plongee-dans-les-datacenters-des-acteurs-du-cloud/},
year = {2011}
}
@misc{Brooks2017,
abstract = {I have been working on an upcoming post about megatrends and how they drive tech. I had included the end of Moore's Law to illustrate how the end of a megatrend might also have a big influence on tech, but that section got away from me, becoming much larger than the sections on each individual current megatrend. So I decided to break it out into a separate post and publish it first. Here it is.},
author = {Brooks, Rodney},
title = {{The End of Moore's Law – Rodney Brooks}},
url = {https://rodneybrooks.com/the-end-of-moores-law/},
urldate = {2018-05-25},
year = {2017}
}
@article{Rupp2013,
abstract = {Recently I was looking for useful graphs on recent parallel computing hardware for reuse in a presentation, but struggled to find any. While I know that colleagues have such graphs and data in use in their presentations, I couldn't find a convenient source on the net. So I ended up collecting all the data (again) and decided to make the outcome of my efforts available here.},
annote = {https://github.com/karlrupp/cpu-gpu-mic-comparison},
author = {Rupp, Karl},
keywords = {{\'{e}}volution technologique},
mendeley-tags = {{\'{e}}volution technologique},
title = {{CPU , GPU and MIC Hardware Characteristics over Time}},
url = {https://www.karlrupp.net/2013/06/cpu-gpu-and-mic-hardware-characteristics-over-time/},
year = {2013}
}
@article{Araiza2005,
abstract = {As useful as performance counters are, the meaning of reported aggregate event counts is sometimes questionable. Questions arise due to unanticipated processor behavior, overhead associated with the interface, the granularity of the monitored code, hardware errors, and lack of standards w.r.t. event definitions. To explore these issues, we are conducting a sequence of studies using carefully-crafted microbenchmarks that permit the accurate prediction of event counts and investigation of the differences between hardware-reported and predicted event counts. This paper presents the methodology employed, some of the microbenchmarks developed, and some of the information uncovered to date. The information provided by this work allows application developers to better understand the data provided by hardware performance counters and better utilize it to tune application performance. A goal of this research is to develop a cross-platform microbenchmark suite that can be used by application developers for these purposes. Some of the microbenchmarks in this suite are discussed in the paper.},
author = {Araiza, Roberto and {Thientam Pham} and Aguilera, M.G.},
doi = {10.1109/RTCDC.2005.201641},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Towards a cross-platform microbenchmark suite for evaluating hardware performance counter data.pdf:pdf},
isbn = {1-59593-257-7},
journal = {2005 Richard Tapia Celebration of Diversity in Computing Conference},
keywords = {Experimentation,Hardware performance counters,Measurement,Reliability,Verification,microbenchmarks},
pages = {36--39},
title = {{Towards a cross-platform microbenchmark suite for evaluating hardware performance counter data}},
url = {http://digitalcommons.utep.edu/cgi/viewcontent.cgi?article=1263{\&}context=cs{\_}techrep{\%}5Cnhttp://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1570873},
year = {2005}
}
@article{Williams2008,
abstract = {We propose an easy-to-understand, visual performance model that offers insights to programmers and architects on improving parallel software and hardware for floating point computations.},
author = {Patterson, David and WIlliams, Samuel and Waterman, Andrew},
doi = {10.1145/1498765.1498785},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Williams, Waterman, Patterson - 2008 - Roofline An Insightful Visual Performance Model for Floating-Point Programs and Multicore Archite.pdf:pdf},
journal = {Communications of the ACM},
pages = {65--76},
title = {{Roofline: An Insightful Visual Performance Model for Multicore Architectures}},
url = {https://people.eecs.berkeley.edu/{~}kubitron/cs252/handouts/papers/RooflineVyNoYellow.pdf},
year = {2009}
}
@article{Wang2016,
abstract = {Kernel rootkits are formidable threats to computer systems. They are stealthy and can have unrestricted access to system resources. This paper presents NumChecker, a new virtual machine (VM) monitor based framework to detect and identify control-flow modifying kernel rootkits in a guest VM. NumChecker detects and identifies malicious modifications to a system call in the guest VM by measuring the number of certain hardware events that occur during the system call's execution. To automatically count these events, NumChecker leverages the hardware performance counters (HPCs), which exist in modern processors. By using HPCs, the checking cost is significantly reduced and the tamper-resistance is enhanced. We implement a prototype of NumChecker on Linux with the kernel-based VM. An HPC-based two-phase kernel rootkit detection and identification technique is presented and evaluated on a number of real-world kernel rootkits. The results demonstrate its practicality and effectiveness.},
author = {Wang, Xueyang and Karri, Ramesh},
doi = {10.1109/TCAD.2015.2474374},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Reusing Hardware Performance Counters to Detect and Identify Kernel Control-Flow Modifying Rootkits.pdf:pdf},
isbn = {9781450320719},
issn = {02780070},
journal = {IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems},
keywords = {Controlflow Modifying Kernel Rootkits,Hardware Performance Counters,Rootkit Detection and Identification,Virtualization},
month = {mar},
number = {3},
pages = {485--498},
title = {{Reusing Hardware Performance Counters to Detect and Identify Kernel Control-Flow Modifying Rootkits}},
url = {http://ieeexplore.ieee.org/document/7229276/},
volume = {35},
year = {2016}
}
@article{Abbas2017,
author = {Abbas, Muhamed Fauzi Bin and Kadiyala, Sai Praveen and Prakash, Alok and ...},
doi = {10.23919/TRONSHOW.2017.8275073},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Hardware performance counters based runtime anomaly detection using SVM.pdf:pdf},
isbn = {9784893623317},
journal = {TRON Symposium {\ldots}},
month = {dec},
pages = {1--9},
publisher = {IEEE},
title = {{Hardware performance counters based runtime anomaly detection using SVM}},
url = {http://ieeexplore.ieee.org/document/8275073/ http://ieeexplore.ieee.org/abstract/document/8275073/},
year = {2017}
}
@article{Saez2017,
abstract = {{\textcopyright} 2016 The British Computer Society. All rights reserved. Hardware performance monitoring counters (PMCs) have proven effective in characterizing application performance. Because PMCs can only be accessed directly at the OS privilege level, kernellevel tools must be developed to enable the end-user and userspace programs to access PMCs. A large body of work has demonstrated that the OS can perform effective runtime optimizations in multicore systems by leveraging performance-counter data. Special attention has been paid to optimizations in the OS scheduler. While existing performance monitoring tools greatly simplify the collection of PMC application data from userspace, they do not provide an architecture-agnostic kernel-level mechanism that is capable of exposing high-level PMC metrics to OS components, such as the scheduler. As a result, the implementation of PMC-based OS scheduling schemes is typically tied to specific processor models. To address this shortcoming we present PMCTrack, a novel tool for the Linux kernel that provides a simple architecture-independent mechanism that makes it possible for the OS scheduler to access per-thread PMC data. Despite being an OSoriented tool, PMCTrack still allows the gathering of monitoring data from userspace, enabling kernel developers to carry out the necessary offline analysis and debugging to assist them during the scheduler design process. In addition, the tool provides both the OS and the user-space PMCTrack components with other insightful metrics available in modern processors and which are not directly exposed as PMCs, such as cache occupancy or energy consumption. This information is also of great value when it comes to analyzing the potential benefits of novel scheduling policies on real systems. In this paper, we analyze different case studies that demonstrate the flexibility, simplicity and powerful features of PMCTrack.},
author = {Saez, J. C. and Pousa, A. and Rodriguez-Rodriguez, R. and Castro, F. and Prieto-Matias, M.},
doi = {10.1093/comjnl/bxw065},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/PMCTrack- Delivering Performance Monitoring Counter Support to the OS Scheduler.pdf:pdf},
issn = {14602067},
journal = {Computer Journal},
keywords = {Asymmetric multicore,Cache monitoring,Energy efficiency,Intel CMT,Linux kernel,OS scheduling,PMCTrack,Performance monitoring counters},
month = {jan},
number = {1},
pages = {60--85},
title = {{PMCTrack: Delivering performance monitoring counter support to the OS scheduler}},
url = {https://academic.oup.com/comjnl/article-lookup/doi/10.1093/comjnl/bxw065},
volume = {60},
year = {2017}
}
@article{Malone2011,
abstract = {In this paper, we propose to use hardware performance counters (HPC) to detect malicious program modifications at load time (static) and at runtime (dynamic). HPC have been used for program characterization and testing, system testing and performance evaluation, and as side channels. We propose to use HPCs for static and dynamic integrity checking of programs.. The main advantage of HPC-based integrity checking is that it is almost free in terms of hardware cost; HPCs are built into almost all processors. The runtime performance overhead is minimal because we use the operating system for integrity checking, which is called anyway for process scheduling and other interrupts. Our preliminary results confirm that HPC very efficiently detect program modifications with very low cost. {\textcopyright} 2011 ACM.},
address = {New York, New York, USA},
author = {Malone, Corey and Zahran, Mohamed and Karri, Ramesh},
doi = {10.1145/2046582.2046596},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Are Hardware Performance Counters a Cost Effective Way for Integrity Checking of Programs.pdf:pdf},
isbn = {9781450310017},
issn = {15437221},
journal = {Proceedings of the sixth ACM workshop on Scalable trusted computing - STC '11},
keywords = {hardware performance counters,integrity},
pages = {71},
publisher = {ACM Press},
title = {{Are Hardware Performance Counters a Cost Effective Way for Integrity Checking of Programs}},
url = {http://dl.acm.org/citation.cfm?doid=2046582.2046596 http://www.scopus.com/inward/record.url?eid=2-s2.0-80755143408{\&}partnerID=40{\&}md5=ad5db1f8e5c0131a2a17f457ba1b0497{\%}5Cnhttp://dl.acm.org/citation.cfm?doid=2046582.2046596},
year = {2011}
}
@inproceedings{Molka2017,
address = {New York, New York, USA},
author = {Molka, Daniel and Sch{\"{o}}ne, Robert and Hackenberg, Daniel and Nagel, Wolfgang E.},
booktitle = {Proceedings of the 8th ACM/SPEC on International Conference on Performance Engineering - ICPE '17},
doi = {10.1145/3030207.3030223},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Detecting Memory-Boundedness with Hardware Performance Counters.pdf:pdf},
isbn = {9781450344043},
keywords = {benchmarking,hardware performance counters,performance analysis},
pages = {27--38},
publisher = {ACM Press},
title = {{Detecting Memory-Boundedness with Hardware Performance Counters}},
url = {http://dl.acm.org/citation.cfm?doid=3030207.3030223},
year = {2017}
}
@inproceedings{Moore2011,
address = {New York, New York, USA},
author = {Moore, Shirley and Terpstra, Dan and Weaver, Vince and Jagode, Heike and Ralph, James and Dongarra, Jack},
booktitle = {Proceedings of the 2011 companion on High Performance Computing Networking, Storage and Analysis Companion - SC '11 Companion},
doi = {10.1145/2148600.2148603},
isbn = {9781450310307},
keywords = {gpu,hardware counters,performance measurement,virtual computing},
pages = {3},
publisher = {ACM Press},
title = {{Poster: New Features of the PAPI Hardware Counter Library}},
url = {http://dl.acm.org/citation.cfm?doid=2148600.2148603},
year = {2011}
}
@inproceedings{Lee2015,
address = {New York, New York, USA},
author = {Lee, Kyu-Yean and Cho, Seong-Jin and Yoon, Seung-Hyun and Jeon, Jae-Wook},
booktitle = {Proceedings of the 9th International Conference on Ubiquitous Information Management and Communication - IMCOM '15},
doi = {10.1145/2701126.2701150},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Roofline Measurement Application.pdf:pdf},
isbn = {9781450333771},
keywords = {hardware performance counter,mobile AP,monitoring,multicore processor},
pages = {1--6},
publisher = {ACM Press},
title = {{Roofline measurement application}},
url = {http://dl.acm.org/citation.cfm?doid=2701126.2701150 http://dl.acm.org/citation.cfm?id=2701126.2701150},
year = {2015}
}
@article{Neill2017,
abstract = {Collecting hardware event counts is essential to understanding program execution behavior. Contemporary systems offer few Performance Monitoring Counters (PMCs), thus only a small fraction of hardware events can be monitored simultaneously. We present new techniques to acquire counts for all available hardware events with high accuracy by multiplexing PMCs across multiple executions of the same program, then care-fully reconciling and merging the multiple profiles into a single, coherent profile. We present a new metric for assessing the similarity of statistical distributions of event counts and show that our execution profiling approach performs significantly better than Hardware Event Multiplexing.},
author = {Neill, Richard and Drebes, Andi and Pop, Antoniu},
doi = {10.1145/3148054},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Fuse- Accurate Multiplexing of Hardware Performance Counters Across Executions.pdf:pdf},
issn = {15443566},
journal = {ACM Transactions on Architecture and Code Optimization},
keywords = {@BULLET General and reference → Performance,Additional Key Words and Phrases,CCS Concepts,Hardware event monitoring,Measurement,Parallel programming lan-guages,hardware event multiplexing,performance monitoring counters,task-parallel performance analysis},
month = {dec},
number = {4},
pages = {1--26},
publisher = {ACM},
title = {{Fuse: Accurate Multiplexing of Hardware Performance Counters Across Executions}},
url = {http://dl.acm.org/citation.cfm?doid=3154814.3148054 https://doi.org/10.1145/3148054{\%}0Ahttp://dl.acm.org/citation.cfm?doid=3154814.3148054},
volume = {14},
year = {2017}
}
@article{Sithole2011,
abstract = {Enterprise IT environments have seen a sharp growth in content use due to the popularity of on-demand data-intensive applications. In turn, the huge demand in content has spawned off major developments such as growth and distribution of computing nodes as well as the adoption of various implementation technologies. Given the complexity brought to the makeup of business computing environments in addressing the above-mentioned factors, the critical planning task of determining the appropriate infrastructure sizes for supporting firm Quality of Service (QoS) guarantees becomes a very challenging undertaking to fulfil. Benchmarking methods are widely employed in calibrating attainable performance in IT solutions, but these have the drawback of presenting output performance metrics as composite measurements that only give an end-to-end perspective. As an enhancement to benchmarking approaches, we explore the use of Performance Monitoring Counters (PMCs) in obtaining detailed operational performance of CPU and memory hardware. Performance Monitoring Counters (PMCs) are on-chip registers found on most modern processor hardware. We use PMC-derived measurements to validate cache performance trends that have been derived analytically, and in the course of validations, PMC data is also used to investigate the nature and character of surges in cache miss events, which emerge as the memory load generated by runtime processes increases},
address = {New York, New York, USA},
author = {Sithole, Ernest and McClean, Sally and Scotney, Bryan and Parr, Gerard and Moore, Adrian and Dawson, Stephen},
doi = {10.1145/2160803.2160845},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Characterization, Monitoring and Evaluation of Operational Performance Trends on Server Processor Hardware.pdf:pdf},
isbn = {9781450305198},
issn = {01635999},
journal = {ACM SIGMETRICS Performance Evaluation Review},
keywords = {benchmarks,performance evaluations,performance monitoring counters},
number = {3},
pages = {20},
publisher = {ACM Press},
title = {{Characterization, monitoring and evaluation of operational performance trends on server processor hardware}},
url = {http://portal.acm.org/citation.cfm?doid=1958746.1958801},
volume = {39},
year = {2011}
}
@article{Dongarra2013,
author = {Dongarra, Jack and Heroux, Ma},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Toward a new metric for ranking high performance computing systems.pdf:pdf},
issn = {2095-5138},
journal = {Sandia Report, SAND2013-4744},
number = {June},
pages = {1--19},
title = {{Toward a new metric for ranking high performance computing systems}},
url = {http://www.sandia.gov/{~}maherou/docs/HPCG-Benchmark.pdf},
year = {2013}
}
@article{erikssonb2016profiling,
author = {Eriksson, Jerry and Ojeda-may, Pedro and Ponweiser, Thomas and Steinreiter, Thomas},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Profiling and Tracing Tools for Performance Analysis of Large Scale Applications.pdf:pdf},
journal = {PRACE: Partnership for Advanced Computing in Europe},
keywords = {extrae,gromacs,hpctoolkit,itac,paraver,performance analysis,profiling,scalasca,tracing},
pages = {1--30},
title = {{Profiling and Tracing Tools for Performance Analysis of Large Scale Applications}},
year = {2016}
}
@article{dongarra2016high,
abstract = {Flood risk evaluation and prediction represents an essential analytic step to coherently link flood control and disaster mitigation. The paper established a hybrid evaluation model based on fuzzy analytic hierarchy process (AHP) and triangular fuzzy number. It comprises flood risk evaluation and prediction to obtain risk factors ranking and comprehensive flood risk prediction, and then analyzed flood risk response measures. A case study is proposed entailing a flood risk evaluation and prediction in the Lower Yangtze River region. The evaluation results showed that the proposed evaluation and prediction model was capable of adequately representing the actual setting. In addition, a comparison with the previously described AHP and trapezoidal fuzzy AHP, and experimental results are encouraging, which fully demonstrates the effectiveness and superiority of the proposed model.},
author = {Dongarra, Jack and Heroux, Michael A. and Luszczek, Piotr},
doi = {10.1093/nsr/nwv084},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/HPCG Benchmark- a New Metric for Ranking High Performance Computing Systems.pdf:pdf},
issn = {2095-5138},
journal = {National Science Review},
number = {1},
pages = {30--35},
publisher = {SAGE Publications Sage UK: London, England},
title = {{A new metric for ranking high-performance computing systems}},
volume = {3},
year = {2016}
}
@article{Zhang2007,
abstract = {Today's processors provide a rich source of statis- tical information on program execution characteristics through hardware counters. However, traditionally, operating system (OS) support for and utilization of the hardware counter statistics has been limited and ad hoc. In this paper, we make the case for direct OS management of hardware counter statistics. First, we show the utility of processor counter statistics in CPU scheduling (for improved performance and fairness) and in online workload modeling, both of which require online contin- uous statistics (as opposed to ad hoc infrequent uses). Second, we show that simultaneous system and user use of hardware counters is possible via time-division multiplexing. Finally, we highlight potential counter misuses to indicate that the OS should address potential security issues in utilizing processor counter statistics.},
author = {Zhang, Xiao and Dwarkadas, Sandhya and Folkmanis, Girts and Shen, Kai},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Zhang et al. - 2007 - Processor hardware counter statistics as a first-class system resource.pdf:pdf},
pages = {14},
title = {{Processor hardware counter statistics as a first-class system resource}},
url = {http://dl.acm.org/citation.cfm?id=1361397.1361411},
year = {2007}
}
@inproceedings{Tinetti2014,
abstract = {Program performance optimization could be a very complex process, even with current software development facilities/ tools. An Integrated Development Environment (IDE) usually does not include many aids for optimization and/or performance evaluation. We propose to include performance evaluation through hardware monitoring counters into IDE software. Currently, it is possible to reach hardware monitoring counters via many libraries, and we have also seen that many of those libraries are approximately at the same abstraction level (including the way at which they allow access to the hardware counters). Thus, it is not only possible to include some performance evaluation library into the development process but, also, including specific aids to use some library via configurable/adjustable code snippets. We show, as a proof of concept, an Eclipse plug-in to help High Performance Computing (HPC) programmers to access hardware monitoring event counters using PAPI (Performance API). The plug-in is able to automatically include source code to count specific events available via PAPI in sections of source code defined by the programmer. Also, given that the code is automatically included, it would be also possible to remove that code from the release version (for the production environment).},
author = {Tinetti, Fernando G. and M{\'{e}}ndez, Mariano},
booktitle = {Proceedings - 2014 International Conference on Computational Science and Computational Intelligence, CSCI 2014},
doi = {10.1109/CSCI.2014.19},
isbn = {9781479930098},
keywords = {Fortran,Hardware Counters,High Performance Computing,Performance},
month = {mar},
pages = {71--76},
publisher = {IEEE},
title = {{An automated approach to hardware performance monitoring counters}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6822086},
volume = {1},
year = {2014}
}
@inproceedings{Leng2017,
abstract = {{\textcopyright} 2017 IEEE. As technology scaling reaches nanometre scales, the error rate due to variations in temperature and voltage, single event effects and component degradation increases, making components less reliable. In order to ensure a system continues to function correctly while facing known reliability issues, it is imperative that the system should have the means to detect the occurrence of errors due to the presence of faults. A system that behaves normally (no error detected in the system) exhibits a profile, and any deviations from this profile indicate that there is an anomaly in the system. In this paper, we propose to use hardware performance counters (HPCs) to measure events that occur during the execution of the program. We explore the various counters available which could be use to identify the anomalous behaviour in the system and develop a methodology to observe the anomalies using HPCs by creating a fault-free pattern and observing any subsequent changes in that pattern. We evaluate the proposed technique using GemFI, an architectural simulator based on Gem5 with additional fault injection capabilities. We compare the results obtained at the end of the execution with data collected during a time interval. Our results show that HPCs can be used to identify anomalous behaviour in a system that would lead to failure.},
author = {Leng, Elena Woo Lai and Zwolinski, Mark and Halak, Basel},
booktitle = {2017 2nd International Verification and Security Workshop, IVSW 2017},
doi = {10.1109/IVSW.2017.8031548},
isbn = {9781538617083},
month = {jul},
pages = {76--81},
publisher = {IEEE},
title = {{Hardware performance counters for system reliability monitoring}},
url = {http://ieeexplore.ieee.org/document/8031548/},
year = {2017}
}
@inproceedings{london2001end,
author = {London, Kevin S and Dongarra, Jack and Moore, Shirley and Mucci, Philip and Seymour, Keith and Spencer, Thomas},
booktitle = {ISCA PDCS},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/End-user Tools for Application Performance Analysis Using Hardware Counters .pdf:pdf},
pages = {460--465},
title = {{End-user Tools for Application Performance Analysis Using Hardware Counters.}},
year = {2001}
}
@article{Browne2000,
abstract = {The purpose of the PAPI project is to specify a standard application programming interface (API) for accessing hardware performance counters available on most modern microprocessors. These counters exist as a small set of registers that count events, which are occurrences of specific signals and states related to the processor's function. Monitoring these events facilitates correlation between the structure of source/object code and the efficiency of the mapping of that code to the underlying architecture. This correlation has a variety of uses in performance analysis, including hand tuning, compiler optimization, debugging, benchmarking, monitoring, and performance modeling. In addition, it is hoped that this information will prove useful in the development of new compilation technology as well as in steering architectural development toward alleviating commonly occurring bottlenecks in high performance computing.},
author = {Browne, S. and Dongarra, Jack and Garner, N. and Ho, G. and Mucci, P.},
doi = {10.1177/109434200001400303},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/A Portable Programming Interface for Performance Evaluation on Modern Processors.pdf:pdf},
isbn = {1094-3420},
issn = {10943420},
journal = {International Journal of High Performance Computing Applications},
month = {aug},
number = {3},
pages = {189--204},
publisher = {Sage Publications, Inc.},
title = {{A portable programming interface for performance evaluation on modern processors}},
url = {http://journals.sagepub.com/doi/10.1177/109434200001400303},
volume = {14},
year = {2000}
}
@unknown{unknown,
abstract = {This article is reprinted from the Internaional Electron Devices Meeting (1975). It discusses the complexity of integrated circuits, identifies their manufacture, production, and deployment, and addresses trends to their future deployment.},
author = {Moore, Gordon E.},
booktitle = {IEEE Solid-State Circuits Newsletter},
doi = {10.1109/N-SSC.2006.4804410},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Moore - Progress In Digital Integrated Electronics.pdf:pdf},
isbn = {doi:10.1109/N-SSC.2006.4804410},
issn = {1098-4232},
number = {3},
pages = {36--37},
title = {{Progress In Digital Integrated Electronics}},
url = {http://ieeexplore.ieee.org/document/4804410/},
volume = {20},
year = {1975}
}
@article{Moore1998,
abstract = {With unit cost falling as the number of components per circuit rises, by 1975 economics may dictate squeezing as many as 65 000 components on a single silicon chip. The future of integrated electronics is the future of electronics itself. The advantages of integration will bring about a proliferation of electronics, pushing this science into many new areas. Integrated circuits will lead to such wonders as home computers—or at least terminals connected to a central computer—automatic controls for automobiles, and per-sonal portable communications equipment. The electronic wristwatch needs only a display to be feasible today. But the biggest potential lies in the production of large systems. In telephone communications, integrated circuits in digital filters will separate channels on multiplex equip-ment. Integrated circuits will also switch telephone circuits and perform data processing. Computers will be more powerful, and will be organized in completely different ways. For example, memories built of integrated electronics may be distributed throughout the machine instead of being concentrated in a central unit. In addition, the improved reliability made possible by integrated circuits will allow the construction of larger processing units. Machines similar to those in existence today will be built at lower costs and with faster turn-around. I. PRESENT AND FUTURE By integrated electronics, I mean all the various tech-nologies which are referred to as microelectronics today as well as any additional ones that result in electronics functions supplied to the user as irreducible units. These technologies were first investigated in the late 1950's. The object was to miniaturize electronics equipment to include increasingly complex electronic functions in limited space with minimum weight. Several approaches evolved, includ-ing microassembly techniques for individual components, thin-film structures, and semiconductor integrated circuits.},
author = {Moore, G E},
doi = {10.1109/JPROC.1998.658762},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/IEEE, 1998 - Unknown - Cramming more components onto integrated circuits.pdf:pdf},
isbn = {1558605398},
issn = {0018-9219},
journal = {Electronics},
month = {jan},
number = {8},
pages = {82--85},
pmid = {21527652},
title = {{Cramming More Components Onto Integrated Circuits}},
url = {http://scholar.google.com/scholar?q=related:JabE1kOqjR0J:scholar.google.com/{\&}hl=en{\&}num=30{\&}as{\_}sdt=0,5{\%}5Cnpapers2://publication/uuid/3AFDBFE5-84E1-4F06-A839-05A7DB1A5348},
volume = {38},
year = {1965}
}
@article{GeoffreyFoxJhaShantenu2015,
author = {{Geoffrey Fox, Jha Shantenu}, and Lavanya Ramakrishnan},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/First workshop on streaming and steering applications- Requirements and infrastructure.pdf:pdf},
pages = {1--24},
title = {{First workshop on streaming and steering applications: Requirements and infrastructure}},
url = {http://streamingsystems.org/stream2015.html},
year = {2015}
}
@techreport{HerbSutter2011,
abstract = {In the twilight of Moore's Law, the transitions to multicore processors, GPU computing, and HaaS cloud computing are not separate trends, but aspects of a single trend – mainstream computers from desktops to ‘smartphones' are being permanently transformed into heterogeneous supercomputer clusters. Henceforth, a single compute-intensive application will need to harness different kinds of cores, in immense numbers, to get its job done},
author = {{Herb Sutter}},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Herb Sutter - 2011 - Welcome to the Jungle.pdf:pdf},
title = {{Welcome to the Jungle}},
url = {http://herbsutter.com/welcome-to-the-jungle/},
year = {2011}
}
@article{Zhang2007a,
abstract = {Today's processors provide a rich source of statis- tical information on program execution characteristics through hardware counters. However, traditionally, operating system (OS) support for and utilization of the hardware counter statistics has been limited and ad hoc. In this paper, we make the case for direct OS management of hardware counter statistics. First, we show the utility of processor counter statistics in CPU scheduling (for improved performance and fairness) and in online workload modeling, both of which require online contin- uous statistics (as opposed to ad hoc infrequent uses). Second, we show that simultaneous system and user use of hardware counters is possible via time-division multiplexing. Finally, we highlight potential counter misuses to indicate that the OS should address potential security issues in utilizing processor counter statistics.},
author = {Zhang, Xiao and Dwarkadas, Sandhya and Folkmanis, Girts and Shen, Kai},
journal = {usenix.org},
pages = {14},
title = {{Processor hardware counter statistics as a first-class system resource}},
url = {https://www.usenix.org/event/hotos07/tech/full{\_}papers/zhang/zhang.pdf http://dl.acm.org/citation.cfm?id=1361397.1361411},
year = {2007}
}
@phdthesis{Capra2015,
abstract = {Afin de r{\'{e}}pondre aux besoins croissants de la simulation num{\'{e}}rique et de rester {\`{a}} la pointe de la technologie, les supercalculateurs doivent d'{\^{e}}tre constamment am{\'{e}}lior{\'{e}}s. Ces am{\'{e}}liorations peuvent {\^{e}}tre d'ordre mat{\'{e}}riel ou logiciel. Cela force les applications {\`{a}} s'adapter {\`{a}} un nouvel environnement de programmation au fil de son d{\'{e}}veloppement. Il devient alors n{\'{e}}cessaire de se poser la question de la p{\'{e}}rennit{\'{e}} des applications et de leur portabilit{\'{e}} d'une machine {\`{a}} une autre. L'utilisation de machines virtuelles peut {\^{e}}tre une premi{\`{e}}re r{\'{e}}ponse {\`{a}} ce besoin de p{\'{e}}rennisation en stabilisant les environnements de programmation. Gr{\^{a}}ce {\`{a}} la virtualisation, une application peut {\^{e}}tre d{\'{e}}velopp{\'{e}}e au sein d'un environnement fig{\'{e}}, sans {\^{e}}tre directement impact{\'{e}}e par l'environnement pr{\'{e}}sent sur une machine physique. Pour autant, l'abstraction suppl{\'{e}}mentaire induite par les machines virtuelles entraine en pratique une perte de performance. Nous proposons dans cette th{\`{e}}se un ensemble d'outils et de techniques afin de permettre l'utilisation de machines virtuelles en contexte HPC. Tout d'abord nous montrons qu'il est possible d'optimiser le fonctionnement d'un hyperviseur afin de r{\'{e}}pondre le plus fid{\`{e}}lement aux contraintes du HPC que sont : le placement des fils d'ex{\'{e}}cution et la localit{\'{e}} m{\'{e}}moire des donn{\'{e}}es. Puis en s'appuyant sur ce r{\'{e}}sultat, nous avons propos{\'{e}} un service de partitionnement des ressources d'un noeud de calcul par le biais des machines virtuelles. Enfin, pour {\'{e}}tendre nos travaux {\`{a}} une utilisation pour des applications MPI, nous avons {\'{e}}tudi{\'{e}} les solutions et performances r{\'{e}}seau d'une machine virtuelle.},
annote = {Introduction au HPC interessante
Suite de la th{\`{e}}se sur la virtualisation},
author = {Capra, Antoine},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/These - Virtualisation en contexte HPC.pdf:pdf},
keywords = {Calcul haute performance,High Performance Computing,MPI,OpenMP,Virtualisation},
month = {dec},
publisher = {Universit{\'{e}} de Bordeaux},
title = {{Virtualisation en contexte HPC}},
url = {https://tel.archives-ouvertes.fr/tel-01280434/},
year = {2015}
}
@article{Denoyelle2016,
abstract = {L'av{\`{e}}nement des machines multicoeurs et manycoeurs a permis en partie de soutenir la crois-sance de la performance des machines de calcul. Cependant, l'accroissement du nombre d'uni-t{\'{e}}s d'ex{\'{e}}cution n'est pas n{\'{e}}cessairement synonyme de r{\'{e}}duction de la dur{\'{e}}e d'ex{\'{e}}cution. Les diff{\'{e}}rentes t{\^{a}}ches d'une application peuvent entre autre s'interrompre pour envoyer ou at-tendre des messages (synchronisations) ou concourir pour utiliser une ressource mat{\'{e}}rielle partag{\'{e}}e dans la machine. Lorsque la modification du code n'est pas permise (e.g dans un support d'ex{\'{e}}cution), le placement des t{\^{a}}ches et des donn{\'{e}}es permet de surmonter en partie ces probl{\`{e}}mes, respectivement en minimisant le chemin des communications, ou en {\'{e}}quilibrant la charge sur la machine. Dans ces deux cas, il est n{\'{e}}cessaire d'avoir un mod{\`{e}}le de l'architec-ture et des m{\'{e}}triques sur l'utilisation de celle-ci, pour esp{\'{e}}rer caract{\'{e}}riser le couple (machine, application) et calculer un placement efficace. Dans ce contexte, nous proposons un outil de mesure de performances, permettant d'agr{\'{e}}ger des {\'{e}}v{\`{e}}nements collect{\'{e}}s au cours du temps sur des noeuds de la topologie d'une machine afin d'en tirer une analyse coupl{\'{e}}e du programme et de la machine. Cet outil est bas{\'{e}} sur un mod{\`{e}}le d'architecture fourni par hwloc et des greffons de collecte d'{\'{e}}v{\`{e}}nements (impl{\'{e}}ment{\'{e}}s avec papi et maqao) et permet l'analyse d'applications parall{\`{e}}les sur un syst{\`{e}}me. L'outil propos{\'{e}} ne r{\'{e}}alise pas directement cette analyse mais propose des m{\'{e}}canismes pour y parvenir. Un utilitaire permet d'afficher la topologie et l'{\'{e}}volution des {\'{e}}v{\`{e}}nements ainsi que de g{\'{e}}n{\'{e}}rer une trace des {\'{e}}v{\`{e}}nements collect{\'{e}}s, accompagn{\'{e}}s de leur localisation, au cours du temps. Nous montrons avec une application simple {\'{e}}crite pour l'occasion qu'il est possible d'utiliser les r{\'{e}}sultats fournis par notre outil pour d{\'{e}}duire un placement des fils d'ex{\'{e}}cution plus efficace.},
author = {Denoyelle, Nicolas},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/INRIA - Moniteurs hi{\'{e}}rarchiques de performance, pour g{\'{e}}rer l'utilisation des ressources partag{\'{e}}es de la topologie.pdf:pdf},
month = {jul},
title = {{Moniteurs hi{\'{e}}rarchiques de performance, pour g{\'{e}}rer l'utilisation des ressources partag{\'{e}}es de la topologie}},
url = {https://hal.archives-ouvertes.fr/hal-01343152v1},
year = {2016}
}
@article{Poncet2013,
author = {Poncet, Rapha{\"{e}}l},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/High performance computing challenges for research and industrial simulation codes.pdf:pdf},
journal = {hal.archives-ouvertes.fr},
title = {{High performance computing challenges for research and industrial simulation codes}},
url = {https://hal.archives-ouvertes.fr/hal-00855832/},
year = {2013}
}
@phdthesis{Valat2016,
abstract = {L'{\'{e}}volution des architectures des calculateurs actuels est telle que la m{\'{e}}moire devient un probl{\`{e}}me majeur pour les performances. L'{\'{e}}tude d{\'{e}}crite dans ce document montre qu'il est d{\'{e}}j{\`{a}} possible d'observer des pertes importantes imputables aux m{\'{e}}canismes de gestion de cette derni{\`{e}}re. Dans ce contexte, nous nous sommes int{\'{e}}ress{\'{e}}s aux probl{\`{e}}mes de gestion des gros segments m{\'{e}}moire sur les supercalculateurs multicoeurs NUMA de type Tera 100 et Curie. Notre travail est d{\'{e}}taill{\'{e}} ici en suivant trois axes principaux. Nous analysons dans un premier temps les politiques de pagination de diff{\'{e}}rents syst{\`{e}}mes d'exploitation (coloration de pages, grosses pages...). Nous mettons ainsi en {\'{e}}vidence l'existence d'interf{\'{e}}rences n{\'{e}}fastes entre ces politiques et les d{\'{e}}cisions de placement de l'allocateur en espace utilisateur. Nous compl{\'{e}}tons donc les {\'{e}}tudes cache/allocateur et cache/pagination par une analyse de l'interaction cumul{\'{e}}e de ces composants. Nous abordons ensuite la probl{\'{e}}matique des performances d'allocation des grands segments m{\'{e}}moire en consid{\'{e}}rant les {\'{e}}changes entre le syst{\`{e}}me et l'allocateur. Nous montrons ici qu'il est possible d'obtenir des gains significatifs (de l'ordre de 50{\%} sur une grosse application) en limitant ces {\'{e}}changes et en structurant l'allocateur pour un support explicite des architectures NUMA. La description de nos travaux s'ach{\`{e}}ve sur une {\'{e}}tude des probl{\`{e}}mes d'extensibilit{\'{e}} observ{\'{e}}s au niveau des fautes de pages du noyau Linux. Nous avons ainsi propos{\'{e}} une extension de la s{\'{e}}mantique d'allocation afin d'{\'{e}}liminer la n{\'{e}}cessit{\'{e}} d'effectuer les co{\^{u}}teux effacements m{\'{e}}moire des pages au niveau syst{\`{e}}me.},
annote = {Introduction au HPC
Puis la these parle de m{\'{e}}moire allocation large page etc.},
author = {Valat, S{\'{e}}bastien},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Valat - 2014 - Contribution {\`{a}} l'am{\'{e}}lioration des m{\'{e}}thodes d'optimisation de la gestion de la m{\'{e}}moire dans le cadre du Calcul Haute P.pdf:pdf},
keywords = {HPC,NUMA,OS,allocateur,allocator,caches,hierarchie,linux,malloc,memory,multi-coeurs,m{\'{e}}moire,operating system,optimisation,optimization,parallel,supercalculateur,syst{\`{e}}me d'exploitation,threads},
month = {jul},
publisher = {University de Versaille Saint-Quentin en Yvelines},
title = {{Contribution {\`{a}} l'am{\'{e}}lioration des m{\'{e}}thodes d'optimisation de la gestion de la m{\'{e}}moire dans le cadre du Calcul Haute Performance}},
url = {https://hal.archives-ouvertes.fr/tel-01253537/},
year = {2016}
}
@misc{Mewtow2017,
author = {Mewtow},
title = {{Fonctionnement d'un ordinateur - La consommation d'{\'{e}}nergie d'un ordinateur}},
url = {https://fr.wikibooks.org/wiki/Fonctionnement{\_}d{\%}27un{\_}ordinateur/La{\_}consommation{\_}d{\%}27{\'{e}}nergie{\_}d{\%}27un{\_}ordinateur{\#}L'influence{\_}de{\_}la{\_}loi{\_}de{\_}Moore},
urldate = {2018-05-23},
year = {2017}
}
@article{Moore2006,
abstract = {This article is reprinted from the Internaional Electron Devices Meeting (1975). It discusses the complexity of integrated circuits, identifies their manufacture, production, and deployment, and addresses trends to their future deployment.},
author = {Moore, Gordon},
doi = {10.1109/N-SSC.2006.4804410},
isbn = {doi:10.1109/N-SSC.2006.4804410},
issn = {1098-4232},
journal = {IEEE Solid-State Circuits Newsletter},
month = {sep},
number = {3},
pages = {36--37},
title = {{Progress In Digital Integrated Electronics [Technical literaiture, Copyright 1975 IEEE. Reprinted, with permission. Technical Digest. International Electron Devices Meeting, IEEE, 1975, pp. 11-13.]}},
url = {http://ieeexplore.ieee.org/document/4804410/},
volume = {20},
year = {2006}
}
@misc{Top5002018a,
author = {Top500},
title = {{First supercomputer in 1994}},
url = {https://www.top500.org/featured/systems/intel-xps-140-paragon-sandia-national-labs/{\#}Historical},
urldate = {2018-05-23},
year = {2018}
}
@misc{EuroHPC2018,
author = {EuroHPC},
title = {{Commission Europ{\'{e}}enne - Communiqu{\'{e}} de presse HPC et EuroHPC}},
url = {http://europa.eu/rapid/press-release{\_}MEMO-18-3{\_}en.htm},
urldate = {2018-05-23},
year = {2018}
}
@misc{H20202018,
author = {H2020},
title = {{Les diff{\'{e}}rents types de financements - Horizon 2020}},
url = {http://www.horizon2020.gouv.fr/cid81981/les-differents-instruments-regimes-financement.html},
urldate = {2018-05-23},
year = {2018}
}
@article{Saltz2018,
author = {Saltz, J},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Andre et al. - 2018 - BIG DATA AND EXTREME-SCALE COMPUTING PATHWAYS TO CONVERGENCE Toward a Shaping Strategy for a Future Software an(2).pdf:pdf},
number = {Icl},
title = {{Big Data and Extreme-Scale computing: pathways to convergence}},
url = {http://www.exascale.org/bdec/sites/www.exascale.org.bdec/files/whitepapers/bdec2017pathways.pdf},
year = {2018}
}
@article{Nahrstedt2017,
abstract = {As of 2014, 54{\%} of the earth's population resides in urban areas, and it is steadily increasing, expecting to reach 66{\%} by 2050. Urban areas range from small cities with tens of thousands of people to megacities with greater than 10 million people. Roughly 12{\%} of the global population today lives in 28 megacities, and at least 40 are projected by 2030. At these scales, the urban infrastructure such as roads, buildings, and utility networks will cover areas as large as New England. This steady urbanization and the resulting expansion of infrastructure, combined with renewal of aging urban infrastructure, represent tens of trillion of dollars in new urban infrastructure investment over the coming decades. These investments must balance factors including impact on clean air and water, energy and maintenance costs, and the productivity and health of city dwellers. Moreover, cost-effective management and sustainability of these growing urban areas will be one of the most critical challenges to our society, motivating the concept of science- and data-driven urban design, retrofit, and operation-that is, "Smart Cities".},
archivePrefix = {arXiv},
arxivId = {1705.01990},
author = {Nahrstedt, Klara and Cassandras, Christos G. and Catlett, Charlie},
eprint = {1705.01990},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Nahrstedt, Cassandras, Catlett - 2017 - City-Scale Intelligent Systems and Platforms.pdf:pdf},
month = {may},
title = {{City-Scale Intelligent Systems and Platforms}},
url = {http://arxiv.org/abs/1705.01990},
year = {2017}
}
@article{Chen2014,
author = {Chen, Min and Mao, Shiwen and Liu, Yunhao},
doi = {10.1007/s11036-013-0489-0},
issn = {1383-469X},
journal = {Mobile Networks and Applications},
keywords = {Big data,Big data analysis,Cloud computing,Data center,Hadoop,Internet of things,Smart grid},
month = {apr},
number = {2},
pages = {171--209},
publisher = {Springer-Verlag New York, Inc.},
title = {{Big Data: A Survey}},
url = {http://link.springer.com/10.1007/s11036-013-0489-0},
volume = {19},
year = {2014}
}
@article{Calyam2016,
abstract = {The Applications and Services in the Year 2021 workshop was successfully organized on January 27-28, 2016 in Washington DC through funding support from the National Science Foundation (NSF). The goal of the workshop was to foster discussions that bring together applications researchers in multidisciplinary areas, and developers/operators of research infrastructures at both national, regional, university and city levels. Discussions were organized to identify grand challenge applications and obtain the community voice and consensus on the key issues relating to applications and services that might be delivered by advanced infrastructures in the decade beginning in 2020. The timing and organization for the workshop is significant because today's digital infrastructure is undergoing deep technological changes and new paradigms are rapidly taking shape in both the core and edge domains that pose fundamental challenges. The key outcomes of the discussions were targeted to enhance the quality of peoples' lives while addressing important national priorities, leveraging today's cutting edge applications such as the Internet of Things, Big Data Analytics, Robotics, The Industrial Internet, and Immersive Virtual/Augmented Reality. This report summarizes the workshop efforts to bring together diverse groups for delivering targeted short/long talks, sharing latest advances, and identifying gaps that exist in the community for `research' and `infrastructure' needs that require future NSF funding.},
author = {Calyam, Prasad and Ricart, Glenn},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Calyam, Ricart - Unknown - Research and Infrastructure Challenges for Applications and Services in the Year 2021.pdf:pdf},
issn = {0146-4833},
journal = {ACM SIGCOMM Computer Communication Review},
number = {3},
title = {{Research and Infrastructure Challenges for Applications and Services in the Year 2021}},
url = {https://pdfs.semanticscholar.org/94e3/68ac2d38e1a68fb7a1dab1b83f000346f965.pdf},
volume = {46},
year = {2016}
}
@misc{DominikUlmer2016,
abstract = {D'{\'{e}}normes volumes de donn{\'{e}}es sont produits quotidiennement dans des domaines aussi vari{\'{e}}s que le sport ou le commerce, mais {\`{a}} quoi servent toutes ces donn{\'{e}}es si les entreprises ne sont pas en mesure de les exploiter efficacement ?},
author = {{Dominik Ulmer}},
title = {{Big data et HPC : Exploiter le big data de fa{\c{c}}on intelligente gr{\^{a}}ce au supercalcul}},
url = {https://www.decideo.fr/Big-data-et-HPC-Exploiter-le-big-data-de-facon-intelligente-grace-au-supercalcul{\_}a8812.html},
urldate = {2018-05-23},
year = {2016}
}
@article{Dennard1974,
author = {Dennard, R. H. and Gaensslen, F. H. and Yu, H. N. and Rideout, V. L. and Bassous, E. and LeBlanc, A. R.},
doi = {10.1109/JSSC.1974.1050511},
issn = {0018-9200},
journal = {IEEE Journal of Solid-State Circuits},
month = {oct},
number = {5},
pages = {256--268},
title = {{Design for ion-implanted MOSFET's with very small physical dimensions}},
url = {http://ieeexplore.ieee.org/document/1050511/},
volume = {9},
year = {1974}
}
@article{Sutter2005,
abstract = {The biggest sea change in software development since the OO revolution is knocking at the door, and its name is Concurrency.},
author = {Sutter, H},
doi = {10.1002/minf.201100042},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Herb Sutter - Unknown - The free lunch is over A fundamental turn toward concurrency in software.pdf:pdf},
issn = {18681751},
journal = {Dr. Dobb's Journal},
pages = {1--9},
title = {{The free lunch is over: A fundamental turn toward concurrency in software}},
url = {http://mondrian.die.udec.cl/{~}mmedina/Clases/ProgPar/Sutter - The Free Lunch is Over.pdf http://www.mscs.mu.edu/{~}rge/cosc2200/homework-fall2013/Readings/FreeLunchIsOver.pdf},
year = {2005}
}
@article{Dongarra2003,
abstract = {This paper describes the LINPACK Benchmark and some of its variations commonly used to assess the performance of computer systems. Aside from the LINPACK Benchmark suite, the TOP500 and the HPL codes are presented. The latter is frequently used to obtained results for TOP500 submissions. Information is also given on how to interpret the results of the benchmark and how the results fit into the performance evaluation process.},
author = {Dongarra, Jack J. and Luszczek, Piotr and Petite, Antoine},
doi = {10.1002/cpe.728},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Dongarra, Luszczek, Petitet - 2003 - The LINPACK Benchmark past, present and future.pdf:pdf},
isbn = {1532-0634},
issn = {15320626},
journal = {Concurrency Computation Practice and Experience},
keywords = {BLAS,Benchmarking,HPL,High-performance computing,LINPACK,Linear algebra,TOP500},
month = {aug},
number = {9},
pages = {803--820},
publisher = {Wiley-Blackwell},
title = {{The LINPACK benchmark: Past, present and future}},
url = {http://doi.wiley.com/10.1002/cpe.728},
volume = {15},
year = {2003}
}
@article{Kroese2014,
abstract = {Since the beginning of electronic computing, people have been interested in carrying out random experiments on a computer. Such Monte Carlo techniques are now an essential ingredient in many quantitative investigations. Why is the Monte Carlo method (MCM) so important today? This article explores the reasons why the MCM has evolved from a 'last resort'solution to a leading methodology that permeates much of contemporary science, finance, and engineering.},
author = {Kroese, Dirk P and Brereton, Tim and Taimre, Thomas and Botev, Zdravko I},
doi = {10.1002/wics.1314},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Kroese et al. - 2014 - Why the Monte Carlo Method is so important today Uses of the MCM.pdf:pdf},
issn = {19395108},
journal = {WIREs Computational Statistics},
keywords = {estimation,mcmc,monte carlo method,randomized optimization,simulation},
number = {6},
pages = {386--392},
title = {{Why the Monte Carlo Method is so important today Uses of the MCM}},
url = {https://people.smp.uq.edu.au/DirkKroese/ps/whyMCM{\_}final.pdf},
volume = {6},
year = {2014}
}
@misc{BorisMarcaillou2013,
author = {{Boris Marcaillou}},
title = {{D{\'{e}}ploiement de la fl{\^{u}}te et des canons sismiques}},
url = {https://ska-france.oca.eu/fr/antithesis-carnet-de-bord/200-deploiement-de-la-flute-et-des-canons-sismiques},
urldate = {2018-05-23},
year = {2013}
}
@misc{CEA2006,
author = {CEA},
title = {{Androm{\`{e}}de frapp{\'{e}}e en plein coeur}},
url = {http://irfu.cea.fr/Phocea/Vie{\_}des{\_}labos/Ast/ast.php?id{\_}ast=958{\&}t=actu},
urldate = {2018-05-23},
year = {2006}
}
@misc{Total2014,
author = {Total},
title = {{La difficile d{\'{e}}cision de lancer un forage | Plan{\`{e}}te {\'{E}}nergies}},
url = {https://www.planete-energies.com/fr/medias/decryptages/la-difficile-decision-de-lancer-un-forage},
urldate = {2018-05-23},
year = {2014}
}
@article{Block2006,
abstract = {The unusual morphology of the Andromeda galaxy (Messier 31, the closest spiral galaxy to the Milky Way) has long been an enigma. Although regarded for decades as showing little evidence of a violent history, M31 has a well-known outer ring of star formation at a radius of ten kiloparsecs whose centre is offset from the galaxy nucleus. In addition, the outer galaxy disk is warped, as seen at both optical and radio wavelengths. The halo contains numerous loops and ripples. Here we report the presence of a second, inner dust ring with projected dimensions of 1.5 x 1 kiloparsecs and offset by about half a kiloparsec from the centre of the galaxy (based upon an analysis of previously-obtained data). The two rings appear to be density waves propagating in the disk. Numerical simulations indicate that both rings result from a companion galaxy plunging through the centre of the disk of M31. The most likely interloper is M32. Head-on collisions between galaxies are rare, but it appears nonetheless that one took place 210 million years ago in our Local Group of galaxies.},
archivePrefix = {arXiv},
arxivId = {astro-ph/0610543},
author = {Block, D L and Bournaud, F and Combes, F and Groess, R and Barmby, P and Ashby, M. L.N. and Fazio, G G and Pahre, M A and Willner, S P},
doi = {10.1038/nature05184},
eprint = {0610543},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Block et al. - 2006 - An almost head-on collision as the origin of two off-centre rings in the Andromeda galaxy.pdf:pdf},
isbn = {1476-4687 (Electronic) 0028-0836 (Linking)},
issn = {14764687},
journal = {Nature},
number = {7113},
pages = {832--834},
pmid = {17051212},
primaryClass = {astro-ph},
title = {{An almost head-on collision as the origin of two off-centre rings in the Andromeda galaxy}},
url = {https://arxiv.org/pdf/astro-ph/0610543.pdf},
volume = {443},
year = {2006}
}
@techreport{Senoussi2018,
author = {Senoussi, Rachid},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Senoussi - Unknown - Des Mod{\`{e}}les Statistiques.pdf:pdf},
title = {{Des Mod{\`{e}}les Statistiques}},
url = {https://www.eccorev.fr/IMG/pdf/DesModelesStatistiques{\_}RSenoussi.pdf},
year = {2018}
}
@article{Sutter2005a,
abstract = {Leveraging the full power of multicore processors demands new$\backslash$ntools and new thinking from the software industry.$\backslash$nConcurrency has long been touted as the "next big thing" and "the$\backslash$nway of the future," but for the past 30 years, mainstream software$\backslash$ndevelopment has been able to ignore it. Our parallel future has$\backslash$nfinally arrived: new machines will be parallel machines, and this$\backslash$nwill require major changes in the way we develop software. The$\backslash$nintroductory article in this issue ("The Future of Microprocessors"$\backslash$nby Kunle Olukotun and Lance Hammond) describes the hardware$\backslash$nimperatives behind this shift in computer architecture from$\backslash$nuniprocessors to multicore processors, also known as CMPs (chip$\backslash$nmultiprocessors). (For related analysis, see "The Free Lunch Is$\backslash$nOver: A Fundamental Turn Toward Concurrency in Software.")},
author = {Sutter, Herb and Larus, James},
doi = {10.1145/1095408.1095421},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Sutter, Larus - 2005 - Software and the concurrency revolution.pdf:pdf},
issn = {15427730},
journal = {Queue},
number = {7},
pages = {54},
title = {{Software and the concurrency revolution}},
url = {http://delivery.acm.org/10.1145/1100000/1095421/p54-sutter.pdf?ip=130.190.104.85{\&}id=1095421{\&}acc=OPEN{\&}key=4D4702B0C3E38B35.4D4702B0C3E38B35.4D4702B0C3E38B35.6D218144511F3437{\&}{\_}{\_}acm{\_}{\_}=1527088941{\_}22208e17281fab8837c29c5a174d6e8a http://portal.acm.org/citation},
volume = {3},
year = {2005}
}
@phdthesis{Prestor2001,
abstract = {Scalable cache-coherent nonuniform memory access (ccNUMA) architectures are an important design segment for high-performance scalable multiprocessor systems. In order to write applica-tion programs that take advantage of such systems, or port application programs written for sym-metric multiprocessor systems with uniform memory access times, it is important to understand the impact of nonuniform memory access times and the associated ccNUMA cache coherence protocols on aggregate application memory performance. This work presents a detailed memory performance analysis of a particular ccNUMA system (the SGI Origin 2000). The thesis presents a new memory profiling tool, called snperf, and a new set of microbenchmark codes, called snbench, which make such a fine-grained memory performance analysis possible. The anal-ysis was performed on a wide variety of Origin 2000 system configurations and demonstrates that memory locality has a strong impact on application performance. More importantly, the re-sults demonstrate a variety of second-order memory performance effects that are also substantial performance influences. Even though the specific implementation target for this thesis was the Origin 2000 architecture, the methods are applicable to other ccNUMA systems.},
author = {Prestor, Uro{\v{s}}},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Prestor - 2001 - Evaluating the Memory Performance of a ccNUMA System.pdf:pdf},
title = {{Evaluating the Memory Performance of a ccNUMA System}},
url = {http://www.sgidepot.co.uk/origin/thesis.pdf},
year = {2001}
}
@article{Lin2015,
abstract = {Intel Initial Many-Core Instructions (IMCI) for Xeon Phi introduces hardware-implemented Gather and Scatter (G/S) load/store contents of SIMD registers from/to non-contiguous memory locations. However, they can be one of key performance bottlenecks for Xeon Phi. Modelling G/S can provide insights to the performance on Xeon Phi, however, the existing solution needs a hand-written assembly implementation. Therefore, we modeled G/S with hardware performance counters which can be profiled by the tools like PAPI. We profiled Address Generation Interlock (AGI) events as the number of G/S, estimated the average latency of G/S with VPU{\_}DATA{\_}READ, and combined them to model the total latencies of G/S. We applied our model to the 3D 7-point stencil and the result showed G/S spent nearly 40{\%} of total kernel time. We also validated the model by implementing a G/S- free version with intrinsics. The contribution of the work is a performance model for G/S built with hardware counters. We believe the model can be generally applicable to CPU as well.},
author = {Lin, James and Nukada, Akira and Matsuoka, Satoshi},
doi = {10.1109/CCGrid.2015.59},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Modeling Gather and Scatter with Hardware Performance Counters for Xeon Phi.pdf:pdf},
isbn = {9781479980062},
journal = {Proceedings - 2015 IEEE/ACM 15th International Symposium on Cluster, Cloud, and Grid Computing, CCGrid 2015},
keywords = {Gather and Scatter,Hardware performance counters,Performance modeling,Xeon Phi},
pages = {713--716},
title = {{Modeling gather and scatter with hardware performance counters for Xeon Phi}},
year = {2015}
}
@book{Jeannot2014,
abstract = {With recent changes in multicore and general-purpose computing on graphics processing units, the way parallel computers are used and programmed has drastically changed. It is important to provide a comprehensive study on how to use such machines written by specialists of the domain. The book provides recent research results in high-performance computing on complex environments, information on how to efficiently exploit heterogeneous and hierarchical architectures and distributed systems, detailed studies on the impact of applying heterogeneous computing practices to real problems, and applications varying from remote sensing to tomography. The content spans topics such as Numerical Analysis for Heterogeneous and Multicore Systems; Optimization of Communication for High Performance Heterogeneous and Hierarchical Platforms; Efficient Exploitation of Heterogeneous Architectures, Hybrid CPU+GPU, and Distributed Systems; Energy Awareness in High-Performance Computing; and Applications of Heterogeneous High-Performance Computing. • Covers cutting-edge research in HPC on complex environments, following an international collaboration of members of the ComplexHPC • Explains how to efficiently exploit heterogeneous and hierarchical architectures and distributed systems • Twenty-three chapters and over 100 illustrations cover domains such as numerical analysis, communication and storage, applications, GPUs and accelerators, and energy efficiency},
author = {Jeannot, Emmanuel. and Žilinskas, J},
isbn = {9781118712054},
keywords = {COMPUTERS / Programming / Parallel,High performance computing},
pages = {395},
title = {{High Performance Computing on Complex Environments}},
url = {http://ezproxy.eafit.edu.co/login?url=http://search.ebscohost.com/login.aspx?direct=true{\&}db=edsebk{\&}AN=756266{\&}lang=es{\&}site=eds-live},
year = {2014}
}
@book{Primet2010,
abstract = {R{\'{e}}seaux de calcul {\'{e}}tudie le coeur de trois nouvelles architectures informatiques distribu{\'{e}}es : les grappes, les grilles et les nuages de calcul. Il pr{\'{e}}sente leur syst{\`{e}}me nerveux central respectif constitu{\'{e}} d'un r{\'{e}}seau complexe d'interconnexion et de protocoles de communication sp{\'{e}}cifiques.},
author = {Primet, Pascale Vicat-Blanc. and Soudan, S{\'{e}}bastien and Guillier, Romaric and Goglin, Brice},
isbn = {9782746230064},
pages = {213},
publisher = {Hermès Science},
title = {{R{\'{e}}seaux de calcul : Des grappes aux nuages de calcul}},
url = {https://hal.inria.fr/inria-00533072/},
year = {2010}
}
@article{Reed2015a,
author = {Reed, Daniel A. and Dongarra, Jack},
doi = {10.1145/2699414},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Reed, Dongarra - 2015 - Exascale computing and big data.pdf:pdf},
issn = {00010782},
journal = {Communications of the ACM},
month = {jun},
number = {7},
pages = {56--68},
publisher = {ACM},
title = {{Exascale computing and big data}},
url = {http://dl.acm.org/citation.cfm?doid=2797100.2699414},
volume = {58},
year = {2015}
}
@incollection{Dongarra2016,
author = {Dongarra, Jack},
doi = {10.1007/978-3-319-42432-3_1},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Dongarra - 2016 - With Extreme Scale Computing the Rules Have Changed.pdf:pdf},
isbn = {978-1-4503-4314-5},
month = {jul},
pages = {3--6},
publisher = {Springer, Cham},
title = {{With Extreme Scale Computing the Rules Have Changed}},
url = {http://link.springer.com/10.1007/978-3-319-42432-3{\_}1},
year = {2016}
}
@book{Hwang2012,
abstract = {Distributed and Cloud Computing, named a 2012 Outstanding Academic Title by the American Library Association's Choice publication, explains how to create high-performance, scalable, reliable systems, exposing the design principles, architecture, and innovative applications of parallel, distributed, and cloud computing systems. Starting with an overview of modern distributed models, the book provides comprehensive coverage of distributed and cloud computing, including: Facilitating management, debugging, migration, and disaster recovery through virtualization Clustered systems for resear. Distributed system models and enabling technologies -- Computer clusters for scalable parallel computing -- Virtual machines and virtualization of clusters and data centers -- Cloud platform architecture over virtualized data centers -- Service-oriented architectures for distributed computing -- Cloud programming and software environments -- Grid computing systems and resource management -- Peer-to-peer computing and overlay networks -- Ubiquitous clouds and the Internet of things.},
author = {Hwang, Kai and Fox, Geoffrey C. and Dongarra, J. J.},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Distributed and cloud computing - from parallel processing to the Internet of things.pdf:pdf},
isbn = {9780123858801},
title = {{Distributed and Cloud Computing}},
url = {https://books.google.com/books?hl=fr{\&}lr={\&}id=IjgVAgAAQBAJ{\&}oi=fnd{\&}pg=PP1{\&}ots=9QSKg40G4B{\&}sig=IL5fDIQZXvGB5QCIK3Ji3OFmPVg{\#}v=onepage{\&}q{\&}f=false},
year = {2012}
}
@inproceedings{Munipala2016,
author = {Munipala, A WK. Umayanganie and Moore, Shirley V.},
booktitle = {2016 Fourth International Workshop on Software Engineering for High Performance Computing in Computational Science and Engineering (SE-HPCCSE)},
doi = {10.1109/SE-HPCCSE.2016.012},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Code Complexity versus Performance for GPU-accelerated Scientific Applications.pdf:pdf},
isbn = {978-1-5090-5224-0},
month = {nov},
pages = {50--50},
publisher = {IEEE},
title = {{Code Complexity versus Performance for GPU-accelerated Scientific Applications}},
url = {http://ieeexplore.ieee.org/document/7839472/},
year = {2016}
}
@article{Weaver2014,
abstract = {Ideal hardware performance counters provide exact deterministic re-sults. Real-world performance monitoring unit (PMU) implementations do not always live up to this ideal. Events that should be exact and de-terministic (such as retired instructions) show run-to-run variation and overcount on x86 64 machines, even when run in strictly controlled envi-ronments. These effects are non-intuitive to casual users and cause diffi-culties when strict determinism is desirable, such as when implementing deterministic replay or deterministic threading libraries. We investigate eleven different x86 64 CPU implementations and dis-cover the sources of divergence from expected count totals. Of all the counter events investigated, we find only a few that exhibit enough de-terminism to be used without adjustment in deterministic execution envi-ronments. We also briefly investigate ARM, IA64, POWER and SPARC systems and find that on these platforms the counter events have more determinism. We explore various methods of working around the limitations of the x86 64 events, but in many cases this is not possible and would require architectural redesign of the underlying PMU.},
author = {Weaver, Vincent M and Terpstra, Dan and Moore, Shirley},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Weaver, Terpstra, Moore - 2014 - Non-Determinism and Overcount on Modern Hardware Performance Counter Implementations – Extended.pdf:pdf},
title = {{Non-Determinism and Overcount on Modern Hardware Performance Counter Implementations – Extended}},
url = {https://pdfs.semanticscholar.org/340c/a872a54c4767c0e542dfa2ad63ccfb92c8d2.pdf},
year = {2014}
}
@incollection{Labarta2010,
author = {Labarta, Jesus and Terpstra, Daniel K. and Weaver, Vincent M.},
doi = {10.1201/b10509-6},
month = {nov},
pages = {87--122},
publisher = {CRC Press},
title = {{Trace-Based Tools}},
url = {http://www.crcnetbase.com/doi/abs/10.1201/b10509-6},
year = {2010}
}
@article{Weaver2017,
abstract = {Modern processors often have many processing cores in one package (or socket). Traditional hardware performance counters measure only values on a single core. A chip package has many resources which are package-wide and thus need a separate performance reporting mechanism. The values for these shared and off-core resources are reported as " offcore " , " uncore " or " northbridge " events.},
author = {Weaver, Vincent M},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Weaver - 2017 - System-wide Performance Counter Measurements Offcore, Uncore, and Northbridge Performance Events in Modern Processors.pdf:pdf},
title = {{System-wide Performance Counter Measurements: Offcore, Uncore, and Northbridge Performance Events in Modern Processors}},
url = {http://web.eece.maine.edu/{~}vweaver/projects/perf{\_}events/uncore/offcore{\_}uncore.pdf},
year = {2017}
}
@inproceedings{Lopez2015,
address = {New York, New York, USA},
annote = {lol
salut

michel
mdr
ol
rgr gr gr gr

feffe
fe
fe},
author = {Lopez, Ivonne and Moore, Shirley and Weaver, Vincent},
booktitle = {Proceedings of the 2015 XSEDE Conference on Scientific Advancements Enabled by Enhanced Cyberinfrastructure - XSEDE '15},
doi = {10.1145/2792745.2792772},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Lopez, Moore, Weaver - 2015 - A prototype sampling interface for PAPI.pdf:pdf},
isbn = {9781450337205},
keywords = {hardware counters,non-uniform memory access (NUMA),sampling},
pages = {1--4},
publisher = {ACM Press},
title = {{A prototype sampling interface for PAPI}},
url = {http://dl.acm.org/citation.cfm?doid=2792745.2792772},
year = {2015}
}
@article{Lorenzo2014,
abstract = {In this paper, a set of three hardware counter (HC)-based tools to characterise memory access of parallel codes in Symmetric Multiprocessors (SMPs) is presented. This toolkit simplifies accessing and programming HCs, which are included in modern microprocessors. Hardware counters are used to obtain information about memory accesses in a parallel code at very low cost. This information is presented to the user in a friendly way. The first tool can be used to automatically monitor the memory accesses of a system and to analyse a code even if the source is not available. The second tool allows the user to insert in a source code, in a simple and transparent way, the instructions needed to monitor and manage HCs. This way, specific parts of the code can be analysed. The user can either add appropriate directives to a C code or use a graphical interface to select those parts of the code to be analysed. The tool takes this source file and automatically adds the monitoring code. The third tool takes the information gathered by the aforementioned tools, processes it and displays it graphically. This tool shows the information in a comprehensive and simple way, allowing the user to adjust the level of detail. The aim of these tools was to characterise the memory accesses of parallel codes in multicore systems, in which the cache hierarchy can greatly influence the performance. For illustrative purposes, these tools were used to carry out two case studies, a sparse matrix vector product and a dot product. These studies have been made in two different environments. Anyway, they can be used in almost any system as long as the necessary HCs are available.Copyright {\textcopyright} 2013 John Wiley {\&} Sons, Ltd. Copyright {\textcopyright} 2013 John Wiley {\&} Sons, Ltd.},
author = {Lorenzo, Oscar G. and Pena, Tom{\'{a}}s F. and Cabaleiro, Jos{\'{e}} C. and Pichel, Juan C. and Lorenzo, Juan A. and Rivera, Francisco F.},
doi = {10.1002/cpe.3122},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/A hardware counter-based toolkit for the analysis of memory accesses in SMPs.pdf:pdf},
isbn = {9780769547015},
issn = {15320634},
journal = {Concurrency Computation Practice and Experience},
keywords = {hardware counters,irregular codes,memory hierarchy,monitoring,parallel codes},
title = {{A hardware counter-based toolkit for the analysis of memory accesses in SMPs}},
year = {2014}
}
@article{Weaver2010,
abstract = {Experiments involving hardware performance counters would ideally have deterministic results when run in strict ly controlled environments. In practice counters that should be deterministic (such as retired instructions) show variation from run to run on the x86 64 architecture. This causes difficulties when undertaking certain performance counter related tasks, such as simulator validation and performance analysis. These variations also impede software-based deterministic thread-interleaving, useful for debugging and tuning multi-threaded workloads on modern CMP systems. We investigate a variety of x86{\_}64 implementations (including DBI tools) and discover the sources of variations from expected count totals. The largest impact on retired instruction totals is due to the inclusion of hardware interrupt counts. This is difficult to compensate for, limiting the utility of the counters. In addition, counts generated by specific instructions can be counted differently across implementations, leading to cross-machine variations in aggregate counts. We briefly investigate ARM, IA64, POWER and SPARC systems and find that on these platforms the counts do not include hardware interrupts. Non-deterministic limitations to counter use may be a particular feature of the x86{\_}64 architecture. We also apply our methodology to larger programs and find that run-to-run variation can be minimized, but it is difficult to determine known “good” reference counts for comparison.},
author = {Weaver, V and Dongarra, Jack},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Can hardware performance counters produce expected, deterministic results.pdf:pdf},
journal = {Proceedings of Third Workshop on Functionality of Hardware Performance Monitoring},
title = {{Can hardware performance counters produce expected, deterministic results}},
year = {2010}
}
@inproceedings{Zamani2012,
abstract = {Power management and energy savings in high-performance computing has become an increasingly important design constraint. The foundation of many power/energy saving methods is based on power consumption models, which commonly rely on hardware performance monitoring counters (PMCs). Various events are provided by processor manufacturers to be monitored using PMCs. PMC event selection has been mainly based on architectural intuitions. However, efficient use of PMCs requires a carefully selected set of events. Therefore, a comprehensive study of PMC events with regards to power modeling is needed to understand and enhance such power models. In this paper, we study the relationship of PMC events with power consumption in the context of single-PMC and multi-PMC power models. Our OpenMP applications are from NAS Parallel Benchmark (BT, CG, LU, and SP) running on an AMD machine. We present the single-PMC selection results for each of our test applications, as well as a unified list for all four applications. Unlike other work that do not consider PMCs as each others' covariates, we present a method to select the most correlated set of PMC events for a given application. Our method finds the desired set of events with 6 times less number of executions compared to a principal component analysis (PCA) method. In addition, we have investigated variability of measurement for correlation coefficients. The 95{\%} confidence interval of power-PMC and PMC-PMC correlation coefficients falls within 1.6{\%} and 2.3{\%} of their measured values, respectively. Furthermore, we study the power and PMC trends in the context of time-series and show that power estimates can be enhanced more than common regression methods. We show that the ARMAX model, a time-series candidate for real-time power estimation, can estimate system power consumption with a mean absolute error (total signal) of 0.1-0.5{\%} in our applications. {\textcopyright} 2012 IEEE.},
author = {Zamani, Reza and Afsahi, Ahmad},
booktitle = {2012 International Green Computing Conference, IGCC 2012},
doi = {10.1109/IGCC.2012.6322289},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/A study of hardware performance monitoring counter selection in power modeling of computing systems.pdf:pdf},
isbn = {9781467321556},
keywords = {energy saving,performance monitoring counters,power modeling},
title = {{A study of hardware performance monitoring counter selection in power modeling of computing systems}},
year = {2012}
}
@inproceedings{Zaparanuks2009,
abstract = {Many experimental performance evaluations depend on accurate measurements of the cost of executing a piece of code. Often these measurements are conducted using infrastructures to access hardware performance counters. Most modern processors provide such counters to count micro-architectural events such as retired instructions or clock cycles. These counters can be difficult to configure, may not be programmable or readable from user-level code, and can not discriminate between events caused by different software threads. Various software infrastructures address this problem, providing access to per-thread counters from application code. This paper constitutes the first comparative study of the accuracy of three commonly used measurement infrastructures (perfctr, perfmon2, and PAPI) on three common processors (Pentium D, Core 2 Duo, and AMD ATHLON 64 X2).},
author = {Zaparanuks, Dmitrijs and Jovic, Milan and Hauswirth, Matthias},
booktitle = {ISPASS 2009 - International Symposium on Performance Analysis of Systems and Software},
doi = {10.1109/ISPASS.2009.4919635},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Accuracy of performance counter measurements.pdf:pdf},
isbn = {9781424441846},
title = {{Accuracy of performance counter measurements}},
year = {2009}
}
@inproceedings{Bare2010,
abstract = {Black-box instrumentation can support problem diagnosis in distributed systems without the need to modify the application code or to understand its semantics. We explore a novel, low-overhead black-box instrumentation source - CPU/hardware performance counters - for problem diagnosis. Based on our hypothesis that performance problems manifest as observable, anomalous changes in CPU performance counter-values collected across the nodes of a distributed system, we develop a diagnosis approach that is able to detect and localize performance problems injected into RUBiS, a three-tier e-commerce system.},
author = {Bare, Keith A. and Kavulya, Soila and Narasimhan, Priya},
booktitle = {Proceedings of the 2010 IEEE/IFIP Network Operations and Management Symposium, NOMS 2010},
doi = {10.1109/NOMS.2010.5488457},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Hardware Performance Counter-Based Problem Diagnosis for e-Commerce Systems.pdf:pdf},
isbn = {9781424453672},
title = {{Hardware performance counter-based problem diagnosis for e-commerce systems}},
year = {2010}
}
@inproceedings{Weaver2012,
author = {Weaver, Vincent M. and Johnson, Matt and Kasichayanula, Kiran and Ralph, James and Luszczek, Piotr and Terpstra, Dan and Moore, Shirley},
booktitle = {2012 41st International Conference on Parallel Processing Workshops},
doi = {10.1109/ICPPW.2012.39},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Measuring Energy and Power with PAPI.pdf:pdf},
isbn = {978-1-4673-2509-7},
issn = {15302016},
month = {sep},
pages = {262--268},
publisher = {IEEE},
title = {{Measuring Energy and Power with PAPI}},
url = {http://ieeexplore.ieee.org/document/6337489/},
year = {2012}
}
@article{Weaver2013a,
abstract = {Ideal hardware performance counters provide exact deterministic re-sults. Real-world performance monitoring unit (PMU) implementations do not always live up to this ideal. Events that should be exact and de-terministic (such as retired instructions) show run-to-run variation and overcount on x86 64 machines, even when run in strictly controlled envi-ronments. These effects are non-intuitive to casual users and cause diffi-culties when strict determinism is desirable, such as when implementing deterministic replay or deterministic threading libraries. We investigate eleven different x86 64 CPU implementations and dis-cover the sources of divergence from expected count totals. Of all the counter events investigated, we find only a few that exhibit enough de-terminism to be used without adjustment in deterministic execution envi-ronments. We also briefly investigate ARM, IA64, POWER and SPARC systems and find that on these platforms the counter events have more determinism. We explore various methods of working around the limitations of the x86 64 events, but in many cases this is not possible and would require architectural redesign of the underlying PMU.},
author = {Weaver, Vincent M. and Terpstra, Dan and Moore, Shirley},
doi = {10.1109/ISPASS.2013.6557172},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Weaver, Terpstra, Moore - Unknown - Non-Determinism and Overcount on Modern Hardware Performance Counter Implementations.pdf:pdf},
isbn = {9781467357777},
journal = {ISPASS 2013 - IEEE International Symposium on Performance Analysis of Systems and Software},
pages = {215--224},
title = {{Non-determinism and overcount on modern hardware performance counter implementations}},
year = {2013}
}
@article{Arulraj2013,
abstract = {Sequential and concurrency bugs are widespread in deployed software. They cause severe failures and huge financial loss during production runs. Tools that diagnose production-run failures with low overhead are needed. The state-of-the-art diagnosis techniques use software instrumentation to sample program properties at run time and use off-line statistical analysis to identify properties most correlated with failures. Although promising, these techniques suffer from high run-time overhead, which is sometimes over 100{\%}, for concurrency-bug failure diagnosis and hence are not suitable for production-run usage. We present PBI, a system that uses existing hardware performance counters to diagnose production-run failures caused by sequential and concurrency bugs with low overhead. PBI is designed based on several key observations. First, a few widely supported performance counter events can reflect a wide variety of common software bugs and can be monitored by hardware with almost no overhead. Second, the counter overflow interrupt supported by existing hardware and operating systems provides a natural and effective mechanism to conduct event sampling at user level. Third, the noise and non-determinism in interrupt delivery complements well with statistical processing. We evaluate PBI using 13 real-world concurrency and sequential bugs from representative open-source server, client, and utility programs, and 10 bugs from a widely used software-testing benchmark. Quantitatively, PBI can effectively diagnose failures caused by these bugs with a small overhead that is never higher than 10{\%}. Qualitatively, PBI does not require any change to software and presents a novel use of existing hardware performance counters.},
author = {Arulraj, Joy and Chang, Po-Chun and Jin, Guoliang and Lu, Shan},
doi = {10.1145/2499368.2451128},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Production-Run Software Failure Diagnosis via Hardware Performance Counters.pdf:pdf},
isbn = {9781450318709},
issn = {03621340},
journal = {ACM SIGPLAN Notices},
title = {{Production-run software failure diagnosis via hardware performance counters}},
year = {2013}
}
@inproceedings{Weaver2008,
abstract = {When creating architectural tools, it is essential to know whether the generated results make sense. Comparing a toolpsilas outputs against hardware performance counters on an actual machine is a common means of executing a quick sanity check. If the results do not match, this can indicate problems with the tool, unknown interactions with the benchmarks being investigated, or even unexpected behavior of the real hardware. To make future analyses of this type easier, we explore the behavior of the SPEC benchmarks with both dynamic binary instrumentation (DBI) tools and hardware counters. We collect retired instruction performance counter data from the full SPEC CPU 2000 and 2006 benchmark suites on nine different implementations of the times86 architecture. When run with no special preparation, hardware counters have a coefficient of variation of up to 1.07{\%}. After analyzing results in depth, we find that minor changes to the experimental setup reduce observed errors to less than 0.002{\%} for all benchmarks. The fact that subtle changes in how experiments are conducted can largely impact observed results is unexpected, and it is important that researchers using these counters be aware of the issues involved.},
author = {Weaver, Vincent M. and McKee, Sally A.},
booktitle = {2008 IEEE International Symposium on Workload Characterization, IISWC'08},
doi = {10.1109/IISWC.2008.4636099},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Can hardware performance counters produce expected, deterministic results.pdf:pdf},
isbn = {9781424427789},
title = {{Can hardware performance counters be trusted?}},
year = {2008}
}
@article{Abel2012,
author = {Abel, Andreas and Reineke, Jan},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/CacheModelingJRWRTC.pdf:pdf},
journal = {6th Junior Researcher Workshop on Real-Time Computing (in conjunction with RTNS)},
title = {{Automatic Cache Modeling by Measurements}},
year = {2012}
}
@article{Dongarra2004,
author = {Dongarra, Jack and Moore, Shirley and Mucci, Philip and Seymour, Keith},
doi = {http://springerlink.metapress.com/openurl.asp?genre=article&amp;issn=0302-9743&amp;volume=3038&amp;spage=432},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/cache-tlb-iccs2004.pdf:pdf},
isbn = {3-540-22116-6},
issn = {03029743},
journal = {Science-ICCS 2004},
pages = {432--439},
title = {{Accurate cache and TLB characterization using hardware counters}},
url = {http://www.springerlink.com/index/Q9MHWHDE3HQU2K3B.pdf},
year = {2004}
}
@article{Milenkovic2002,
abstract = {Improvement of branch predictors has been one of the focal points of computer architecture research during the last decade, ranging from two-level predictors to complex hybrid mechanisms. Most research efforts try to use real, already implemented, branch predictor sizes and organizations for comparison and evaluation. Yet, little is known about exact predictor implementation in Intel processors, apart from a few hints in the Intel manuals and valuable but unverified hacker efforts. Intel processors include performance monitoring counters that can count the events related to branches, and Intel provides a powerful VTune Performance Analyzer tool enabling easy access to performance counters. In this paper, we propose a series of experiments that explore the organization and size of a branch predictor, and use it to investigate Pentium III and Pentium 4 predictor implementations. Such knowledge could be used in further predictor research, as well as in the design of new, architecture-aware compilers.},
author = {Milenkovic, Milena and Milenkovic, Aleksandar and Kulick, Jeffrey},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Demystifying Intel Branch Predictors.pdf:pdf},
journal = {Proceedings of the 2002 Workshop on Duplicating, Deconstructing and Debunking (WDDD'02)},
number = {Figure 1},
title = {{Demystifying Intel branch predictors}},
year = {2002}
}
@article{Thomborson2000,
abstract = {We develop an analytic model, and a set of microbenchmark programs for the measurement of the structural parameters of data cache memories and data TLBs. Running under Linux, our microbenchmarks accurately measure data cache capacity, data cache line size, data cache associativity, effective cache latency, effective data path parallelism, data TLB size, data TLB associativity, and TLB latency. We present experimental results from running our microbenchmarks on Pentium II and Pentium III...},
author = {Thomborson, Clark and Yu, Yuanhua},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/cachespects.pdf:pdf},
keywords = {analysis,cache memory,computer performance,computer software,performance,translation},
pages = {383--390},
title = {{Measuring Data Cache and TLB Parameters Under Linux}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.36.1427},
year = {2000}
}
@article{John2007,
author = {John, Tobias and Baumgartl, Robert},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Exact Cache Characterization by Experimental Parameter Extraction.pdf:pdf},
journal = {Proceedings of the 15th International Conference on Real-Time and Network Systems RTNS07},
pages = {65--74},
title = {{Exact cache characterization by experimental parameter extraction}},
year = {2007}
}
@article{Ciesielski2016,
author = {Ciesielski, Frederic},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Fred - memory Dabndwidth exposed.pdf:pdf},
title = {{Memory Bandwidth Exposed}},
year = {2016}
}
@article{Moseley2011,
annote = {1{\`{e}}re lecture : synth{\`{e}}se

Objectif du papier, probl{\`{e}}me trait{\'{e}}
Le papier fait le tour des technologies HPM
Description soft des HPM dans le but d'influencer les architects des futures processeurs

Conclusion
Les HPM sont, sur le papiers, tr{\`{e}}s utiles et donnent des informations importantes
Mais ils ne sont pas facilement atteignable, complexe {\`{a}} utiliser et mal interpr{\'{e}}t{\'{e}} 
Mesur{\'{e}} des evenements simples est aussi compliqu{\'{e}} que des evnt. complexes
There is a disturbing lack of research describing best practices for sampling and multiplexing.

Informations trouv{\'{e}}es

Mesur{\'{e}} des evenements simples est aussi compliqu{\'{e}} que des evnt. complexes
Les utilisateurs sont toujours en demande de r{\'{e}}ponse
Les utilisateurs les utilisent pour des usages non pr{\'{e}}vu et cela continuera
D{\'{e}}finition des HPM: tous mechanismes hardware qui permet d'avoir des insight sur les performance du soft sur un microprocesseur (timer-based interrupt, instruction-based sampling, compteur d'evenements)
Performance tuning: c'est indentifier les hot spot qui sont representatifs du code et determiner par rapport a la peak performance s'ils sont satisfaisant ou non
Dans l'optimisation d'applciation on en vient toujours {\`{a}} un point ou il fut connaitre au cycle pret le comportement de la microarch. L'effort demand{\'{e}} est alors grand, c'est pour cela qu'on repousse cette {\'{e}}tape {\`{a}} la fin.},
author = {Moseley, Tipp and Vachharajani, Neil and Jalby, William},
doi = {10.1007/978-3-642-24403-2_23},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Hardware Performance Monitoring for the Rest of Us- A Position and Survey.pdf:pdf},
isbn = {9783642244025},
issn = {03029743},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {293--312},
title = {{Hardware performance monitoring for the rest of us: A position and survey}},
volume = {6985 LNCS},
year = {2011}
}
@article{Gepner2012,
abstract = {In this paper we will present a detailed study on tuning double-precision matrix-matrix multiplication (DGEMM) on the Intel Xeon E5-2680 CPU. We selected an optimal algorithm from the instruction set perspective as well software tools optimized for Intel Advance Vector Extensions (AVX). Our optimizations included the use of vector memory operations, and AVX instructions. Our proposed algorithm achieves a performance improvement of 33{\%} compared to the latest results achieved using the Intel Math Kernel Library DGEMM subroutine. {\textcopyright} 2012 Published by Elsevier Ltd.},
author = {Gepner, Pawel and Gamayunov, Victor and Fraser, David L.},
doi = {10.1016/j.procs.2012.04.014},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Effective Implementation of DGEMM on modern multicore CPU .pdf:pdf},
issn = {18770509},
journal = {Procedia Computer Science},
keywords = {AVX,HPC,Intel,Performance},
pages = {126--135},
publisher = {Elsevier Masson SAS},
title = {{Effective implementation of DGEMM on modern multicore CPU}},
url = {http://dx.doi.org/10.1016/j.procs.2012.04.014},
volume = {9},
year = {2012}
}
@book{Gough2015,
abstract = {The article reports on the Energy Star requirements for computer servers established by the U.S. Environmental Protection Agency (EPA) in Washington, D.C. It mentions that computer servers that meet the requirements of the Energy Star will be thirty percent more energy efficient compared to standard servers.},
author = {Gough, Corey and Steiner, Ian and Saunders, Winston},
booktitle = {Appliance Design},
doi = {10.1007/978-1-4302-6638-9},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Energy Efficient Servers - Blueprints for Data Center Optimization.pdf:pdf},
isbn = {978-1-4302-6637-2},
issn = {15525937},
keywords = {CLIENT/SERVER computing,COMPUTERS,ENERGY conservation,UNITED States,UNITED States. Environmental Protection Agency,WASHINGTON (D.C.)},
number = {7},
pages = {9},
title = {{Energy Efficient Servers}},
url = {http://search.ebscohost.com/login.aspx?direct=true{\&}db=bth{\&}AN=42634069{\&}site=ehost-live{\%}5Cnhttp://link.springer.com/10.1007/978-1-4302-6638-9},
volume = {57},
year = {2015}
}
@article{Diamond2011,
abstract = {The computation nodes of modern supercomputers commonly consist of multiple multicore processors. To maximize the performance of such systems requires measurement, analysis, and optimization techniques that specifically target multicore environments. This paper first examines traditional unicore metrics and demonstrates how they can be misleading in a multicore system. Second, it examines and characterizes performance bottlenecks specific to multicore-based systems. Third, it describes performance measurement challenges that arise in multicore systems and outlines methods for extracting sound measurements that lead to performance optimization opportunities. The measurement and analysis process is based on a case study of the HOMME atmospheric modeling benchmark code from NCAR running on supercomputers built upon AMD Barcelona and Intel Nehalem quad-core processors. Applying the multicore bottleneck analysis to HOMME led to multicore aware source-code optimizations that increased performance by up to 35{\%}. While the case studies were carried out on multichip nodes of supercomputers using an HPC application as the target for optimization, the pitfalls identified and the insights obtained should apply to any system that is composed of multicore processors.},
author = {Diamond, Jeff and Burtscher, Martin and McCalpin, John D. and Kim, Byoung Do and Keckler, Stephen W. and Browne, James C.},
doi = {10.1109/ISPASS.2011.5762713},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Evaluation and Optimization of Multicore Performance Bottlenecks in Supercomputing Applications.pdf:pdf},
isbn = {9781612843681},
journal = {ISPASS 2011 - IEEE International Symposium on Performance Analysis of Systems and Software},
number = {December 2016},
pages = {32--43},
title = {{Evaluation and optimization of multicore performance bottlenecks in supercomputing applications}},
year = {2011}
}
@article{Browne2000a,
abstract = {The purpose of the PAPI project is to specify a standard API for accessing hardware performance counters available on most modern microprocessors. These counters exist as a small set of registers that count "events", which are occurrences of specific signals and states related to the processor's function. Monitoring these events facilitates correlation between the structure of source/object code and the efficiency of the mapping of that code to the underlying architecture. This correlation has a variety of uses in performance analysis and tuning. The PAPI project has proposed a standard set of hardware events and a standard cross-platform library interface to the underlying counter hardware. The PAPI library has been or is in the process of being implemented on all major HPC platforms. The PAPI project is developing end-user tools for dynamically selecting and displaying hardware counter performance data. PAPI support is also being incorporated into a number of third-party tools.},
author = {Browne, S. and Dongarra, J. and Garner, N. and London, K. and Mucci, P.},
doi = {10.1109/SC.2000.10029},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/A Scalable Cross-Platform Infrastructure for Application Performance Tuning Using Hardware Counters.pdf:pdf},
isbn = {0-7803-9802-5},
issn = {1063-9535},
journal = {ACM/IEEE SC 2000 Conference (SC'00)},
title = {{A Scalable Cross-Platform Infrastructure for Application Performance Tuning Using Hardware Counters}},
year = {2000}
}
@article{Saavedra1996,
abstract = {Standard benchmarking provides to run-times for given programs on given machines, but fails to provide insight as to why those results were obtained (either in terms of machine or program characteristics) and fails to provide run-times for that program on some other machine, or some other programs on that machine. We have developed a machine-imdependent model of program execution to characterize both machine performance and program execution. By merging these machine and program characterizations, we can estimate execution time for arbitrary machine/program combinations. Our technique allows us to identify those operations, either on the machine or in the programs, which dominate the benchmark results. This information helps designers in improving the performance of future machines and users in tuning their applications to better utilize the performance of existing machines. Here we apply our methodology to characterize benchmarks and predict their execution times. We present extensive run-time statistics for a large set of benchmarks including the SPEC and Perfect Club suites. We show how these statistics can be used to identify important shortcoming in the programs. In addition, we give execution time estimates for a large sample of programs and machines and compare these against benchmark results. Finally, we develop a metric for program similarity that makes it possible to classify benchmarks with respect to a large set of characteristics.},
author = {Saavedra, Rafael H. and Smith, Alan J.},
doi = {10.1145/235543.235545},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Analysis of Benchmark Characteristics and Benchmark Performance Prediction Saavedra.pdf:pdf},
isbn = {0734-2071},
issn = {07342071},
journal = {ACM Transactions on Computer Systems},
number = {4},
pages = {344--384},
title = {{Analysis of benchmark characteristics and benchmark performance prediction}},
url = {http://portal.acm.org/citation.cfm?doid=235543.235545},
volume = {14},
year = {1996}
}
@article{Deng2013,
abstract = {The biannual TOP500 list of the highest performing supercomputers has chronicled, and even fostered, the development of recent supercomputing platforms. Coupled with the GREEN500 list that launched in November 2007, the TOP500 list has enabled analysis of multiple aspects of supercomputer design. In this comparative and retrospective study, we examine all of the available data contained in these two lists through November 2012 and propose a novel representation and analysis of the data, highlighting several major evolutionary trends. {\textcopyright} 2013 Elsevier Ltd.},
author = {Deng, Yuefan and Zhang, Peng and Marques, Carlos and Powell, Reid and Zhang, Li},
doi = {10.1016/j.parco.2013.04.007},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Analysis of Linpack and power efficiencies of the world's TOP500 supercomputers.pdf:pdf},
issn = {01678191},
journal = {Parallel Computing},
keywords = {Benchmark,Linpack,Power efficiency,Supercomputers},
number = {6-7},
pages = {271--279},
publisher = {Elsevier B.V.},
title = {{Analysis of Linpack and power efficiencies of the world's TOP500 supercomputers}},
url = {http://dx.doi.org/10.1016/j.parco.2013.04.007},
volume = {39},
year = {2013}
}
@misc{Zijlstra2009,
author = {Zijlstra, Peter and Mackerras, Paul and Richter, Robert and De, Arnaldo Carvalho and Galbraith, Mike and Gleixner, Thomas and Fengguang, Wu and Singh, Jaswinder and Wang, Yong and Weisbecker, Frederic and Lu, Yinghai and Henriques, Luis and Paris, Eric and Ven, Arjan Van De and Blechmann, Tim and Whitehouse, Steven and Singh, Jaswinder and Anvin, H Peter and Seto, Hidetoshi and Aktas, Erdem and Morton, Andrew and Molnar, Ingo},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Ingo Molnar- [Announce] Performance Counters for Linux, v8.pdf:pdf},
isbn = {0000000000},
pages = {1--11},
title = {{Email: Performance Counters for Linux}},
year = {2009}
}
@article{Spafford2012,
abstract = {We present a new approach to analytical performance modeling using Aspen, a domain specific langauge. Aspen (Abstract Scalable Performance Engineering Notation) fills an important gap in existing performance modeling techniques and is designed to enable rapid exploration of new algorithms and architectures. It includes a formal specification of an application's performance behavior and an abstract machine model. We provide an overview of Aspen's features and demonstrate how it can be used to express a performance model for a three dimensional Fast Fourier Transform. We then demonstrate the composability and modularity of Aspen by importing and reusing the FFT model in a molecular dynamics model. We have also created a number of tools that allow scientists to balance application and system factors quickly and accurately.},
author = {Spafford, Kyle L. and Vetter, Jeffrey S.},
doi = {10.1109/SC.2012.20},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Aspen- A Domain Specific Language for Performance Modeling.pdf:pdf},
isbn = {978-1-4673-0806-9},
issn = {2167-4329},
journal = {2012 International Conference for High Performance Computing, Networking, Storage and Analysis},
pages = {1--11},
title = {{Aspen: A domain specific language for performance modeling}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6468530},
year = {2012}
}
@article{Groves1995,
abstract = {future system designs can be expected to leverage both density and speed to achieve additional exploiting both technological density and speed. Since these basic trends are projected to continue, this, computer systems have been required to take approaches that improve performance by exceeds the rate of improvement in transistor speed and digital storage access speeds. To achieve Despite these underlying trends, the performance of computer systems has increased at a rate which technological approaches. projected to continue through the foreseeable future in the absence of fundamentally new data stored. These basic trends have been true throughout the history of computer systems and are important trend reflected is that capacity is improving at a faster rate than time taken to access the technologies: dynamic random access memories and magnetic disk storage. Here again, the Figures 2 and 3 show the trends in capacity and access time for two important digital storage important trend to notice is that capacity improves at a more rapid rate than transistor speed. shows the trend in increasing capacity and performance for digital switching technologies. The density and speed of digital switches and the density and access time of digital storage. Figure 1 The key hardware technologies that affect computer architectures are those that determine the General projected. technologies on computer system architectures past, present, and future will be explored and language, have also resulted in new capabilities and design points. The impact of these machine language to assembly language to high-level procedural language to object-oriented in software, which includes the transition of the predominant approach to programming from fundamentally different trade-offs in the architecture of computer systems. Additional advances electromechanical relays to vacuum tubes to transistors to integrated circuits has driven underlying trends and capabilities of hardware and software technologies. The transition from Computer system architecture has been, and always will be, significantly influenced by the Abstract IBM, Austin, Texas},
author = {Groves, R. D.},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Brief History of Computer Architecture Evolution and Future Trends.pdf:pdf},
pages = {147--159},
title = {{Brief History of Computer Architecture Evolution and Future Trends}},
year = {1995}
}
@article{Ilic2014,
abstract = {—The Roofline model graphically represents the attainable upper bound performance of a computer architecture. This paper analyzes the original Roofline model and proposes a novel approach to provide a more insightful performance modeling of modern architectures by introducing cache-awareness, thus significantly improving the guidelines for application optimization. The proposed model was experimentally verified for different architectures by taking advantage of built-in hardware counters with a curve fitness above 90{\%}.},
author = {Ilic, Aleksandar and Pratas, Frederico and Sousa, Leonel},
doi = {10.1109/L-CA.2013.6},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Cache-aware Roofline model- Upgrading the loft CARM.pdf:pdf},
issn = {15566056},
journal = {IEEE Computer Architecture Letters},
keywords = {C.0.d Modeling of computer architecture {\textless} C.0 G,C.0.e System architectures,C.4.d Modeling techniques {\textless} C.4 Performance of,C.4.g Measurement,evaluation,integration and modeling {\textless} C.0 General {\textless} C C,modeling,simulation of multiple-processor systems {\textless} C.4},
number = {1},
pages = {21--24},
title = {{Cache-aware roofline model: Upgrading the loft}},
volume = {13},
year = {2014}
}
@article{Treibig2012,
abstract = {Many tools and libraries employ hardware performance monitoring (HPM) on modern processors, and using this data for performance assessment and as a starting point for code optimizations is very popular. However, such data is only useful if it is interpreted with care, and if the right metrics are chosen for the right purpose. We demonstrate the sensible use of hardware performance counters in the context of a structured performance engineering approach for applications in computational science. Typical performance patterns and their respective metric signatures are defined, and some of them are illustrated using case studies. Although these generic concepts do not depend on specific tools or environments, we restrict ourselves to modern x86-based multicore processors and use the likwid-perfctr tool under the Linux OS.},
archivePrefix = {arXiv},
arxivId = {1206.3738},
author = {Treibig, Jan and Hager, Georg and Wellein, Gerhard},
doi = {10.1007/978-3-642-36949-0_50},
eprint = {1206.3738},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Best practices for HPM-assisted performance engineering on modern multicore processors.pdf:pdf},
isbn = {9783642369483},
issn = {03029743},
journal = {arXiv preprint arXiv:1206.3738},
pages = {10},
title = {{Best practices for HPM-assisted performance engineering on modern multicore processors}},
url = {http://arxiv.org/abs/1206.3738},
year = {2012}
}
@article{Bennett2014,
abstract = {Lattice Quantum ChromoDynamics (QCD), and by extension its parent field, Lattice Gauge Theory (LGT), make up a significant fraction of supercomputing cycles worldwide. As such, it would be irresponsible not to evaluate machines' suitability for such applications. To this end, a benchmark has been developed to assess the performance of LGT applications on modern HPC platforms. Distinct from previous QCD-based benchmarks, this allows probing the behaviour of a variety of theories, which allows varying the ratio of demands between on-node computations and inter-node communications. The results of testing this benchmark on various recent HPC platforms are presented, and directions for future development are discussed.},
archivePrefix = {arXiv},
arxivId = {1401.3733},
author = {Bennett, Ed and {Del Debbio}, Luigi and Jordan, Kirk and Lucini, Biagio and Patella, Agostino and Pica, Claudio and Rago, Antonio},
doi = {10.1109/HPCSim.2016.7568421},
eprint = {1401.3733},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/BSMBench- A Flexible and Scalable HPC Benchmark from Beyond the Standard Model Physics.pdf:pdf},
keywords = {and the numerical techniques,as gauge theories,benchmarking,beyond,can also,developed to study qcd,family of models known,qcd lies in a,quantum chromodynamics,the standard model,xeon phi},
title = {{BSMBench: a flexible and scalable supercomputer benchmark from computational particle physics}},
url = {http://arxiv.org/abs/1401.3733{\%}0Ahttp://dx.doi.org/10.1109/HPCSim.2016.7568421},
year = {2014}
}
@article{Putigny2015,
author = {Putigny, Bertrand and Putigny, Bertrand and Approaches, Benchmark-driven and Modeling, Performance},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Benchmark-driven approaches to performance modeling of multi-core architectures - These.pdf:pdf},
title = {{Benchmark-driven Approaches to Performance Modeling of Multi-Core Architectures}},
year = {2015}
}
@article{Gonzalez2009,
abstract = {Analyzing parallel programs has become increasingly difficult due to the immense amount of information collected on large systems. The use of clustering techniques has been proposed to analyze applications. However, while the objective of previous works is focused on identifying groups of processes with similar characteristics, we target a much finer granularity in the application behavior. In this paper, we present a tool that automatically characterizes the different computation regions between communication primitives in message-passing applications. This study shows how some of the clustering algorithms which may be applicable at a coarse grain are no longer adequate at this level. Density-based clustering algorithms applied to the performance counters offered by modern processors are more appropriate in this context. This tool automatically generates accurate displays of the structure of the application as well as detailed reports on a broad range of metrics for each individual region detected.},
author = {Gonzalez, Juan and Gimenez, Judit and Labarta, Jesus},
doi = {10.1109/IPDPS.2009.5161027},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Automatic Detection of Parallel Applications Computation Phases.pdf:pdf},
isbn = {9781424437504},
issn = {1530-2075},
journal = {IPDPS 2009 - Proceedings of the 2009 IEEE International Parallel and Distributed Processing Symposium},
title = {{Automatic Detection of Parallel Applications Computation Phases}},
year = {2009}
}
@article{Hammer2015,
abstract = {Analytic performance models are essential for understanding the performance characteristics of loop kernels, which consume a major part of CPU cycles in computational science. Starting from a validated performance model one can infer the relevant hardware bottlenecks and promising optimization opportunities. Unfortunately, analytic performance modeling is often tedious even for experienced developers since it requires in-depth knowledge about the hardware and how it interacts with the software. We present the "Kerncraft" tool, which eases the construction of analytic performance models for streaming kernels and stencil loop nests. Starting from the loop source code, the problem size, and a description of the underlying hardware, Kerncraft can ideally predict the single-core performance and scaling behavior of loops on multicore processors using the Roofline or the Execution-Cache-Memory (ECM) model. We describe the operating principles of Kerncraft with its capabilities and limitations, and we show how it may be used to quickly gain insights by accelerated analytic modeling.},
archivePrefix = {arXiv},
arxivId = {1509.03778},
author = {Hammer, Julian and Hager, Georg and Eitzinger, Jan and Wellein, Gerhard},
doi = {10.1145/2832087.2832092},
eprint = {1509.03778},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Automatic Loop Kernel Analysis and Performance Modeling With Kerncraft.pdf:pdf},
isbn = {9781450340090},
journal = {Proceedings of the 6th International Workshop on Performance Modeling, Benchmarking, and Simulation of High Performance Computing Systems (PMBS '15)},
pages = {1--11},
title = {{Automatic Loop Kernel Analysis and Performance Modeling with Kerncraft}},
url = {http://dl.acm.org/citation.cfm?id=2832087.2832092},
year = {2015}
}
@article{Yotov2005,
abstract = {On modern computers, the running time of many applications is dominated by the cost of memory operations. To optimize such applications for a given platform, it is necessary to have a detailed knowledge of the memory hierarchy parameters of that platform. In practice, this information is usually poorly documented if at all. Moreover, there is growing interest in self-tuning, autonomic software systems that can optimize themselves for different platforms, and these systems must determine memory hierarchy parameters automatically without human intervention. One solution is to use micro-benchmarks to determine the parameters of the memory hierarchy. In this paper, we argue that existing micro-benchmarks are inadequate, and present novel micro-benchmarks for determining the parameters of all levels of the memory hierarchy, including registers, all caches levels and the translation look-aside buffer. We have implemented these micro-benchmarks into an integrated tool that can be ported with little effort to new platforms. We present experimental results that show that this tool successfully determines memory hierarchy parameters on many current platforms, and compare its accuracy with that of existing tools.},
annote = {- plsu precis que lmbench et calibrator
- assure de bien mesurer un level de cache},
author = {Yotov, Kamen and Pingali, Keshav and Stodghill, Paul},
doi = {10.1145/1071690.1064233},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Automatic Measurement of Memory Hierarchy Parameters.pdf:pdf},
isbn = {1595930221},
issn = {01635999},
journal = {ACM SIGMETRICS Performance Evaluation Review},
number = {1},
pages = {181},
title = {{Automatic measurement of memory hierarchy parameters}},
url = {http://portal.acm.org/citation.cfm?doid=1071690.1064233},
volume = {33},
year = {2005}
}
@article{Rohl2015,
author = {R{\"{o}}hl, Thomas and Treibig, Jan and Hager, Georg and Wellein, Gerhard},
doi = {10.1109/ICPPW.2014.34},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Overhead analysis of performance counter measurements.pdf:pdf},
isbn = {9781479956159},
issn = {15302016},
journal = {Proceedings of the International Conference on Parallel Processing Workshops},
keywords = {Hardware Performance Counters,Overhead,Profiling,Tools,X86},
number = {September},
pages = {176--185},
title = {{Overhead Analysis of Performance Counter Measurements}},
volume = {2015-May},
year = {2015}
}
@article{Duchateau2008,
author = {Duchateau, Alexandre X and Sidelnik, Albert},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/P-Ray- A Suite of Micro-benchmarks for Multi-core Architectures.pdf:pdf},
title = {{P-Ray : A Suite of Micro-benchmarks for Multi-core Architectures}},
year = {2008}
}
@article{Eranian2006,
abstract = {Apis to read performance counter - perfmon2 read to see how it is implemented power5, intel,arm are supported QorIQ is not supported!!},
author = {Eranian, St{\'{e}}phane},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Perfmon2- a flexible performance monitoring interface for Linux.pdf:pdf},
journal = {Proc of the 2006 Ottawa Linux Symposium},
pages = {269--288},
title = {{Perfmon2: a flexible performance monitoring interface for Linux}},
url = {http://landley.net/kdocs/ols/2006/ols2006v1-pages-269-288.pdf},
year = {2006}
}
@article{Burtscher2010,
abstract = {HPC systems are notorious for operating at a small fraction of their peak performance, and the ongoing migration to multi-core and multi-socket compute nodes further complicates performance optimization. The readily available performance evaluation tools require considerable effort to learn and utilize. Hence, most HPC application writers do not use them. As remedy, we have developed PerfExpert, a tool that combines a simple user interface with a sophisticated analysis engine to detect probable core, socket, and node-level performance bottlenecks in each important procedure and loop of an application. For each bottle-neck, PerfExpert provides a concise performance assessment and suggests steps that can be taken by the programmer to improve performance. These steps include compiler switches and optimization strategies with code examples. We have applied PerfExpert to several HPC production codes on the Ranger supercomputer. In all cases, it correctly identified the critical code sections and provided accurate assessments of their performance.},
annote = {PerfExpertautomatically runs the same application multiple times. To beable to check the variability between runs, one counter is al-ways programmed to count cycles.
PerfExpert measures 15 different event types(Section II.A.1) to compute the overall LCPI and the LCPIcontribution of the six categories},
author = {Burtscher, Martin and Kim, Byoung Do and Diamond, Jeff and McCalpin, John and Koesterke, Lars and Browne, James},
doi = {10.1109/SC.2010.41},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/PerfExpert- An Easy-to-Use Performance Diagnosis Tool for HPC Applications .pdf:pdf},
isbn = {9781424475575},
journal = {2010 ACM/IEEE International Conference for High Performance Computing, Networking, Storage and Analysis, SC 2010},
keywords = {Bottleneck diagnosis,HPC systems,Multicore performance,Performance analysis,Performance metric},
title = {{PerfExpert: An easy-to-use performance diagnosis tool for HPC applications}},
year = {2010}
}
@phdthesis{Larysch2016,
author = {Larysch, Florian},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/ma{\_}2016{\_}larysch{\_}florian{\_}Memory{\_}Bandwidth{\_}Utilization.pdf:pdf},
number = {September 2015},
title = {{Fine-Grained Estimation of Memory Bandwidth Utilization}},
year = {2016}
}
@article{Browne2010,
author = {Browne, James C},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Making Sense of Performance Counter Measurements on Supercomputing Applications.pdf:pdf},
journal = {Memory},
number = {April 2016},
title = {{Making Sense of Performance Counter Measurements on Supercomputing Applications Intranode Strong Scaling , seconds to complete}},
year = {2010}
}
@article{Selva2017,
author = {Selva, Manuel and Morel, Lionel and Marquet, Kevin},
doi = {10.1109/SAMOS.2016.7818331},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/numap- A Portable Library For Low Level Memory Profiling.pdf:pdf},
isbn = {9781509030767},
journal = {Proceedings - 2016 16th International Conference on Embedded Computer Systems: Architectures, Modeling and Simulation, SAMOS 2016},
pages = {55--62},
title = {{Numap: A portable library for low-level memory profiling}},
year = {2017}
}
@article{WilliamsSamuelWebbandWatermanAndrewandPatterson2008,
abstract = {We propose an easy-to-understand, visual performance model that offers insights to programmers and architects on improving parallel software and hardware for floating point computations.},
author = {{Williams, Samuel Webb and Waterman, Andrew and Patterson}, David A. and Williams, Samuel Webb and Waterman, Andrew and Patterson, David A},
doi = {10.1145/1498765.1498785},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Roofline- An Insightful Visual Performance Model for Floating-Point Programs and Multicore Architectures.pdf:pdf},
isbn = {978-0-9825442-3-5},
issn = {00010782},
journal = {EECS Department, University of California, Berkeley Tech Reports: UCB/EECS-2008-134},
number = {4},
pages = {65----76},
title = {{Roofline: An Insightful Visual Performance Model for Floating-Point Programs and Multicore Architectures}},
url = {http://www.eecs.berkeley.edu/Pubs/TechRpts/2008/EECS-2008-134.html},
volume = {52},
year = {2008}
}
@phdthesis{Steinmann2012,
abstract = {In the beginning, CPU performance was mainly improved by increasing the core frequency. This approach is limited by several physical issues. These limits were reached about one decade ago. Since then performance improvements have been achieved at the cost of a strong increase in CPU complexity. This raises the dif- ficulty to understand their behavior, which is important for software performance optimization. In this context, insightful yet easy to understand performance models are of great value. The Roofline Model premises to fulfill these criteria. Unfortunately, to our best knowledge there is no tool available to measure the required quantities and generate Roofline Plots. We built such a tool, evaluated the quality of it's measurement results and generated Roofline Plots of various code samples. The tool is freely available and intended to be used by a wider audience.},
author = {Steinmann, Ruedi},
doi = {10.3929/ethz-a-007305123},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Roof line model.pdf:pdf},
isbn = {9781479936069},
title = {{Applying the Roofline Model}},
year = {2012}
}
@article{DaCosta2017,
abstract = {Using power meters and performance counters to get insight on system's behavior in terms of power consumption is common nowadays. The values coming from these external or internal meters are usually used directly by the research community, for instance to derive higher-level power models with learning techniques or to use them in decision tools such as schedulers in HPC and Cloud Computing. While it is reasonable when one wants only to have a broad view on the power consumption, they can not be used directly in most cases: We prove in this article that the problems of distributed measure and hardware limits are way more complex and create bias, and we give the keys to understand and chose the proper methodology to handle these bias to obtain relevant values for enhanced usage. A generic methodology is analyzed and its main lessons extracted for a direct usage by the research community to master system and power measures for servers in datacenter.},
author = {{Da Costa}, Georges and Pierson, Jean Marc and Fontoura-Cupertino, Leandro},
doi = {10.1016/j.suscom.2017.05.003},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Mastering system and power measures for servers in datacenter.pdf:pdf},
issn = {22105379},
journal = {Sustainable Computing: Informatics and Systems},
keywords = {Datacenter,Methodology,Performance counters,Power measure,System measure},
pages = {28--38},
publisher = {Elsevier Inc.},
title = {{Mastering system and power measures for servers in datacenter}},
url = {http://dx.doi.org/10.1016/j.suscom.2017.05.003},
volume = {15},
year = {2017}
}
@article{Bodin2015,
abstract = {La majorit{\'{e}} des applications HPC repose sur des technologies et pratiques logicielles invent{\'{e}}es il y a plusieurs d{\'{e}}cennies. Les consid{\'{e}}rations {\'{e}}nerg{\'{e}}tiques, la fin de la loi de Moore et la production d'{\'{e}}normes volumes de donn{\'{e}}es ont initi{\'{e}} une mutation profonde du paysage du HPC o{\`{u}} les {\'{e}}quilibres calcul et stockage sont remis en cause. Face {\`{a}} cette rupture technologique c'est l'ensemble de la pile logicielle (syst{\`{e}}mes, support d'ex{\'{e}}cutions, compilateurs, biblioth{\`{e}}ques) et les codes applicatifs qui doit {\'{e}}voluer.},
author = {Bodin, Franc{\c{c}}ois and M{\'{e}}haut, Jean-Fran{\c{c}}ois},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Programmation et Exploitation des Platesformes HPC- D´efis et challenges.pdf:pdf},
title = {{Programmation et Exploitation des Platesformes HPC : D{\'{e}}fis et challenges}},
url = {https://hal.archives-ouvertes.fr/hal-01174302},
year = {2015}
}
@article{Chandran2011,
author = {Chandran, Varadharajan},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Robust Method to Determine Cache and TLB Characteristics.pdf:pdf},
title = {{Robust Method to Determine Cache and TLB Characteristics}},
year = {2011}
}
@article{Tanica2014,
abstract = {{\textcopyright} Springer International Publishing Switzerland 2014.Accurate characterization of modern systems and applications requires run-time and simultaneous assessment of several executionrelated parameters. Although hardware monitoring facilities in modern multi-cores allow low-level profiling, it is not always easy to convert the acquired data into insightful information. For this, a low-overhead monitoring tool (SchedMon) is proposed herein, which relies on hardware facilities and interacts with the operating system scheduler to capture the run-time behavior of single and multi-threaded applications, even in presence of nested parallelism. By tracking the attainable performance, power and energy consumption of monitored applications, SchedMon also allows their insightful characterization with the Cache-aware Roofline model. In addition, the proposed tool provides application monitoring, either in their entirety or at the level of the function calls, without requiring any changes to the original source code. Experimental results show that SchedMon introduces negligible execution overheads, while capturing the interference of several co-scheduled SPEC2006 applications.},
author = {Tani{\c{c}}a, Lu{\'{i}}s and Ilic, Aleksandar and Tom{\'{a}}s, Pedro and Sousa, Leonel},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/SchedMon- A Performance and Energy Monitoring Tool for Modern Multi-cores CARM.pdf:pdf},
isbn = {978-3-319-14312-5},
issn = {16113349},
journal = {Euro-Par Workshops (2)},
keywords = {application character-,ization,power and performance counters,power and performance monitoring},
pages = {230--241},
title = {{SchedMon: A Performance and Energy Monitoring Tool for Modern Multi-cores.}},
url = {http://dblp.uni-trier.de/db/conf/europar/europar2014w2.html{\#}TanicaITS14},
volume = {8806},
year = {2014}
}
@article{Treibig2011,
abstract = {Exploiting the performance of today's microprocessors requires intimate knowledge of the microarchitecture as well as an awareness of the ever-growing complexity in thread and cache topology. LIKWID is a set of command line utilities that addresses four key problems: Probing the thread and cache topology of a shared-memory node, enforcing thread-core affinity on a program, measuring performance counter metrics, and microbenchmarking for reliable upper performance bounds. Moreover, it includes an mpirun wrapper allowing for portable thread-core affinity in MPI and hybrid MPI/threaded applications. To demonstrate the capabilities of the tool set we show the in uence of thread affinity on performance using the well-known OpenMP STREAM triad benchmark, use hardware counter tools to study the performance of a stencil code, and finally show how to detect bandwidth problems on ccNUMA-based compute nodes.},
archivePrefix = {arXiv},
arxivId = {1104.4874},
author = {Treibig, Jan and Hager, Georg and Wellein, Gerhard},
doi = {10.1007/978-3-642-24025-6},
eprint = {1104.4874},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Poster LIKWID lightweight performance tools.pdf:pdf},
isbn = {978-3-642-24024-9},
journal = {Competence in High Performance {\ldots}},
pages = {12},
title = {{LIKWID: Lightweight Performance Tools}},
url = {http://arxiv.org/abs/1104.4874},
year = {2011}
}
@article{Broquedis2010,
abstract = {The increasing numbers of cores, shared caches and memory nodes within machines introduces a complex hardware topology. High-performance computing applications now have to carefully adapt their placement and behavior according to the underlying hierarchy of hardware resources and their software affinities. We introduce the Hardware Locality (hwloc) software which gathers hardware information about processors, caches, memory nodes and more, and exposes it to applications and runtime systems in a abstracted and portable hierarchical manner. hwloc may significantly help performance by having runtime systems place their tasks or adapt their communication strategies depending on hardware affinities. We show that hwloc can already be used by popular high-performance OpenMP or MPI software. Indeed, scheduling OpenMP threads according to their affinities or placing MPI processes according to their communication patterns shows interesting performance improvement thanks to hwloc. An optimized MPI communication strategy may also be dynamically chosen according to the location of the communicating processes in the machine and its hardware characteristics.},
author = {Broquedis, Fran{\c{c}}ois and Clet-Ortega, J{\'{e}}r{\^{o}}me and Moreaud, St{\'{e}}phanie and Furmento, Nathalie and Goglin, Brice and Mercier, Guillaume and Thibault, Samuel and Namyst, Raymond},
doi = {10.1109/PDP.2010.67},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/hwloc- a Generic Framework for Managing Hardware Affinities in HPC Applications.pdf:pdf},
isbn = {9780769539393},
issn = {1066-6192},
journal = {Proceedings of the 18th Euromicro Conference on Parallel, Distributed and Network-Based Processing, PDP 2010},
pages = {180--186},
title = {{hwloc: A generic framework for managing hardware affinities in HPC applications}},
year = {2010}
}
@article{Marr2002,
abstract = {Intels Hyper-Threading Technology brings the concept of simultaneous multi-threading to the Intel Architecture. Hyper-Threading Technology makes a single physical processor appear as two logical processors; the physical execution resources are shared and the architecture state is duplicated for the two logical processors. From a software or architecture perspective, this means operating systems and user programs can schedule processes or threads to logical processors as they would on multiple physical processors. From a microarchitecture perspective, this means that instructions from both logical processors will persist and execute simultaneously on shared execution resources. This paper describes the Hyper-Threading Technology architecture, and discusses the microarchitecture details of Intel's first implementation on the Intel Xeon processor family. Hyper-Threading Technology is an important addition to Intels enterprise product line and will be integrated into a wide variety of products.},
author = {Marr, Deborah T and Binns, Frank and Hill, David L and Hinton, Glenn and Koufaty, David a and Miller, J Alan and Upton, Michael},
doi = {11.1535/itj.1101.08},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Hyper-Threading Technology Architecture and Microarchitecture .pdf:pdf},
isbn = {1535766X},
issn = {1535766X},
journal = {Intel Technology Journal},
number = {1},
pages = {1--12},
title = {{Hyper-Threading Technology Architecture and Microarchitecture}},
url = {http://download.intel.com/technology/itj/2002/volume06issue01/art01{\_}hyper/vol6iss1{\_}art01.pdf},
volume = {6},
year = {2002}
}
@article{Denoyelle2015,
abstract = {Nowadays, performance optimization involves careful data and task placement to deal with parallel application needs with respect to the underlying hardware topology. Monitoring the application behavior provides useful information that still needs to be matched with the actual placement, for instance to understand whether bottlenecks are caused by the sequential code itself or by shared resources in parallel programs. We propose an insightful monitoring tool based on two cornerstones of hardware performance counters monitoring and hardware locality modeling, respectively named PAPI and hwloc. It enables a dynamic visual analysis of parallel applications' phases at runtime, revealing their possibly variable and heterogeneous behaviors and needs. A purpose designed application shows that the topology-aware visual representation of hardware counters can help figuring out shared resource bottlenecks and ease the task placement decision process in runtime systems.},
author = {Denoyelle, Nicolas and Goglin, Brice and Jeannot, Emmanuel},
doi = {10.1007/978-3-319-27308-2_57},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/INRIA - A Topology-Aware Performance Monitoring Tool for Shared Resource Management in Multicore Systems.pdf:pdf},
isbn = {9783319273075},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
pages = {710--721},
title = {{A topology-aware performance monitoring tool for shared resource management in multicore systems}},
volume = {9523},
year = {2015}
}
@article{HPE2016,
author = {HPE},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/HPE race to exascale.pdf:pdf},
journal = {HP white paper},
title = {{Exascale : A race to the future of HPC}},
year = {2016}
}
@article{Weaver2013,
author = {Weaver, Vincent M},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Linux perf event Features and Overhead.pdf:pdf},
journal = {The 2nd International Workshop on Performance Analysis of Workload Optimized Systems, FastPath},
number = {April},
pages = {80},
title = {{Linux perf{\_}event Features and Overhead}},
year = {2013}
}
@article{Goglin2016,
abstract = {High-performance computing requires a deep knowledge of the hardware platform to fully exploit its computing power. The performance of data transfer between cores and memory is becoming critical. Therefore locality is a major area of optimization on the road to exascale. Indeed, tasks and data have to be carefully distributed on the computing and memory resources.$\backslash$nWe discuss the current way to expose processor and memory locality information in the Linux kernel and in user-space libraries such as the hwloc software project. The current de facto standard structural modeling of the platform as the tree is not perfect, but it offers a good compromise between precision and convenience for HPC runtimes.$\backslash$nWe present an in-depth study of the software view of the upcoming Intel Knights Landing processor. Its memory locality cannot be properly exposed to user-space applications without a significant rework of the current software stack. We propose an extension of the current hierarchical platform model in hwloc. It correctly exposes new heterogeneous architectures with high-bandwidth or non-volatile memories to applications, while still being convenient for affinity-aware HPC runtimes.},
author = {Goglin, Brice},
doi = {10.1145/2989081.2989115},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/hwloc Exposing the Locality of Heterogeneous Memory Architectures to HPC Applications.pdf:pdf},
isbn = {9781450343053},
journal = {Proceedings of the Second International Symposium on Memory Systems - MEMSYS '16},
keywords = {-heterogeneous memory,affinity,high-,linux,locality,nodes and caches with,performance computing,respect to,structural modeling,the locality of numa,user-space runtimes},
pages = {30--39},
title = {{Exposing the Locality of Heterogeneous Memory Architectures to HPC Applications}},
url = {http://dl.acm.org/citation.cfm?doid=2989081.2989115},
year = {2016}
}
@article{Treibig2010,
abstract = {Exploiting the performance of today's processors requires intimate knowledge of the microarchitecture as well as an awareness of the ever-growing complexity in thread and cache topology. LIKWID is a set of command-line utilities that addresses four key problems: Probing the thread and cache topology of a shared-memory node, enforcing thread-core affinity on a program, measuring performance counter metrics, and toggling hardware prefetchers. An API for using the performance counting features from user code is also included. We clearly state the differences to the widely used PAPI interface. To demonstrate the capabilities of the tool set we show the influence of thread pinning on performance using the well-known OpenMP STREAM triad benchmark, and use the affinity and hardware counter tools to study the performance of a stencil code specifically optimized to utilize shared caches on multicore chips.},
archivePrefix = {arXiv},
arxivId = {1004.4431},
author = {Treibig, Jan and Hager, Georg and Wellein, Gerhard},
doi = {10.1109/ICPPW.2010.38},
eprint = {1004.4431},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/LIKWID A Lightweight Performance-Oriented Tool Suite for x86 Multicore Environments.pdf:pdf},
isbn = {9780769541570},
issn = {15302016},
journal = {Proceedings of the International Conference on Parallel Processing Workshops},
number = {December 2016},
pages = {207--216},
title = {{LIKWID: A lightweight performance-oriented tool suite for x86 multicore environments}},
year = {2010}
}
@article{Staelin2004,
abstract = {Lmbench is a powerful and extensible suite of micro-benchmarks that measures a variety of important aspects of system performance. It has a powerful timing harness that manages most of the housekeeping chores associated with benchmarking, making it easy to create new benchmarks that analyze systems or components of specific interest to the user. In many ways lmbench is a Swiss army knife for performance analysis. It includes an extensive suite of micro-benchmarks that give powerful insights into system performance. For those aspects of system or application performance not covered by the suite, it is generally a simple task to create new benchmarks using the timing harness. lmbench is written in ANSI-C and uses POSIX interfaces, so it is portable across a wide variety of systems and architectures. It also includes powerful new tools that measure performance under scalable loads to analyze SMP and clustered system performance. Copyright 2005 John Wiley {\&} Sons, Ltd.},
author = {Staelin, Carl},
doi = {10.1002/spe.665},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/lmbench -- an extensible micro-benchmark suite.pdf:pdf},
issn = {00380644},
journal = {Software - Practice and Experience},
keywords = {Measurement,Micro-benchmarking,Performance analysis},
number = {11},
pages = {1079--1105},
title = {{Lmbench: An extensible micro-benchmark suite}},
url = {http://www.hpl.hp.com/techreports/2004/HPL-2004-213.pdf},
volume = {35},
year = {2005}
}
@article{Denoyelle2017,
author = {Denoyelle, Nicolas and Goglin, Brice and Ilic, Aleksandar and Jeannot, Emmanuel and Denoyelle, Nicolas and Goglin, Brice and Ilic, Aleksandar and Jeannot, Emmanuel and Sousa, Leonel and Large, Modeling and Denoyelle, Nicolas and Goglin, Brice and Ilic, Aleksandar and Jeannot, Emmanuel},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/INRIA Modeling Large Compute Nodes with Heterogeneous Memories with Cache-Aware Roofline Model.pdf:pdf},
isbn = {9783319729718},
title = {{Modeling Large Compute Nodes with Heterogeneous Memories with Cache-Aware Roofline Model}},
year = {2017}
}
@inproceedings{Denoyelle2018,
abstract = {In order to fulfill modern applications needs, computing sys-tems become more powerful, heterogeneous and complex. NUMA plat-forms and emerging high bandwidth memories offer new opportunities for performance improvements. However they also increase hardware and software complexity, thus making application performance analy-sis and optimization an even harder task. The Cache-Aware Roofline Model (CARM) is an insightful, yet simple model designed to address this issue. It provides feedback on potential applications bottlenecks and shows how far is the application performance from the achievable hard-ware upper-bounds. However, it does not encompass NUMA systems and next generation processors with heterogeneous memories. Yet, some application bottlenecks belong to those memory subsystems, and would benefit from the CARM insights. In this paper, we fill the missing require-ments to scope recent large shared memory systems with the CARM. We provide the methodology to instantiate, and validate the model on a NUMA system as well as on the latest Xeon Phi processor equiped with configurable hybrid memory. Finally, we show the model ability to exhibits several bottlenecks of such systems, which were not supported by CARM.},
author = {Denoyelle, Nicolas and Goglin, Brice and Ilic, Aleksandar and Jeannot, Emmanuel and Sousa, Leonel},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-72971-8_5},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/INRIA Modeling Large Compute Nodes with Heterogeneous Memories with Cache-Aware Roofline Model.pdf:pdf},
isbn = {9783319729701},
issn = {16113349},
pages = {91--113},
title = {{Modeling large compute nodes with heterogeneous memories with cache-aware roofline model}},
volume = {10724 LNCS},
year = {2018}
}
@article{Staelin2002,
abstract = {lmbench, benchmarking, distributed systems, parallel systems, multi-processor systems lmbench3 extends the lmbench2 system to measure a system's performance under scalable load to make it possible to assess parallel and distributed computer performance with the same power and flexibility that lmbench2 brought to uni-processor performance analysis. There is a new timing harness, benchmp, designed to measure performance at specific levels of parallel (simultaneous) load, and most existing benchmarks have been converted to use the new harness. lmbench is a micro-benchmark suite designed to focus attention on the basic building blocks of many common system applications, such as databases, simulations, software development, and networking. It is also designed to make it easy for users to create additional micro-benchmarks that can measure features, algorithms, or subsystems of particular interest to the user. ABSTRACT lmbench3 extends the lmbench2 system to measure a system'sp erformance under scal-able load to makei tp ossible to assess parallel and distributed computer performance with the same power and flexibility that lmbench2 brought to uni-processor performance analysis. There is a newtiming harness, benchmp,designed to measure performance at specific levels of parallel (simultaneous) load, and most existing benchmarks have been converted to use the new harness. lmbench is a micro-benchmark suite designed to focus attention on the basic building blocks of manyc ommon system applications, such as databases, simulations, software development, and networking. It is also designed to makei te asy for users to create additional micro-benchmarks that can measure features, algorithms, or subsystems of particular interest to the user.},
author = {Staelin, Carl and Israel, Hewlett-Packardlaboratories},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/lmbench3$\backslash$: measuring scalability.pdf:pdf},
title = {{Lmbench3: Measuring Scalability}},
year = {2002}
}
@book{Kukunas2015,
author = {Kukunas, Jim},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Jim Kukunas Power and Performance Software Analysis and Optimization.pdf:pdf},
isbn = {0128007265, 9780128007266},
title = {{Power and Performance: Software Analysis and Optimization}},
year = {2015}
}
@article{Luszczek2005,
abstract = {The HPC Challenge1 benchmark suite has been re- leased by the DARPA HPCS program to help define the performance boundaries of future Petascale computing systems. HPC Challenge is a suite of tests that exam- ine the performance of HPC architectures using kernels with memory access patterns more challenging than those of the High Performance Linpack (HPL) bench- mark used in the Top500 list. Thus, the suite is designed to augment the Top500 list, providing benchmarks that bound the performance of many real applications as a function of memory access characteristics e.g., spa- tial and temporal locality, and providing a framework for including additional tests. In particular, the suite is composed of several well known computational ker- nels (STREAM, HPL, matrix multiply – DGEMM, paral- lel matrix transpose – PTRANS, FFT, RandomAccess, and bandwidth/latency tests – b eff) that attempt to span high and lowspatial and temporal locality space. By de- sign, the HPC Challenge tests are scalable with the size of data sets being a function of the largest HPL matrix for the tested system.},
author = {Luszczek, Piotr and Dongarra, Jack J. and Koester, David and Rabenseifner, Rolf and Lucas, Bob and Kepner, Jeremy and McCalpin, John and Bailey, David and Takahashi, Daisuke},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Introduction to the HPC Challenge Benchmark Suite .pdf:pdf},
isbn = {ICL-UT-05-01},
journal = {Challenge},
number = {December},
pages = {13},
title = {{Introduction to the HPC challenge benchmark suite}},
url = {http://escholarship.org/uc/item/6sv079jp.pdf},
year = {2005}
}
@article{Cache1997,
author = {Cache, Instruction and Processors, Technology},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Using the RDTSC Instruction for Performance Monitoring.pdf:pdf},
journal = {Cycle},
pages = {1--12},
title = {{Using the RDTSC Instruction for Performance Monitoring CONTENTS :}},
year = {1997}
}
@article{Dongarra2001,
abstract = {PAPI is a specification of a cross-platform interface to hardware performance counters on modern microprocessors. These counters exist as a small set of registers that count events, which are occurrences of specific signals related to a processor's function. Monitoring these events has a variety of uses in application performance analysis and tuning. The PAPI specification consists of both a standard set of events deemed most relevant for application performance tuning, as well as both high-level and low-level sets of routines for accessing the counters. The high level interface simply provides the ability to start, stop, and read sets of events, and is intended for the acquisition of simple but accurate measurement by application engineers. The fully programmable low-level interface provides sophisticated options for controlling the counters, such as setting thresholds for interrupt on overflow, as well as access to all native counting modes and events, and is intended for third-party tool writers or users with more sophisticated needs. PAPI has been implemented on a number of platforms, including Linux/x86 and Linux/IA-64. The Linux/x86 implementation requires a kernel patch that provides a driver for the hardware counters. The driver memory maps the counter registers into user space and allows virtualizing the counters on a perprocess or per-thread basis. The kernel patch is being proposed for inclusion in the main Linux tree. The PAPI library provides access on Linux platforms not only to the standard set of events mentioned above but also to all the Linux/x86 and Linux/IA-64 native events. PAPI has been installed and is in use, either directly or through incorporation into third-party end-user performance analysis tools, on a number of Linux clusters, including the New Mexico LosLobos cluster and Linux clusters at NCSA and the University of Tennessee being used for the GrADS (Grid Application Development Software) project. 1},
author = {Dongarra, J and London, K and Moore, S},
doi = {10.1.1.130.2950},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Using PAPI for hardware performance monitoring on Linux systems.pdf:pdf},
journal = {Proc. Conf. on Linux {\ldots}},
pages = {25--27},
title = {{Using PAPI for hardware performance monitoring on Linux systems}},
url = {http://web.eecs.utk.edu/{~}shirley/papers/lci2001.pdf},
year = {2001}
}
@article{Drepper2007,
abstract = {Abstract As CPU cores become both faster and more numerous, the limiting factor for most programs is now, and will be for some time, memory access. Hardware designers have come up with ever more sophisticated memory handling and acceleration techniques– ...$\backslash$n},
author = {Drepper, U},
doi = {10.1.1.91.957},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/What Every Programmer Should Know About Memory.pdf:pdf},
issn = {0361526X},
journal = {Red Hat},
pages = {1--114},
pmid = {11807537},
title = {{What every programmer should know about memory}},
url = {http://diyhpl.us/{~}bryan/papers2/distributed/distributed-systems/what-every-programmer-should-know-about-memory.2007.pdf{\%}5Cnfile:///Files/AB/AB18CBAA-9B5C-4FAD-867F-04D554B39DF3.pdf{\%}5Cnfile:///Files/27/27A2BAA0-7DC4-47F6-94CF-3C51B27567C4.pdf},
year = {2007}
}
@article{Sutter2005b,
abstract = {The biggest sea change in software development since the OO revolution is knocking at the door, and its name is Concurrency.},
author = {Sutter, H},
doi = {10.1002/minf.201100042},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/The Free Lunch Is Over{\_} A Fundamental Turn Toward Concurrency in Software.pdf:pdf},
issn = {18681751},
journal = {Dr. Dobb's Journal},
pages = {1--9},
title = {{The free lunch is over: A fundamental turn toward concurrency in software}},
url = {http://www.mscs.mu.edu/{~}rge/cosc2200/homework-fall2013/Readings/FreeLunchIsOver.pdf},
year = {2005}
}
@article{London2001,
author = {London, Kevin and Moore, Shirley and Mucci, Philip and Seymour, Keith and Luczak, Richard},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/The PAPI Cross-Platform Interface to Hardware Performance Counters.pdf:pdf},
journal = {Department of Defense Users' Group Conference Proceedings},
pages = {18--21},
title = {{The PAPI Cross-Platform Interface to Hardware Performance Counters}},
year = {2001}
}
@article{Mahapatra1999a,
author = {Mahapatra, Nihar R. and Venkatrao, Balakrishna},
doi = {10.1145/357783.331677},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/The Processor-Memory bottleneck- Problems and Solutions.pdf:pdf},
issn = {1528-4972},
journal = {Crossroads},
number = {3es},
pages = {1--8},
title = {{The Processor-memory Bottleneck: Problems and Solutions}},
url = {http://doi.acm.org/10.1145/357783.331677{\%}5Cnhttp://dl.acm.org/ft{\_}gateway.cfm?id=331677{\&}type=html},
volume = {5},
year = {1999}
}
@article{Yotov2004,
abstract = {There is growing interest in autonomic, self-tuning software that $\backslash$ncan optimize itself on new platforms, without manual intervention. Optimization requires detailed knowledge of the target platform such as the latency and throughput of instructions, the numbers of registers, and the organization of the memory hierarchy. An autonomic optimization system needs to determine such platform-specific information on its own. In this paper, we describe the design and implementation of X-Ray, which is a tool that automatically measures a large number of such platform-specific parameters. For some of these parameters, we also describe novel algorithms, which are more robust than existing ones. X-Ray is written in C for maximum portability, and it is based on accurate timing of a number of carefully designed micro-benchmarks. A novel feature of X-Ray is that it is easily extensible because it provides simple infrastructure and a code generator that can be used to produce the large number of micro-benchmarks needed for such measurements. There are few existing tools that address this problem. Our experiments show that X-Ray produces more accurate and more complete results than any of them.},
author = {Yotov, Kamen and Pingali, Keshav and Stodghill, Paul},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/X-ray{\_}a{\_}tool{\_}for{\_}automatic{\_}measurement{\_}o.pdf:pdf},
journal = {Framework},
pages = {1--22},
title = {{X-Ray : Automatic Measurement of Hardware Parameters ∗}},
year = {2004}
}
@article{CEA2010,
author = {CEA},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Simulation au CEA.pdf:pdf},
number = {1},
pages = {1--9},
title = {{La simulation num{\'{e}}rique}},
year = {2010}
}
@phdthesis{Valat2016a,
abstract = {L'{\'{e}}volution des architectures des calculateurs actuels est telle que la m{\'{e}}moire devient un probl{\`{e}}me majeur pour les performances. L'{\'{e}}tude d{\'{e}}crite dans ce document montre qu'il est d{\'{e}}j{\`{a}} possible d'observer des pertes importantes imputables aux m{\'{e}}canismes de gestion de cette derni{\`{e}}re. Dans ce contexte, nous nous sommes int{\'{e}}ress{\'{e}}s aux probl{\`{e}}mes de gestion des gros segments m{\'{e}}moire sur les supercalculateurs multicoeurs NUMA de type Tera 100 et Curie. Notre travail est d{\'{e}}taill{\'{e}} ici en suivant trois axes principaux. Nous analysons dans un premier temps les politiques de pagination de diff{\'{e}}rents syst{\`{e}}mes d'exploitation (coloration de pages, grosses pages...). Nous mettons ainsi en {\'{e}}vidence l'existence d'interf{\'{e}}rences n{\'{e}}fastes entre ces politiques et les d{\'{e}}cisions de placement de l'allocateur en espace utilisateur. Nous compl{\'{e}}tons donc les {\'{e}}tudes cache/allocateur et cache/pagination par une analyse de l'interaction cumul{\'{e}}e de ces composants. Nous abordons ensuite la probl{\'{e}}matique des performances d'allocation des grands segments m{\'{e}}moire en consid{\'{e}}rant les {\'{e}}changes entre le syst{\`{e}}me et l'allocateur. Nous montrons ici qu'il est possible d'obtenir des gains significatifs (de l'ordre de 50{\%} sur une grosse application) en limitant ces {\'{e}}changes et en structurant l'allocateur pour un support explicite des architectures NUMA. La description de nos travaux s'ach{\`{e}}ve sur une {\'{e}}tude des probl{\`{e}}mes d'extensibilit{\'{e}} observ{\'{e}}s au niveau des fautes de pages du noyau Linux. Nous avons ainsi propos{\'{e}} une extension de la s{\'{e}}mantique d'allocation afin d'{\'{e}}liminer la n{\'{e}}cessit{\'{e}} d'effectuer les co{\^{u}}teux effacements m{\'{e}}moire des pages au niveau syst{\`{e}}me.},
author = {Valat, S{\'{e}}bastien},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Valat - 2014 - Contribution {\`{a}} l'am{\'{e}}lioration des m{\'{e}}thodes d'optimisation de la gestion de la m{\'{e}}moire dans le cadre du Calcul Haute P.pdf:pdf},
title = {{Contribution {\`{a}} l'am{\'{e}}lioration des m{\'{e}}thodes d'optimisation de la gestion de la m{\'{e}}moire dans le cadre du Calcul Haute Performance}},
year = {2016}
}
@article{Patil2015,
author = {Patil, Dipak and Kharat, Prashant and Gupta, Anil Kumar},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/STUDY OF PERFORMANCE COUNTERS AND PROFILING TOOLS TO MONITOR PERFORMANCE OF APPLICATION.pdf:pdf},
isbn = {9789382702757},
journal = {Proceedings of 21st IRF International Conference.},
keywords = {- pmu,msr,performance event,pmc},
number = {March},
pages = {45--49},
title = {{Study of Performance Counters and Profiling Tools}},
year = {2015}
}
@phdthesis{Sterz2016,
abstract = {ware systems. These models can be embedded in operating systems and execution en- vironments to optimize execution at run time. Adapting such models to newhardware architectures requires them to reflect more and more of the paradigms available to modern parallel computing systems. This thesis provides an overview of parallel cost models and describes that with the advent of non-uniform memory access architectures (NUMA), new models have been developed and the priorly existing ones need to be refined. Therefore, the existing NUMA models are analyzed, and a two-step strategy is proposed that incorporates low-level hardware counters as performance indicators. This thesis further focuses on these low-overhead hardware counters, which are avail- able to all modern CPUs. For four major CPU vendors—ARM, AMD, Intel, and IBM— hardware counter specifics are presented and explained in detail. Four tools are devel- oped, all accumulating and enriching specific counter information, to explore, measure, and visualize these low-overhead performance indicators. EvSel allows for measuring the whole plenitude of available counters. Two},
author = {Sterz, Christoph},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Thesis{\_}Analyzing NUMA Performance Based on Hardware Event Counters.pdf:pdf},
title = {{Analyzing NUMA Performance Based on Hardware Event Counters}},
year = {2016}
}
@article{Weaver2015,
abstract = {Most modern CPUs include hardware performance counters: architectural registers that allow programmers to gain low-level insight into system performance. Low-overhead access to these counters is necessary for accurate performance analysis, making the operating system interface critical to providing lowlatency performance data. We investigate the overhead of selfmonitoring performance counter measurements on the Linux perf event interface. We find that default code (such as that used by PAPI) implementing the perf event self-monitoring interface can have large overhead: up to an order of magnitude larger than the previously used perfctr and perfmon2 performance counter implementations. We investigate the causes of this overhead and find that with proper coding this overhead can be greatly reduced on recent Linux kernels.},
annote = {Self-monitoring Overhead of the Linux perf event
Contenu 
On regarde que linux car 97{\%} du top500 l'a 
2009: premier support dans linux des HC —{\textgreater} 2.6.31 perf{\_}event subsystem
3 fa{\c{c}}on d'utiliser les performances counters
Aggreagate measurements:
On active les compteurs avant le code et on r{\'{e}}colte les valeurs a la fin
facile et peu d'overhead
L'OS s'occupe des changement de contexte
Mais pas de d{\'{e}}tail par fonction.
Statistique
Gather results at regular interval via timer or overflow
Enregistre le pointeur d'instruction
Le r{\'{e}}sultats peut {\^{e}}tre d{\'{e}}duits de fa{\c{c}}on statistique
Pas besoin de modifier le code pour appeler des routines
D{\'{e}}tail par fonction —{\textgreater} Mais valeurs pas exactes —{\textgreater} sauf si fr{\'{e}}quences tr{\`{e}}s {\'{e}}lev{\'{e}} —{\textgreater} alors trop d'overhead
Self-monitoring
Instrumenter les parties du codes pour avoir le d{\'{e}}tail par fonction sans overhead
Appel a des routines en modifiant le code ou by linking some library to intercept the calls
PAPI
Est pass{\'{e}} de perfctr+perfmon2 {\`{a}} perf{\_}event —{\textgreater} overhead au d{\'{e}}but
Linux
Apporte une interface qui permet de choisir les {\'{e}}v{\'{e}}nements, start/stop, lire, passer les overflow information to the user, per thread counting, professa tachina (pour un process qui existe d{\'{e}}j{\`{a}})
Plusieurs essais avant d'utiliser perf{\_}Event:
OProfile en 2002
Interface dans un pseudo FS /dev/oprofile
System wide only, requires starting a daemon as root
Does not support self monitoring
Perfctr en 1999
Opending a /dev/perfctr device and accessing it with ioctl() calls
Libperfctr abstract the kernel interface
Use rdpmc pour lire —{\textgreater} reduction overhead 
Perfmon2
System call via /sys pseudo FS
Post perf{\_}Event
Likwid
Bypass le noyeau et accede directement les hardware conteurs
Peut entrer en conflit avec perf{\_}event


Perf{\_}event
Cr{\'{e}}e en 2009 en r{\'{e}}ponse a PErfmon2
Maintenant qu'il est dans le kernel c'est un gros frein pour la concurrence qui sera prouver une grosse plus value pour que les utilisateurs veuillent les utiliser
Philosophie: proposer le plus de fonctionnalit{\'{e}} et d'abstraction dans le kernel pour l'utilisateur
Interface autour d'un file descriptor ouvert avec l'appel system perf{\_}event{\_}open ()
En lui passant une structure complexe de plus de 40 champs
Counter activ{\'{e}} ou d{\'{e}}sactiv{\'{e}} avec ioctl et prctrl
On lit les counters avec read ()},
author = {Weaver, Vincent M.},
doi = {10.1109/ISPASS.2015.7095789},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Self-monitoring Overhead of the Linux perf event Performance Counter Interface.pdf:pdf},
isbn = {9781479919567},
journal = {ISPASS 2015 - IEEE International Symposium on Performance Analysis of Systems and Software},
pages = {102--111},
title = {{Self-monitoring overhead of the Linux perf- event performance counter interface}},
year = {2015}
}
@article{Wonnacott2000,
abstract = {Time skewing is a compile-time optimization that can provide$\backslash$narbitrarily high cache hit rates for a class of iterative calculations,$\backslash$ngiven a sufficient number of time steps and sufficient cache memory.$\backslash$nThus, it can eliminate processor idle time caused by inadequate main$\backslash$nmemory bandwidth. In this article, we give a generalization of time$\backslash$nskewing for multiprocessor architectures, and discuss time skewing for$\backslash$nmultilevel caches. Our generalization for multiprocessors lets us$\backslash$neliminate processor idle time caused by any combination of inadequate$\backslash$nmain memory bandwidth, limited network bandwidth, and high network$\backslash$nlatency, given a sufficiently large problem and sufficient cache. As in$\backslash$nthe uniprocessor case, the cache requirement grows with the machine$\backslash$nbalance rather than the problem size. Our techniques for using$\backslash$nmultilevel caches reduce the LI cache requirement, which would otherwise$\backslash$nbe unacceptably high for some architectures when using arrays of high$\backslash$ndimension},
author = {Wonnacott, D.},
doi = {10.1109/ipdps.2000.845979},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Time skewing.pdf:pdf},
isbn = {0-7695-0574-0},
issn = {1530-2075},
journal = {Proceedings 14th International Parallel and Distributed Processing Symposium. IPDPS 2000},
pages = {171--180},
title = {{Using time skewing to eliminate idle time due to memory bandwidth and network limitations}},
url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=845979},
year = {2002}
}
@phdthesis{Farjallah2015,
author = {Farjallah, Asma},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/THESE - Preparing depth imaging applications for Exascale challenges and impacts .pdf:pdf},
keywords = {-seismic applications,exascale,per-,rtm,stencil},
number = {4},
pages = {2--3},
title = {{Preparing depth imaging applications for the Exascale}},
year = {2015}
}
@phdthesis{Besnard2015,
author = {Besnard, Jean-baptiste and France, F- Arpajon},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/THESE - Profiling and debugging by efficient tracing of hybrid multi-threaded HPC applications.pdf:pdf},
title = {{Profiling and debugging by efficient tracing of hybrid multi-threaded HPC applications}},
year = {2015}
}
@phdthesis{Glesser2016,
abstract = {The field of High Performance Computing (HPC) is characterized by the contin-uous evolution of computing architectures, the proliferation of computing resourcesand the increasing complexity of applications users wish to solve. One of the mostimportant software of the HPC stack is the Resource and Job Management System(RJMS) which stands between the user workloads and the platform, the applica-tions and the resources. This specialized software provides functions for building,submitting, scheduling and monitoring jobs in a dynamic and complex computingenvironment.In order to reach exaflops HPC systems, new constraints and objectives havebeen introduced. This thesis develops and tests the idea that the users of suchsystems can help reaching the exaflopic scale. Specifically, we show and introducenew techniques that employ users behaviors to improve energy consumption andoverall cluster performances.To test the proposed techniques, we need to develop new tools and method-ologies that scale up to large HPC clusters. Thus, we designed adequate tools thatassess new RJMS scheduling algorithms of such large systems. These tools areable to run on small clusters by emulating or simulating bigger platforms. Afterevaluating different techniques to measure the energy consumption of HPC clusters,we propose a new heuristic, based on the popular Easy Backfilling algorithm, inorder to control the power consumption of such huge systems. We also demonstrate,using the same idea, how to control the energy consumption during a time period.The proposed mechanism is able to limit the energy consumption while keepingsatisfying performances. If energy is a limited resource, it has to be shared fairly.We also present a mechanism which shares energy consumption among users. Weargue that sharing fairly the energy among users should motivate them to reducethe energy consumption of their applications. Finally, we analyze past and presentbehaviors of users using learning algorithms in order to improve the performancesof the parallel platforms. This approach does not only outperform state of the artmethods, it also shows promising insight on how such method can improve otheraspects of RJMS.},
author = {Glesser, David},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/These - Road to exascale - improving scheduling performances.pdf:pdf},
keywords = {Calcul haute performance,Energy,Exascale,Hpc,Ordonnancement,Resources,Ressources,Scheduling,{\'{E}}nergie},
title = {{Road to exascale : improving scheduling performances and reducing energy consumption with the help of end-users}},
url = {https://tel.archives-ouvertes.fr/tel-01679734/},
year = {2016}
}
@article{Rohou2012,
abstract = {Hardware performance monitoring counters have recently received a lot of attention. They have been used by diverse communities to understand and improve the quality of computing systems: for example, architects use them to extract application characteristics and propose new hardware mechanisms, compiler writers study how generated code behaves on particular hardware, software developers identify critical regions of their applications and evaluate design choices to select the best performing implementation. In this paper, we propose that counters be used by all categories of users, in particular non-experts, and we advocate that a few simple metrics derived from these counters are relevant and useful. For example, a low IPC (number of executed instructions per cycle) indicates that the hardware is not performing at its best, a high cache miss ratio can suggest several causes, such as conflicts between processes in a multicore environment. We also introduce a new simple and flexible user-level tool that collects these data on Linux platforms, and we illustrate its practical benefits through several use cases.},
author = {Rohou, Erven},
doi = {10.1109/ICPPW.2012.58},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Tiptop- Hardware Performance Counters for the Masses.pdf:pdf},
isbn = {9780769547954},
issn = {15302016},
journal = {Proceedings of the International Conference on Parallel Processing Workshops},
keywords = {PMU,hardware counters,performance,tool},
pages = {404--413},
title = {{Tiptop: Hardware performance counters for the masses}},
year = {2012}
}
@article{Yasin2014,
abstract = {Optimizing an application's performance for a given microarchitecture has become painfully difficult. Increasing microarchitecture complexity, workload diversity, and the unmanageable volume of data produced by performance tools increase the optimization challenges. At the same time resource and time constraints get tougher with recently emerged segments. This further calls for accurate and prompt analysis methods. In this paper a Top-Down Analysis is developed – a practical method to quickly identify true bottlenecks in out-of-order processors. The developed method uses designated performance counters in a structured hierarchical approach to quickly and, more importantly, correctly identify dominant performance bottlenecks. The developed method is adopted by multiple in-production tools including VTune. Feedback from VTune average users suggests that the analysis is made easier thanks to the simplified hierarchy which avoids the high-learning curve associated with microarchitecture details. Characterization results of this method are reported for the SPEC CPU2006 benchmarks as well as key enterprise workloads. Field case studies where the method guides software optimization are included, in addition to architectural exploration study for most recent generations of Intel Core™ products. The insights from this method guide a proposal for a novel performance counters architecture that can determine the true bottlenecks of a general out-of-order processor. Unlike other approaches, our analysis method is low-cost and already featured in in-production systems – it requires just eight simple new performance events to be added to a traditional PMU. It is comprehensive – no restriction to predefined set of performance issues. It accounts for granular bottlenecks in super-scalar cores, missed by earlier approaches.},
author = {Yasin, Ahmad},
doi = {10.1109/ISPASS.2014.6844459},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/TopDown-Yasin-ISPASS14.pdf:pdf},
isbn = {9781479936052},
journal = {ISPASS 2014 - IEEE International Symposium on Performance Analysis of Systems and Software},
pages = {35--44},
title = {{A Top-Down method for performance analysis and counters architecture}},
year = {2014}
}
@incollection{Antao2014,
author = {Ant{\~{a}}o, Diogo and Tani{\c{c}}a, Lu{\'{i}}s and Ilic, Aleksandar and Pratas, Frederico and Tom{\'{a}}s, Pedro and Sousa, Leonel},
doi = {10.1007/978-3-642-55224-3_70},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/CARM Monitoring Performance and Power for Application Characterization with the Cache-aware Roofline Model.pdf:pdf},
keywords = {application character-,ization,power and performance counters,power and performance monitoring},
pages = {747--760},
title = {{Monitoring Performance and Power for Application Characterization with the Cache-Aware Roofline Model}},
url = {http://link.springer.com/10.1007/978-3-642-55224-3{\_}70},
year = {2014}
}
@article{Hagersten2016,
author = {Hagersten, Erik},
file = {:Users/jean/Google Drive/ENS HPE - These/{\_}BIBLIO{\_}/Papiers, th{\`{e}}se/Cours{\_}Memory{\_}and{\_}optimization-1.pdf:pdf},
number = {1},
pages = {1},
title = {{Memory Technology}},
year = {2016}
}
