\section{Conlusion}\label{sec:conclusion-hpc}





\textbf{TODO}
 
 - L'interconnexion de toutes ces ressources est réalisée dans le seul but de réduire le temps nécessaire à la résolution d'un problème. En effet, pour une expérience donnée si l'on possède un cluster de 1000 machines on ne voudra pas réaliser l'expérience un millier de fois mais plutôt réduire le temps d'une expérience par un facteur 1000 pour ensuite analyser les résultats, changer les paramètres et pouvoir lancer une nouvelle expérimentation. La calcul parallèle est un ensemble de moyens, logiciel et matériel qui permettent de réaliser des instructions simultanément. L'idée principale du calcul parallèle est de réduire le temps de calcul d'un programme en divisant le travail à réaliser, le partager en sous-problèmes qui peuvent être résolu de façon indépendante par plusieurs ressources de calcul, comme des processeurs. Un exemple concret de résolution d'un problème grâce à la programmation parallèle est donnée dans la section \ref{sub_reso_partage}

- Ces innovations se sont accumulées rendant les architectures très complexes et offrant de nouveaux défis comme la compréhension fine de leur comportement et leur optimisation. Ainsi ces innovations du matériel ont un impact direct sur les applications. Ces dernières doivent être adaptées pour pouvoir profiter de ces innovations comme l’expliquait l’article The Free Lunch is Over en 2005 [12].
  
  
- on comptait sur l'évolution des processeurs, peu de travail d'optimisation
      

- L'apparition des caches à permis d'augmenter la performance relative des applications en camouflant l'augmentation faible de la bande passante. Cela à contribué à l'augmentation de la complexité des architectures et il faut désormais au programmeur des connaissances solides pour aller tirer le maximum de performances de ces processeurs

