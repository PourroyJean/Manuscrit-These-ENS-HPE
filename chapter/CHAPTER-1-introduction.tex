\chapter{Introduction Générale} \label{chap:intro}
        
Le \gls{HPC} est le domaine  consistant à regrouper des milliers de ressources de calculs (processeurs, mémoire, stockage, réseau) pour exécuter des applications de simulations numériques complexes ou permettre le traitement massif de données. Ces simulations contribuent au développement de nouveaux produits (voitures, avions), à l'extraction d'énergies (recherche pétrolière, conception d'éolienne). Le HPC est un outil inhabituel, car il ne se limite pas à un domaine particulier et possède des applications dans de nombreux domaines :
\begin{itemize}
    \item En physique théorique, les expérimentations réalisées au Grand Collisionneur de Hadrons (LHC) demandent de grandes puissances de calcul pour analyser les données générées chaque seconde par 600 millions de collisions. L'analyse de ces données en un temps raisonnable est rendue possible grâce à une plateforme distribuée entre plusieurs pays formés d'un million de coeurs et d'un exabyte de stockage ($10^{18}$ octets). Cette plateforme de calculs a permis d'importantes découvertes comme celle du boson de Higgs \cite{Belyaev2017}. 
    
    \item Dans le domaine de la santé, le Living Heart Project \cite{Baillargeon2014} a pour objectif de modéliser un coeur pour aider à comprendre son fonctionnement. Depuis 2017, les fabricants peuvent utiliser cette modélisation pour tester un nouveau traitement avant de l'utiliser sur un patient. Un second projet nommé \textit{Human Brain Project} \cite{Markram2012} a pour objectif de simuler d'ici 2024 le fonctionnement d'un cerveau humain permettant de développer des traitements pour les maladies neurologiques.
    
    \item En mécanique des fluides, les simulations numériques sont utilisées dans l'aéronautique (maximiser le portage des ailes), la recherche d'hydrocarbures (modélisation 3D des fonds marins), la météorologie...

\end{itemize}
Ainsi, le HPC participe activement à l'amélioration des connaissances scientifiques mais aussi à l'évolution de nos sociétés, par exemple, en rendant nos vies plus sécurisés en anticipant certaines catastrophes naturelles (météorologie, épidémiologie, développement de médicaments\footnote{En mars 2020, l'institut Pasteur et les équipes de recherche de Sorbonne Université ont utilisé le supercalculateur Jean Zay, conçu par HPE, pour étudier la structure moléculaire du SARS-CoV-2 afin d'aider au développement de médicaments pour soigner le coronavirus - \url{http://www.genci.fr/fr/node/1043}.}, etc).
Ces simulations nécessitent d'énormes quantités de calculs pour être exécutées et utilisent des plateformes informatiques constituées de milliers de ressources de calculs appelées supercalculateur. L'amélioration de la performance de ces plateformes année après année a permis de réaliser de nombreuses avancés dans les domaines cités ci-dessus. En effet, leur performance a évolué exponentiellement ces 30 dernières années. Une montre connectée récente possède une puissance de calcul supérieure à la plateforme la plus puissante développée en 1985 (Cray-2), alors utilisée dans le domaine nucléaire. 


%Une grande majorité des objets avec lesquels nous interagissons tous les jours ont été conçues à l'aide de simulations numériques.

%et à l'évolution de nos sociétés. 
% à rendre nos vies plus sécurisées en prévoyant par exemple 

Le développement de plateformes de calculs plus puissantes va aider notre société à relever les nombreux défis qui lui font face: explosion démographique, réchauffement climatique ou comme plus récemment avec l'apparition d'un nouveau virus. Cette nouvelle génération de supercalculateurs permettra d'obtenir des réponses plus fiables et plus rapidement et permettra notamment d'analyser les grands volumes de données générées par les objets connectés. Ces analyses peuvent utiliser des algorithmes d'apprentissage automatique qui ne deviennent efficaces qu'après avoir suivi de nombreux entraînements. Ces derniers nécessitent de grandes puissances de calcul. Si la société veut continuer à profiter de nouvelles améliorations, comme la médecine personnalisée ou l'étude du réchauffement climatique, la performance des supercalculateurs doit continuer à évoluer.


\section{Supercalculateur exascale}

Actuellement, le défi de l'industrie des semi-conducteurs est la construction d'une plateforme capable d'exécuter $10^{18}$ opérations par seconde nommée supercalculateur \gls{exascale}. L'accès à une telle plateforme permettra alors d'exécuter les applications actuelles sept fois plus rapidement\footnote{En novembre 2019, le supercalculateur le plus puissant de la planète est capable d'exécuter $148 \times 10^{15}$ opérations par seconde.} (ou nécessitant sept fois plus d'opérations) que les systèmes \textit{petascale} les plus puissants aujourd'hui. L'obtention du premier supercalculateur exascale est très convoitée et représente un symbole de réussite pour son acquéreur (pays, université, industrie), ainsi que pour l'entreprise qui aura réalisé sa construction. L'industrie du \gls{HPC}, mais aussi les nations, se sont donc lancées dans cette course effrénée en y allouant des moyens considérables. Nous pouvons notamment citer les investissements réalisés par les États-Unis (PathForward, 400 millions d'euros) ou l'Union européenne  (EuroHPC, 486 millions d'euros). Ces projets sont réalisés en partenariat avec des entreprises privées du domaine HPC (dont HPE) qui financent d'un même montant chaque projet.


    \subsection{Les défis d'Exascale} 

        Pour comprendre la complexité des défis à relever pour construire une plateforme \gls{exascale}, le classement du Top500\footnote{Classement du Top500 - \url{https://www.top500.org/}} peut être consulté. Le Top500 classe deux fois par an les 500 supercalculateurs les plus puissants de la planète à l'aide de application de calcul intensif High-Performance Linpack  (\verb|HPL| \cite{Dongarra2003}). Cette application permet de mesurer le débit d'\gls{FLOPS} exécutées chaque seconde. Ce classement existe depuis 1993 et permet de mesurer et d'apprécier l'évolution de la performance des supercalculateurs les plus puissants tous les 6 mois.
        
        La \autoref{fig:top500_evolution_intro} présente l'évolution de la performance cumulée des 500 supercalculateurs apparaissant dans chaque classement du Top500. De 1993 à 2012, cette performance cumulée a évolué d'un facteur 1000 tous les 11 ans pendant près de 20 ans. Durant cette période, les processeurs ont pu profiter de nombreuses évolutions technologiques (voir \autoref{fig:evo_cpu1}) : augmentation du nombre de transistors et de la fréquence des processeurs, augmentation du nombre de coeurs ou encore l'implémentation d'une hiérarchie mémoire accélérant les accès aux données.
        %\clearpage
        
        \begin{figure}[hb!]
            \centering
            \includegraphics[width=13cm]{images/top500_evolution.png}
            \caption{\label{fig:top500_evolution_intro} Évolution de la performance cumulée des 500 supercalculateurs les plus puissants au monde, mesurée en \gls{FLOPS} à l'aide de l'application \texttt{HPL} \cite{Dongarra2003}.}

            \vspace*{\floatsep}% https://tex.stackexchange.com/q/26521/5764

            \includegraphics[width=13cm]{images/evo_proc.png}
            \caption{\label{fig:evo_cpu1} Évolution des principales caractéristiques des processeurs (données originales tirées de \cite{rupp40years}\protect\footnotemark).}
        \end{figure}
        \footnotetext{Les données sont accessibles sur le dépôt GitHub de l'auteur:  \url{https://github.com/karlrupp/microprocessor-trend-data}}
        
        Pour atteindre une puissance de \gls{exaFLOPS}, nous pouvons ainsi nous demander s'il ne suffirait pas d'attendre les prochaines générations de processeurs et d'augmenter le nombre de serveurs formant un supercalculateur. Malheureusement, nous constatons à partir de 2012 un ralentissement de l'évolution des performances des supercalculateurs. Il faudra maintenant attendre 23 ans pour voir leur performance évoluer du même facteur 1000.
        Ce ralentissement est dû à un ensemble de contraintes qu'il n'est plus possible d'éviter, dont les deux principales sont :
        \begin{itemize}
            \item la fin de la validité de loi de Moore, qui prévoyait l'augmentation du nombre de transistors chaque année;
            \item la fin de la loi de Dennard, qui assurait l'augmentation de la fréquence de chaque nouvelle génération de processeur.
        \end{itemize}
        De plus, les systèmes mémoires et les capacités de traitement des processeurs ont évolué inégalement. Lorsque la performance calculatoire augmentait de 50\% chaque année, le débit mémoire n'augmentait que de 23\%. Ce déséquilibre entre ces deux parties fondamentales de l'architecture Von Neumann a donné naissance au ``mur de la mémoire'' \cite{Wilkes2001}. À cause de cette inégalité d'évolution, les supercalculateurs modernes ne parviennent à extraire qu'une fraction des performances théoriques des architectures. Celle-ci est même couramment inférieure à 10\% \cite{Oliker2005} lors de l'exécution d'applications HPC utilisées dans le domaine du calcul scientifique.
        
        En plus des freins présentés ci-dessus, de nombreux défis supplémentaires doivent être relevés afin de poursuivre le rythme d'évolution des performances des supercalculateurs. Plusieurs études \cite{Sutter2005b, bergman2008exascale, Lucas2014, HPE2016} prédisent depuis plusieurs années les principales difficultés rencontrées pour développer une plateforme \gls{exascale} : 
        \begin{itemize}
            \item amélioration de l'efficacité énergétique : la consommation électrique des plateformes doit être réduite de plusieurs facteurs pour permettre l'exécution de $10^{18}$ opérations par secondes. Le défi énergétique s'applique à l'ensemble de la plateforme (processeur, mémoire, système d'interconnexion et de refroidissement);
            \item développement de nouvelles technologies mémoire : de nouvelles mémoires doivent être développées permettant de fournir un débit suffisamment élevé et des capacités adaptés à l'exécution d'applications exascales;
            \item amélioration de la performance (débit, latence) des technologies d'interconnexion;
            \item résistance aux erreurs, sécurité des données sensibles, productivité des utilisateurs, etc. 
        \end{itemize}

        Le principal défi est celui de l'énergie. Avec les technologies actuelles, un supercalculateur exascale consommerait plusieurs centaines de mégawatts, équivalent à la consommation électrique de deux villes comme Gap (19 000 foyers). Pour des raisons de coût et de faisabilité, l'objectif de l'industrie HPC est de construire une plateforme exascale consommant entre 20 et 30 MW. Le challenge est donc de construire une plateforme sept fois plus puissante que le supercalculateur le plus puissant actuel, tout en étant trois fois plus efficace énergétiquement. Les déplacements de données à l'intérieur (entre la mémoire et le processeur) et à l'extérieur (entre deux serveurs) des serveurs sont les principales opérations consommatrices d'énergie. Cette consommation peut être en partie expliquée par la complexité des microarchitectures, mais aussi par les technologies d'interconnexion utilisées. Cette complexité due aux nombreuses améliorations qu'ont connu les processeurs (hiérarchies mémoire, pipeline, d'unité de préchargement mémoire) est aussi à l'origine de la difficulté des applications à extraire plus d'une fraction des performances théoriques disponibles.
        

    \subsection{Les opportunités}


        L'incapacité des technologies actuelles à réaliser efficacement des calculs et l'explosion du nombre de données à traiter nécessite de repenser les matériels utilisés et l'architecture des systèmes informatiques. Heureusement de nouvelles technologies sont développées et représentent de réelles opportunités pour le développement des prochaines générations de plateformes de calculs.
        
        La première opportunité vient du développement de nouvelles technologies mémoire permettant de combler la différence d'évolution de performance constatée entre les processeurs et les mémoires. La faiblesse des débits mémoires et la consommation électrique du système mémoire obligent les architectures à évoluer drastiquement: 
        \begin{itemize}
            \item la première solution est de rapprocher ces espaces mémoire des zones de calculs en plaçant la mémoire sur les puces de calculs (On Package Memory) ou en réalisant les calculs directement en mémoire (In Memory Computing); 
            \item la seconde solution est de développer de nouvelles technologies mémoire afin de combler le trou séparant les caractéristiques des mémoires DRAM et des disques de stockage flash. Pour cela des technologies NVRAM (non-volatile RAM) sont développées en s'appuyant notamment sur de nouvelles technologies mémoire SCM (storage class memory).
        \end{itemize}
        
        Une deuxième opportunité technologique pouvant aider à l'élaboration d'une plateforme exascale est le développement de technologies photoniques. Ces technologies utilisent la lumière pour transférer des informations et permettre d'atteindre des débits élevés avec des latences faibles. Une autre qualité de ces technologies est d'être indépendant de la distance. Bien que la génération d'un signal optique ou électronique nécessite la même énergie, celle-ci n'évolue que faiblement avec la distance de communication. Le coût énergétique d'un accès mémoire local est alors proche de celui d'un accès distant. 
        
        Ces nouvelles technologies, associées aux méthodes de co-design qui associent les utilisateurs et les fabricants, vont permettre de développer de nouvelles architectures optimisées pour certains algorithmes.  L'utilisation d'architectures hétérogènes dans une même plateforme a été démocratisée par l'utilisation massive des GPU et doit être poursuivie avec de nouvelles architectures très différentes de celles actuellement utilisées : les accélérateurs produits par Google (TPU), NEC (SX-Aurora), ou encore Pezy (PEZY-SC2). 
        
        Afin de faciliter l'intégration de ces nouvelles technologies (mémoires, processeurs) provenant de différents constructeurs, un nouveau protocole est en cours de développement. Nommé Gen-Z, ce projet est développé par 70 des plus grandes sociétés des technologies de l'information (IT). Il repose sur l'utilisation d'un protocole de communication à sémantique mémoire \verb=load/store=. Gen-Z permettra d'adresser un espace mémoire mille fois plus grand que notre espace numérique actuel et interconnecter 16 millions d'objets sur une même matrice de commutation (\textit{fabric}). Les premières versions d'architecture Gen-Z permettront d'atteindre des débits mémoires de plusieurs téraoctets par seconde contre quelques centaines de gigaoctets actuellement.

\section{Travail de thèse}
    
    
    De nouvelles technologies vont permettre à l'industrie d'implémenter cette vision du calcul extrême qui consiste à l'exécution d'applications demandant des ressources dépassant de très loin les moyens disponibles actuels (villes intelligentes, analyse de données, etc). Afin que l'exécution de ces applications puisse être réalisée dans des contraintes raisonnables (temps, budget, complexité, etc.), le calcul extrême nécessite de revoir l'architecture des plateformes de calculs de manière holistique. Face à l'explosion de la quantité de données générées, il est indispensable de repenser l'architecture des centres de données, mais aussi de la plateforme permettant leur collection, leur transfert et leur analyse. Le développement de supercalculateurs exascale permettra l'exécution d'applications ne pouvant pas être exécutées dans des temps raisonnables actuellement.  



    \subsection{Problématique}
    
        Pour subvenir au besoin de puissance grandissant et ne pouvant compter sur les anciens leviers à sa disposition, l'industrie doit s'appuyer sur l'utilisation de nouvelles technologies émergentes qui permettront de faire des sauts de performance de plusieurs facteurs (10, 100 voir 1000).
        Grâce au protocole universel Gen-Z et aux nouvelles technologies présentées précédemment, de nombreuses architectures vont être développées et seront très différentes de celles que nous utilisons actuellement (processeurs x86, GPU). Ces processeurs seront alors très efficaces pour l'exécution de certains types d'opérations et d'algorithmes. Afin de répondre aux besoins de performances supplémentaires et relever le défi de l'efficacité énergétique, il est nécessaire d'utiliser les architectures les plus adaptées pour l'exécution d'une application. Pour ce faire, trois tâches principales doivent être réalisées :
        \begin{enumerate}
            \item La première tâche est de caractériser l'ensemble de ces architectures pour trouver leurs forces et leurs faiblesses: débit et latence mémoire, capacité calculatoire, performance du système mémoire pour certains motifs d'accès. Ces nouvelles architectures pouvant être très différentes de celles que nous utilisons actuellement, il est indispensable de disposer d'un ensemble d'outils permettant de les caractériser pour différents types d'algorithmes.
        
            \item La deuxième tâche consiste à modéliser les performances d'une application pour en connaître ses besoins (débit et/ou latence mémoire, calcul vectoriel, système d'interconnexion, stockage…). Les gains de performance ne viendront pas seulement de l'utilisation d'accélérateurs puissants, mais de leur diversité et de la capacité des programmeurs à bien les utiliser. Pour une même application, plusieurs zones de code prenant une part importante dans son exécution peuvent avoir des besoins différents. Il est ainsi nécessaire de pouvoir identifier ces zones et de les modéliser indépendamment. 
        
            \item La troisième tâche est de sélectionner les architectures adéquates pour une application et adapter le code pour permettre son exécution. Il est ensuite nécessaire de pouvoir mesurer les performances de l'application et d'appliquer les optimisations adaptées pour obtenir une part significative de la performance théorique.
        \end{enumerate}
    
    \subsection{Travaux existants}
    
        Afin de répondre aux tâches présentées ci-dessus, les programmeurs doivent alors avoir à leur disposition un ensemble d'outils permettant de caractériser le comportement des architectures et d'étudier la performance de leurs applications lors de l'exécution.
        
        
        \subsubsection{Caractérisation des architectures}
            
            Afin de caractériser les architectures et d'en mesurer les performances atteignables, il est courant d'utiliser des \glspl{benchmark}. En informatique, un benchmark est un code, ou un ensemble de codes, permettant de mesurer la performance d'une solution et d'en vérifier ses fonctionnalités. Dans ce travail de thèse, nous nous intéressons à la caractérisation de deux composants fondamentaux des processeurs utilisés pour l'exécution d'applications \gls{HPC} : la hiérarchie mémoire et les unités de calcul vectorielles. 
            
            \paragraph{Caractérisation du système mémoire} 
            
                Le système mémoire étant la principale ressource limitant la performance des applications HPC, de nombreux travaux ont été menés pour aider à sa caractérisation. Le plus utilisé d'entre eux est le benchmark \verb|STREAM| \cite{McCalpin1995}. Ce code utilise 4 noyaux de calculs différents pour mesurer le débit mémoire atteignable par une architecture. Pour cela, un tableau de nombres aléatoires stocké en mémoire est accédé par des instructions de lecture/écriture pour réaliser quatre opérations différentes (copy, scale, add, triad). La simplicité de ces codes permet souvent d'atteindre les débits maximaux du bus mémoire et fait de \verb|STREAM| un benchmark de référence. Un autre benchmark largement utilisé dans l'industrie est celui de \verb|Lmbench| \cite{Staelin2004}. \verb|Lmbench| est une suite de microbenchmarks utilisée pour mesurer la latence et le débit atteignable par un coeur pour différentes opérations : accès mémoire, ouverture de fichier, création de pipe. Il permet aussi de trouver certaines caractéristiques de la microarchitecture comme la taille d'une ligne de cache, la fréquence du processeur ou encore la taille et la latence du répertoire de pages actives (TLB). 
                
                Que ce soient \verb|STREAM|, \verb|lmbench| ou d'autres benchmarks mémoires \cite{Yotov2005, Luszczek2006, gonzalez2010servet, Dongarra2013, Bucek2018}, aucun code disponible ne permet de caractériser la hiérarchie mémoire lors d'accès mémoire par sauts d'adresses. De nombreuses applications utilisent des accès mémoires réalisant des sauts d'adresses (\gls{stride}) de taille constante. Par exemple, le parcours d'un tableau d'objet pour accéder à un certain champ ou le parcours d'une matrice (en ligne ou en colonne). Pour accélérer ces accès, l'unité de préchargement mémoire peut anticiper les prochaines adresses accédées en comprenant le motif d'accès utilisé et commencer leur chargement avant qu'elles ne soient accédées. Pour des applications utilisant ce type d'accès par saut, il est primordial que ce composant fonctionne correctement pour réduire au maximum le \gls{miss} d'une donnée dans le cache lors de son accès.
                Le benchmark s'approchant le plus de cet objectif est celui de Saavedra  \cite{Saavedra1995}.  Il utilise une taille de saut fixée au début de l'exécution pour accéder à un jeu de données. La taille de ce dernier doit être un multiple d'une puissance de 2 et ne permet pas de dépasser la taille du dernier niveau de cache. Le problème d'une telle approche est de vouloir mesurer tous les niveaux de la hiérarchie simultanément. Les mesures peuvent alors être influencées par différents paramètres de différents niveaux de caches.
        
            \paragraph{Caractérisation des unités de calculs vectorielles}
            
                La majorité des applications utilisées en \gls{HPC} exécutent des \gls{FLOP}. Ces instructions sont exécutées par un matériel spécialisé nommé \gls{FPU}. La capacité des FPU à exécuter plusieurs instructions vectorielles par cycle est alors primordiale pour la bonne performance des applications. La littérature dénombre plusieurs benchmarks couramment utilisés pour mesurer la performance de calcul des processeurs. Le benchmark \verb|HPL| \cite{Dongarra2003} a été développé pour caractériser la microarchitecture d'un processeur lors de la résolution de problèmes algébriques. Ce code permet aujourd'hui de mesurer et classer les supercalculateurs dans le classement du Top500. Cependant, la simplicité du benchmark \verb|HPL| n'est pas représentative d'applications réelles. Ainsi, un ensemble de codes a été rassemblé pour améliorer la caractérisation des supercalculateurs. Nommée \verb|HPCG| \cite{Dongarra2013}, cette suite de codes permet de couvrir plusieurs motifs de communication et de calculs. Les versions récentes du Top500 publient pour certains supercalculateurs la performance obtenue sur le benchmark \verb|HPCG|. Nous pouvons aussi citer le benchmark \verb|HPCC| \cite{Luszczek2006} qui est une suite de 6 benchmarks contenant notamment \verb|HPL| et \verb|STREAM|. Un seul exécutable est généré pour l'ensemble de la suite permettant d'éviter l'utilisation de certaines optimisations pour un code particulier (pages larges, réglage BIOS, utilisation d'une fréquence définie, etc). 

                Les benchmarks existants se basent sur des \gls{kernel} qui permettent de mesurer la performance que d'un ensemble d'opérations restreint. De plus, une grande majorité des benchmarks n'utilisent pas de code assembleur. La performance mesurée peut alors fortement varier avec la qualité du compilateur utilisé. Ainsi, nous constatons le manque d'un outil permettant de valider précisément les performances des \gls{FPU}. Un tel outil permettrait de mesurer le nombre d'instructions par cycle que le processeur est capable d'exécuter en faisant varier différents paramètres: type d'opérations, taille des instructions vectorielles, mélange d'instructions différentes, ajout de chaînes de dépendances, etc.
                
                
        \subsubsection{Suivi de performance des applications}
        
            Le deuxième aspect de nos recherches concerne le suivi de performances des applications. L'objectif de celui-ci est de récolter des informations concernant l'exécution de l'application. Ces informations peuvent être obtenues en instrumentant le code (manuellement ou grâce au compilateur) ou en récupérant certaines informations de l'architecture. Pour cela, les processeurs possèdent des registres (compteurs) qui permettent de compter des événements (matériels ou logiciels) avec un faible impact sur l'application exécutée. Chaque famille de processeurs possède un jeu différent de compteurs matériels pouvant avoir des noms différents, rendant leur programmation difficile. Afin de faciliter leur programmation, des interfaces comme \verb=PAPI= \cite{Browne2000} et \verb=Perf Events= \cite{Weaver2013} ont été développées. Celles-ci permettent de développer des outils sans se préoccuper de l'implémentation matérielle des compteurs facilitant ainsi la portabilité des outils. Nous pouvons notamment citer les outils d'Intel (\verb=VTune=), de Linux (\verb=perf=) ou encore ceux développés au Barcelona Supercomputing Center (\verb=Extrae= \cite{Rodriguez}, \verb=Paraver= \cite{Pillet1995} et \verb=Dymemas= \cite{Labarta1997}).
            
            Afin de répondre aux différentes tâches de la problématique traitée dans ce travail de thèse, deux outils principaux sont nécessaires. 
            Le premier doit permettre de suivre l'activité du bus mémoire. Ce bus est une ressource critique des architectures modernes, rendant sa bonne utilisation indispensable. Il est donc nécessaire de posséder un outil permettant de suivre l'état du trafic sur ce bus. Pour cela, des outils existent, mais sont soit propriétaires, soit trop complexes pour être facilement installés sur des architectures novatrices. 
            Un second outil doit être capable d'extraire les zones de codes les plus intéressantes à porter sur ces nouvelles architectures. Les supercalculateurs seront hétérogènes et utiliseront plusieurs types d'accélérateurs adaptés à certaines fonctions. Une même application pourra alors faire appel à plusieurs d'entre eux. Il est donc nécessaire de posséder un outil permettant de caractériser les différentes fonctions d'une même application en extrayant ces zones de codes et en les caractérisant.  Ce même outil pourrait alors aussi être utilisé pour comprendre les performances de l'application et appliquer les optimisations adéquates. 

\section{Contributions et plan du manuscrit}
    
    L'industrie va faire face à une multitude de révolutions technologiques (nouvelles mémoires, interconnexion photonique, protocole Gen-Z) et les utilisateurs de supercalculateur doivent se préparer à ces changements profonds pour pouvoir les exploiter au maximum. Contrairement aux évolutions précédentes où il suffisait d'installer un nouveau processeur ou d'ajouter des barrettes mémoires, il faut repenser entièrement notre façon d'appréhender les architectures des supercalculateurs et adapter les algorithmes qui y seront exécutés. Cependant, en l'absence de méthode de caractérisation fine de la performance des codes, ces architectures innovantes sont potentiellement condamnées puisque peu d'experts savent les valoriser. En effet, la loi de Moore a permis d'assurer une évolution constante des processeurs, laissant les domaines de l'analyse et de l'optimisation des codes, alors économiquement non rentables, en second plan. Ainsi, nous avons constaté à travers de nombreuses rencontres avec les utilisateurs, mais aussi par les expériences internes à HPE, le manque d'outils et de connaissances nécessaires à la réalisation de ce travail.
    
    \subsection{Contributions}
    
        Ce travail de thèse contribue à l'étude du domaine du calcul haute performance. Le déroulement de cette thèse dans un cadre industriel a permis de réaliser un large état de l'art de ce domaine complexe. Nous regroupons et expliquons les principaux défis auxquels les constructeurs font face dans le développement des prochaines générations de supercalculateurs. Afin d'y répondre, nous présentons les principales opportunités technologiques qui vont permettre de relever les défis présentés et notamment de la nécessité d'utiliser des architectures novatrices pour soutenir la contrainte énergétique.
        
        Afin de pouvoir profiter de ces nouvelles technologies, nous proposons dans ce travail une méthodologie en 5 étapes permettant de modéliser les performances d'une application, de les projeter sur de nouvelles architectures et d'en extraire la performance maximale. Pour chaque étape, nous avons développé et sélectionné des outils permettant de répondre aux challenges présentés et qui respectent les critères suivants :
        \begin{itemize}
            \item les sources de l'outil doivent être disponibles pour permettre de les adapter à des architectures différentes et pour développer une communauté d'utilisateurs autour de ces sujets;
            \item les outils doivent être simples, ne cherchant pas à répondre à une multitude de questions qui rendent difficile leur portabilité, mais aussi les conclusions pouvant être tirées des résultats obtenus. La simplicité des outils permet aussi de réduire leur dépendance à des librairies externes, souvent difficiles à installer dans les environnements HPC;
            \item les outils doivent permettre d'analyser des applications HPC dont l'exécution peut durer plusieurs heures. Il faut aussi réduire au maximum la nécessité de posséder des droits spéciaux (root) pour les utiliser, ce dernier étant rarement disponible pour les utilisateurs de supercalculateurs.
        \end{itemize}
 
        Contrairement à une majorité d'outils existants, ceux développés durant ce travail de thèse n'ont pas vocation d'automatiser entièrement le travail d'analyse. La puissance de ces outils vient de leur utilisation complémentaire. Leur utilisation assume que l'utilisateur possède de solides connaissances. En fournissant une méthodologie permettant de bien les utiliser et en réduisant la complexité de l'outillage, nous espérons que leur adoption auprès des programmeurs sera plus grande. Un outil a été développé pour chaque étape de la méthodologie lorsqu'aucun programme existant ne répondait aux besoins, ou qu'il ne respectait pas les critères énoncés ci-dessus. La principale difficulté concernant l'utilisation ou le développement d'outils concerne leur compatibilité avec différentes architectures. Celles-ci pourront être produites par différents constructeurs (Intel, ARM, IBM, Nvidia, etc.), il était primordial d'utiliser ou de développer des outils ayant le plus de chances d'être compatibles avec ces nouvelles architectures. Ce besoin de compatibilité nous a obligés à restreindre la dépendance des outils à certaines fonctionnalités des processeurs, notamment l'utilisation des compteurs matériels.
        
        
        En résumé, les principales contributions de cette thèse sont :
        \begin{enumerate}

            \item \textbf{Un benchmark mémoire} – \verb=DML_MEM= : ce benchmark permet de mesurer le débit mémoire en réalisant des accès dans un tableau avec des motifs de sauts (strides). La taille du tableau et des sauts peut varier et permet de caractériser les différents niveaux de la hiérarchie mémoire. Cet outil est utile pour caractériser une nouvelle architecture, mais peut aussi être utilisé lors du design d'un nouveau processeur pour vérifier le bon fonctionnement du \gls{prelecteur}.
            
            \item \textbf{Un benchmark de FPU} – \verb|Kernel Generator| : ce générateur permet de caractériser finement les performances des unités de calcul en virgule flottante (FPU). Pour cela, il génère un benchmark assembleur en utilisant des instructions vectorielles de taille différentes (SSE, AVX2, AVX-512). L'utilisateur peut alors choisir le type d'opération à exécuter (addition, multiplication, FMA) et la précision (simple, double). La partie du benchmark mesurée ne contient que des instructions de calculs et permet de mesurer très précisément la performance atteignable par le processeur (souvent proche ou égale à la performance théorique).
                        
            \item \textbf{Un outil de suivi d'activité du bus mémoire} -  \verb=YAMB= : cet outil  permet de suivre le trafic du bus mémoire. Pour cela, l'outil suit l'activité de chaque contrôleur mémoire à l'aide de l'interface \verb=Perf Events= en mesurant le nombre de transactions (lecture et écriture) ainsi que le nombre d'accès manqués (miss) dans le dernier niveau de cache (LLC). Pour corréler l'activité du bus avec les parties du code qui en sont responsables, le graphique peut facilement être annoté en utilisant une API C/C++/Fortran, appelée directement depuis le code source.
            
            \item \textbf{Un outil d'analyse de performance} – \verb=Oprofile++=: cet outil permet de désassembler le code d'une application, d'extraire les \gls{hotspot} et d'en mesurer leur performance. Dans un premier temps, l'outil  relie le profil de performance obtenu lors de l'exécution aux instructions assembleurs exécutées. Ensuite, il s'occupe d'extraire les boucles critiques et de mesurer le nombre d'\gls{IPC}. Il est ensuite possible de quantifier des opportunités d'amélioration, mais aussi de prédire la performance en fonction d'une amélioration du matériel ou du logiciel.
            
        \end{enumerate}
        
                
    %\newpage    
    \subsection{Plan du manuscrit}
    
        Cette thèse propose une analyse étendue du domaine du \gls{HPC} pour comprendre et contribuer à l'élaboration des prochaines générations de plateformes de calculs. Pour cela, le manuscrit suit la structure suivante :
        
        \begin{itemize}
            \item Le \autoref{chap:hpc} introduit le domaine du calcul haute performance en étudiant ses origines et en présentant leur architecture actuelle et les moyens employés pour les programmer. Nous discutons ensuite de leur performance en étudiant l'évolution du classement du Top500 et nous expliquons les principaux freins qui empêchent de construire des plateformes toujours plus puissantes avec les méthodes actuelles consistant à ajouter indéfiniment des serveurs. Afin d'y parvenir, nous présentons les principales opportunités technologiques actuellement développées qui vont nous permettre de repenser l'architecture des plateformes. La fin de ce chapitre s'intéresse plus précisément à la caractérisation des microarchitectures et à l'analyse de performance d'applications.
        
            \item Le \autoref{chap:dev} présente les principaux outils développés durant ces travaux de thèse. Nous discutons des motivations et des critères de développement qui nous ont conduits à développer deux benchmarks et deux outils d'analyse de performance.
        
            \item Le \autoref{chap:methodo} présente la méthodologie élaborée et utilisée pour caractériser et choisir une architecture pour une application donnée. Nous utilisons l'analyse du benchmark \verb|STREAM| et d'un processeur Intel Skylake pour illustrer chacune des 5 étapes.
        \end{itemize}
            
        Afin de permettre la réalisation des différents codes développés, l'étude approfondie des différentes évolutions technologiques des processeurs et des techniques de suivi de performances sont présentées dans deux annexes:
        \begin{itemize}
            \item L'\aref{annexe:CHAPITRE_ARCHITECTURE} couvre l'origine et l'évolution de la microarchitecture des processeurs. Nous présentons les fonctionnalités clés des processeurs modernes qu'il est nécessaire de connaître pour comprendre la performance des applications: pipeline, instructions vectorielles, etc. Nous étudions plus précisément la hiérarchie mémoire qui est la ressource critique de nombreuses applications. 
            
            \item L'\aref{annexe:hardware_counter} présente les compteurs matériels. Ces registres spéciaux du processeur permettent de suivre l'exécution d'une application en mesurant le nombre d'évènements logiciels et matériels. La programmation des compteurs est très difficile et nécessite le recours à des codes bas niveaux. Plusieurs méthodes peuvent alors être employées et demandent une bonne expérience pour être mises en oeuvre.
        \end{itemize}
       Nous faisons référence à ces deux annexes lorsque les concepts qui y sont étudiés sont utilisés dans le manuscrit.